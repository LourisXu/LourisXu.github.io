<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="renderer" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge"><link rel="dns-prefetch" href="https://lourisxu.github.io"><title>(7) Deep Learning: Recurrent Neural Networks | Louris&#39; Blog</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="description" content="循环神经网络 含隐藏状态的循环神经网络1234567from mxnet import ndX, W_xh &#x3D; nd.random.normal(shape&#x3D;(3, 1)), nd.random.normal(shape&#x3D;(1, 4))H, W_hh &#x3D; nd.random.normal(shape&#x3D;(3, 4)), nd.random.normal(shape&#x3D;(4, 4))print(nd."><meta property="og:type" content="article"><meta property="og:title" content="(7) Deep Learning: Recurrent Neural Networks"><meta property="og:url" content="https://lourisxu.github.io/2020/03/25/7-deep-learning-recurrent-neural-networks.html/index.html"><meta property="og:site_name" content="Louris&#39; Blog"><meta property="og:description" content="循环神经网络 含隐藏状态的循环神经网络1234567from mxnet import ndX, W_xh &#x3D; nd.random.normal(shape&#x3D;(3, 1)), nd.random.normal(shape&#x3D;(1, 4))H, W_hh &#x3D; nd.random.normal(shape&#x3D;(3, 4)), nd.random.normal(shape&#x3D;(4, 4))print(nd."><meta property="article:published_time" content="2020-03-25T07:01:37.000Z"><meta property="article:modified_time" content="2024-01-19T02:20:51.962Z"><meta property="article:author" content="Louris"><meta property="article:tag" content="ML"><meta property="article:tag" content="DL"><meta name="twitter:card" content="summary"><link rel="alternative" href="/atom.xml" title="Louris&#39; Blog" type="application/atom+xml"><link rel="icon" href="/assets/img/blog/favicon.png"><link rel="stylesheet" href="/./main.e8862b.css"><style>#container.show{background:linear-gradient(200deg,#a0cfe4,#e8c37e)}</style><meta name="generator" content="Hexo 4.2.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style></head><body><div id="container" q-class="show:isCtnShow"><canvas id="anm-canvas" class="anm-canvas"></canvas><div class="left-col" q-class="show:isShow"><div class="overlay" style="background:#00bfff"></div><div class="intrude-less"><header id="header" class="inner"> <a href="/" class="profilepic"><img src="/assets/img/blog/userpic.jpg" class="js-avatar"></a><hgroup><h1 class="header-author"><a href="/">Louris</a></h1></hgroup><p class="header-subtitle">Do what I can</p><nav class="header-menu"><ul><li><a href="/">Home</a></li><li><a href="/categories/DS/">DS</a></li><li><a href="/tags/ML/">ML&amp;DL</a></li><li><a href="/tags/Tech/">Tech</a></li><li><a href="/tags/Algorithm/">Algorithm</a></li></ul></nav><nav class="header-smart-menu"> <a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">All articles</a> <a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">Friends</a> <a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">About me</a></nav><nav><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="230" height="86" src="//music.163.com/outchain/player?type=2&id=2080322&auto=1&height=66"></iframe></nav><nav class="header-nav"><div class="social"><a class="github" target="_blank" href="https://github.com/LourisXu" title="github"><i class="icon-github"></i></a><a class="weibo" target="_blank" href="http://weibo.com/5634881238/profile?rightmod=1&wvr=6&mod=personinfo" title="weibo"><i class="icon-weibo"></i></a><a class="mail" target="_blank" href="mailto:louris@csu.edu.cn" title="mail"><i class="icon-mail"></i></a></div></nav></header></div></div><div class="mid-col" q-class="show:isShow,hide:isShow|isFalse"><nav id="mobile-nav"><div class="overlay js-overlay" style="background:#00bfff"></div><div class="btnctn js-mobile-btnctn"><div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div></div><div class="intrude-less"><header id="header" class="inner"><div class="profilepic"> <img src="/assets/img/blog/userpic.jpg" class="js-avatar"></div><hgroup><h1 class="header-author js-header-author">Louris</h1></hgroup><p class="header-subtitle"><i class="icon icon-quo-left"></i>Do what I can<i class="icon icon-quo-right"></i></p><nav class="header-nav"><div class="social"><a class="github" target="_blank" href="https://github.com/LourisXu" title="github"><i class="icon-github"></i></a><a class="weibo" target="_blank" href="http://weibo.com/5634881238/profile?rightmod=1&wvr=6&mod=personinfo" title="weibo"><i class="icon-weibo"></i></a><a class="mail" target="_blank" href="mailto:louris@csu.edu.cn" title="mail"><i class="icon-mail"></i></a></div></nav><nav class="header-menu js-header-menu"><ul style="width:70%"><li style="width:20%"><a href="/">Home</a></li><li style="width:20%"><a href="/categories/DS/">DS</a></li><li style="width:20%"><a href="/tags/ML/">ML&amp;DL</a></li><li style="width:20%"><a href="/tags/Tech/">Tech</a></li><li style="width:20%"><a href="/tags/Algorithm/">Algorithm</a></li></ul></nav></header></div><div class="mobile-mask" style="display:none" q-show="isShow"></div></nav><div id="wrapper" class="body-wrap"><div class="menu-l"><div class="canvas-wrap"><canvas data-colors="#eaeaea" data-sectionheight="100" data-contentid="js-content" id="myCanvas1" class="anm-canvas"></canvas></div><div id="js-content" class="content-ll"><article id="post-DL/7-Deep-Learning-Recurrent-Neural-Networks" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 class="article-title" itemprop="name"> (7) Deep Learning: Recurrent Neural Networks</h1><a href="/2020/03/25/7-deep-learning-recurrent-neural-networks.html/" class="archive-article-date"><time datetime="2020-03-25T07:01:37.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i> 2020-03-25</time></a></header><div class="article-entry" itemprop="articleBody"><link rel="stylesheet" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1></blockquote><h2 id="含隐藏状态的循环神经网络"><a href="#含隐藏状态的循环神经网络" class="headerlink" title="含隐藏状态的循环神经网络"></a>含隐藏状态的循环神经网络</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"></span><br><span class="line">X, W_xh = nd.random.normal(shape=(<span class="number">3</span>, <span class="number">1</span>)), nd.random.normal(shape=(<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">H, W_hh = nd.random.normal(shape=(<span class="number">3</span>, <span class="number">4</span>)), nd.random.normal(shape=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">print(nd.dot(X, W_xh)+nd.dot(H, W_hh))</span><br><span class="line">print(nd.dot(nd.concat(X, H, dim=<span class="number">1</span>), nd.concat(W_xh, W_hh, dim=<span class="number">0</span>)))</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[ <span class="number">3.1951556</span>  <span class="number">-7.028843</span>    <span class="number">6.2385654</span>   <span class="number">3.5568767</span> ]</span><br><span class="line"> [ <span class="number">2.809851</span>   <span class="number">-1.8081225</span>   <span class="number">0.6729961</span>  <span class="number">-0.23211202</span>]</span><br><span class="line"> [<span class="number">-0.14438549</span> <span class="number">-2.5961134</span>  <span class="number">-1.1423202</span>  <span class="number">-4.142916</span>  ]]</span><br><span class="line">&lt;NDArray <span class="number">3</span>x4 @cpu(<span class="number">0</span>)&gt;</span><br><span class="line"></span><br><span class="line">[[ <span class="number">3.1951556</span>  <span class="number">-7.0288424</span>   <span class="number">6.2385654</span>   <span class="number">3.556877</span>  ]</span><br><span class="line"> [ <span class="number">2.8098507</span>  <span class="number">-1.8081226</span>   <span class="number">0.67299604</span> <span class="number">-0.23211199</span>]</span><br><span class="line"> [<span class="number">-0.14438534</span> <span class="number">-2.5961134</span>  <span class="number">-1.14232</span>    <span class="number">-4.142916</span>  ]]</span><br><span class="line">&lt;NDArray <span class="number">3</span>x4 @cpu(<span class="number">0</span>)&gt;</span><br></pre></td></tr></tbody></table></figure><h2 id="语言模型数据集（周杰伦专辑歌词）"><a href="#语言模型数据集（周杰伦专辑歌词）" class="headerlink" title="语言模型数据集（周杰伦专辑歌词）"></a>语言模型数据集（周杰伦专辑歌词）</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_jay_lyrics</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(<span class="string">'./data/jaychou_lyrics.txt.zip'</span>) <span class="keyword">as</span> zin:</span><br><span class="line">        <span class="keyword">with</span> zin.open(<span class="string">'jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            corpus_chars = f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(corpus_chars[:<span class="number">40</span>])</span><br><span class="line"></span><br><span class="line">    corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">    <span class="comment"># 仅使用前1万个字符训练模型</span></span><br><span class="line">    corpus_chars = corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line">    idx_to_char = list(set(corpus_chars))</span><br><span class="line">    char_to_idx = dict([(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)])</span><br><span class="line">    vocab_size = len(char_to_idx)</span><br><span class="line">    print(vocab_size)</span><br><span class="line">    print(char_to_idx)</span><br><span class="line"></span><br><span class="line">    x = [(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)]</span><br><span class="line">    print(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集中的每个字符转化成索引</span></span><br><span class="line">    corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line">    <span class="comment"># 打印前20个字符机器对应的索引</span></span><br><span class="line">    sample = corpus_indices[:<span class="number">20</span>]</span><br><span class="line">    print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line">    print(<span class="string">'indices:'</span>, sample)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">load_data_jay_lyrics()</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">想要有直升机</span><br><span class="line">想要和你飞到宇宙去</span><br><span class="line">想要和你融化在一起</span><br><span class="line">融化在宇宙里</span><br><span class="line">我每天每天每</span><br><span class="line"><span class="number">1027</span></span><br><span class="line">{<span class="string">'变'</span>: <span class="number">0</span>, <span class="string">'甘'</span>: <span class="number">1</span>, <span class="string">'排'</span>: <span class="number">2</span>, <span class="string">'人'</span>: <span class="number">3</span>, <span class="string">'诗'</span>: <span class="number">4</span>, <span class="string">'将'</span>: <span class="number">5</span>, <span class="string">'晚'</span>: <span class="number">6</span>, <span class="string">'话'</span>: <span class="number">7</span>, <span class="string">'雨'</span>: <span class="number">8</span>, <span class="string">'熟'</span>: <span class="number">9</span>, <span class="string">'向'</span>: <span class="number">10</span>, <span class="string">'彻'</span>: <span class="number">11</span>, <span class="string">'黄'</span>: <span class="number">12</span>, ...}</span><br><span class="line">[(<span class="string">'变'</span>, <span class="number">0</span>), (<span class="string">'甘'</span>, <span class="number">1</span>), (<span class="string">'排'</span>, <span class="number">2</span>), (<span class="string">'人'</span>, <span class="number">3</span>), (<span class="string">'诗'</span>, <span class="number">4</span>), (<span class="string">'将'</span>, <span class="number">5</span>), (<span class="string">'晚'</span>, <span class="number">6</span>), (<span class="string">'话'</span>, <span class="number">7</span>), (<span class="string">'雨'</span>, <span class="number">8</span>), (<span class="string">'熟'</span>, <span class="number">9</span>), (<span class="string">'向'</span>, <span class="number">10</span>), (<span class="string">'彻'</span>, <span class="number">11</span>), (<span class="string">'黄'</span>, <span class="number">12</span>), (<span class="string">'丽'</span>, <span class="number">13</span>), (<span class="string">'怎'</span>, <span class="number">14</span>), (<span class="string">'世'</span>, <span class="number">15</span>), (<span class="string">'升'</span>, <span class="number">16</span>), (<span class="string">'妥'</span>, <span class="number">17</span>), (<span class="string">'怕'</span>, <span class="number">18</span>), (<span class="string">'碌'</span>, <span class="number">19</span>)]</span><br><span class="line">chars: 想要有直升机 想要和你飞到宇宙去 想要和</span><br><span class="line">indices: [<span class="number">634</span>, <span class="number">189</span>, <span class="number">133</span>, <span class="number">768</span>, <span class="number">16</span>, <span class="number">530</span>, <span class="number">157</span>, <span class="number">634</span>, <span class="number">189</span>, <span class="number">925</span>, <span class="number">1020</span>, <span class="number">308</span>, <span class="number">735</span>, <span class="number">467</span>, <span class="number">473</span>, <span class="number">282</span>, <span class="number">157</span>, <span class="number">634</span>, <span class="number">189</span>, <span class="number">925</span>]</span><br></pre></td></tr></tbody></table></figure><table><thead><tr><th align="center">采样类型</th><th align="left">说明</th></tr></thead><tbody><tr><td align="center">随机采样</td><td align="left">相邻两次迭代采样序列不相邻！随机！循环神经网络隐藏状态需要每次初始化</td></tr><tr><td align="center">相邻采样</td><td align="left">相邻两次迭代采样序列相邻！循环神经网络隐藏状态只需要第一次初始化</td></tr></tbody></table><h3 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_jay_lyrics</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(<span class="string">'./data/jaychou_lyrics.txt.zip'</span>) <span class="keyword">as</span> zin:</span><br><span class="line">        <span class="keyword">with</span> zin.open(<span class="string">'jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            corpus_chars = f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(corpus_chars[:<span class="number">40</span>])</span><br><span class="line"></span><br><span class="line">    corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">    <span class="comment"># 仅使用前1万个字符训练模型</span></span><br><span class="line">    corpus_chars = corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line">    idx_to_char = list(set(corpus_chars))</span><br><span class="line">    char_to_idx = dict([(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)])</span><br><span class="line">    vocab_size = len(char_to_idx)</span><br><span class="line">    print(vocab_size)</span><br><span class="line">    print(char_to_idx)</span><br><span class="line"></span><br><span class="line">    x = [(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)]</span><br><span class="line">    print(x[:<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集中的每个字符转化成索引</span></span><br><span class="line">    corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line">    <span class="comment"># 打印前20个字符机器对应的索引</span></span><br><span class="line">    sample = corpus_indices[:<span class="number">20</span>]</span><br><span class="line">    print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line">    print(<span class="string">'indices:'</span>, sample)</span><br><span class="line">    <span class="keyword">return</span> idx_to_char, char_to_idx, corpus_indices</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># num_steps: 6</span></span><br><span class="line"><span class="comment"># corpus_indices:</span></span><br><span class="line"><span class="comment"># [0,1,2,...,29]</span></span><br><span class="line"><span class="comment"># [[0,...,5],[6,..,11],[12,...,17],[18,...,23],[24,...,29]]</span></span><br><span class="line"><span class="comment"># example_indices: [0,1,2,3,4]</span></span><br><span class="line"><span class="comment"># shuffle example_indices: [...]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    <span class="comment"># 减1是因为输出的索引是相应输入的索引加1</span></span><br><span class="line">    <span class="comment"># [0,1,2,3]输出标签应为[1,2,3,4]</span></span><br><span class="line">    <span class="comment"># 为了防止溢出</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps</span><br><span class="line">    print(num_examples)</span><br><span class="line">    epoch_size = num_examples // batch_size</span><br><span class="line">    example_indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(example_indices)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回从pos开始的长为num_steps的序列</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span><span class="params">(pos)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        <span class="comment"># 每次读取batch_size个随机样本</span></span><br><span class="line">        i = i * batch_size</span><br><span class="line">        batch_indices = example_indices[i:i + batch_size]</span><br><span class="line">        X = [_data(j * num_steps) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j * num_steps + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> nd.array(X, ctx), nd.array(Y, ctx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">my_seq = list(range(<span class="number">30</span>))</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter_random(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">6</span>):</span><br><span class="line">    print(<span class="string">'X:'</span>, X, <span class="string">'\nY:'</span>, Y, <span class="string">'\n'</span>)</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">X:</span><br><span class="line">[[ <span class="number">6.</span>  <span class="number">7.</span>  <span class="number">8.</span>  <span class="number">9.</span> <span class="number">10.</span> <span class="number">11.</span>]</span><br><span class="line"> [<span class="number">18.</span> <span class="number">19.</span> <span class="number">20.</span> <span class="number">21.</span> <span class="number">22.</span> <span class="number">23.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x6 @cpu(<span class="number">0</span>)&gt;</span><br><span class="line">Y:</span><br><span class="line">[[ <span class="number">7.</span>  <span class="number">8.</span>  <span class="number">9.</span> <span class="number">10.</span> <span class="number">11.</span> <span class="number">12.</span>]</span><br><span class="line"> [<span class="number">19.</span> <span class="number">20.</span> <span class="number">21.</span> <span class="number">22.</span> <span class="number">23.</span> <span class="number">24.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x6 @cpu(<span class="number">0</span>)&gt;</span><br><span class="line"></span><br><span class="line">X:</span><br><span class="line">[[ <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">2.</span>  <span class="number">3.</span>  <span class="number">4.</span>  <span class="number">5.</span>]</span><br><span class="line"> [<span class="number">12.</span> <span class="number">13.</span> <span class="number">14.</span> <span class="number">15.</span> <span class="number">16.</span> <span class="number">17.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x6 @cpu(<span class="number">0</span>)&gt;</span><br><span class="line">Y:</span><br><span class="line">[[ <span class="number">1.</span>  <span class="number">2.</span>  <span class="number">3.</span>  <span class="number">4.</span>  <span class="number">5.</span>  <span class="number">6.</span>]</span><br><span class="line"> [<span class="number">13.</span> <span class="number">14.</span> <span class="number">15.</span> <span class="number">16.</span> <span class="number">17.</span> <span class="number">18.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x6 @cpu(<span class="number">0</span>)&gt;</span><br></pre></td></tr></tbody></table></figure><h3 id="相邻采样"><a href="#相邻采样" class="headerlink" title="相邻采样"></a>相邻采样</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_jay_lyrics</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(<span class="string">'./data/jaychou_lyrics.txt.zip'</span>) <span class="keyword">as</span> zin:</span><br><span class="line">        <span class="keyword">with</span> zin.open(<span class="string">'jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            corpus_chars = f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(corpus_chars[:<span class="number">40</span>])</span><br><span class="line"></span><br><span class="line">    corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">    <span class="comment"># 仅使用前1万个字符训练模型</span></span><br><span class="line">    corpus_chars = corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line">    idx_to_char = list(set(corpus_chars))</span><br><span class="line">    char_to_idx = dict([(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)])</span><br><span class="line">    vocab_size = len(char_to_idx)</span><br><span class="line">    print(vocab_size)</span><br><span class="line">    print(char_to_idx)</span><br><span class="line"></span><br><span class="line">    x = [(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)]</span><br><span class="line">    print(x[:<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集中的每个字符转化成索引</span></span><br><span class="line">    corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line">    <span class="comment"># 打印前20个字符机器对应的索引</span></span><br><span class="line">    sample = corpus_indices[:<span class="number">20</span>]</span><br><span class="line">    print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line">    print(<span class="string">'indices:'</span>, sample)</span><br><span class="line">    <span class="keyword">return</span> idx_to_char, char_to_idx, corpus_indices</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># num_steps: 6</span></span><br><span class="line"><span class="comment"># corpus_indices:</span></span><br><span class="line"><span class="comment"># [0,1,2,...,29]</span></span><br><span class="line"><span class="comment"># [[0,...,5],[6,..,11],[12,...,17],[18,...,23],[24,...,29]]</span></span><br><span class="line"><span class="comment"># example_indices: [0,1,2,3,4]</span></span><br><span class="line"><span class="comment"># shuffle example_indices: [...]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    <span class="comment"># 减1是因为输出的索引是相应输入的索引加1</span></span><br><span class="line">    <span class="comment"># [0,1,2,3]输出标签应为[1,2,3,4]</span></span><br><span class="line">    <span class="comment"># 为了防止溢出</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps</span><br><span class="line">    print(num_examples)</span><br><span class="line">    epoch_size = num_examples // batch_size</span><br><span class="line">    example_indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(example_indices)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回从pos开始的长为num_steps的序列</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span><span class="params">(pos)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        <span class="comment"># 每次读取batch_size个随机样本</span></span><br><span class="line">        i = i * batch_size</span><br><span class="line">        batch_indices = example_indices[i:i + batch_size]</span><br><span class="line">        X = [_data(j * num_steps) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j * num_steps + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> nd.array(X, ctx), nd.array(Y, ctx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    corpus_indices = nd.array(corpus_indices, ctx=ctx)</span><br><span class="line">    data_len = len(corpus_indices)</span><br><span class="line">    batch_len = data_len // batch_size</span><br><span class="line">    indices = corpus_indices[<span class="number">0</span>:batch_size * batch_len].reshape((batch_size, batch_len))</span><br><span class="line">    print(indices)</span><br><span class="line">    epoch_size = (batch_len - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i:i + num_steps]</span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>:i + num_steps + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">my_seq = range(<span class="number">30</span>)</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter_consecutive(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">6</span>):</span><br><span class="line">    print(<span class="string">'X: '</span>, X, <span class="string">'\nY:'</span>, Y, <span class="string">'\n'</span>)</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[[ <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">2.</span>  <span class="number">3.</span>  <span class="number">4.</span>  <span class="number">5.</span>  <span class="number">6.</span>  <span class="number">7.</span>  <span class="number">8.</span>  <span class="number">9.</span> <span class="number">10.</span> <span class="number">11.</span> <span class="number">12.</span> <span class="number">13.</span> <span class="number">14.</span>]</span><br><span class="line"> [<span class="number">15.</span> <span class="number">16.</span> <span class="number">17.</span> <span class="number">18.</span> <span class="number">19.</span> <span class="number">20.</span> <span class="number">21.</span> <span class="number">22.</span> <span class="number">23.</span> <span class="number">24.</span> <span class="number">25.</span> <span class="number">26.</span> <span class="number">27.</span> <span class="number">28.</span> <span class="number">29.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x15 @cpu(<span class="number">0</span>)&gt;</span><br><span class="line">X:  </span><br><span class="line">[[ <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">2.</span>  <span class="number">3.</span>  <span class="number">4.</span>  <span class="number">5.</span>]</span><br><span class="line"> [<span class="number">15.</span> <span class="number">16.</span> <span class="number">17.</span> <span class="number">18.</span> <span class="number">19.</span> <span class="number">20.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x6 @cpu(<span class="number">0</span>)&gt;</span><br><span class="line">Y:</span><br><span class="line">[[ <span class="number">1.</span>  <span class="number">2.</span>  <span class="number">3.</span>  <span class="number">4.</span>  <span class="number">5.</span>  <span class="number">6.</span>]</span><br><span class="line"> [<span class="number">16.</span> <span class="number">17.</span> <span class="number">18.</span> <span class="number">19.</span> <span class="number">20.</span> <span class="number">21.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x6 @cpu(<span class="number">0</span>)&gt;</span><br><span class="line"></span><br><span class="line">X:  </span><br><span class="line">[[ <span class="number">6.</span>  <span class="number">7.</span>  <span class="number">8.</span>  <span class="number">9.</span> <span class="number">10.</span> <span class="number">11.</span>]</span><br><span class="line"> [<span class="number">21.</span> <span class="number">22.</span> <span class="number">23.</span> <span class="number">24.</span> <span class="number">25.</span> <span class="number">26.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x6 @cpu(<span class="number">0</span>)&gt;</span><br><span class="line">Y:</span><br><span class="line">[[ <span class="number">7.</span>  <span class="number">8.</span>  <span class="number">9.</span> <span class="number">10.</span> <span class="number">11.</span> <span class="number">12.</span>]</span><br><span class="line"> [<span class="number">22.</span> <span class="number">23.</span> <span class="number">24.</span> <span class="number">25.</span> <span class="number">26.</span> <span class="number">27.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x6 @cpu(<span class="number">0</span>)&gt;</span><br></pre></td></tr></tbody></table></figure><h2 id="循环神经网络从零开始实现"><a href="#循环神经网络从零开始实现" class="headerlink" title="循环神经网络从零开始实现"></a>循环神经网络从零开始实现</h2><h3 id="one-hot向量"><a href="#one-hot向量" class="headerlink" title="one_hot向量"></a>one_hot向量</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_onehot</span><span class="params">(X, size)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [nd.one_hot(x, size) <span class="keyword">for</span> x <span class="keyword">in</span> X.T]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># my_seq = range(30)</span></span><br><span class="line"><span class="comment"># for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):</span></span><br><span class="line"><span class="comment">#     print('X: ', X, '\nY:', Y, '\n')</span></span><br><span class="line"></span><br><span class="line">idx_to_char, char_to_idx, corpus_indices, vocab_size = load_data_jay_lyrics()</span><br><span class="line"></span><br><span class="line">X = nd.arange(<span class="number">10</span>).reshape((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">print(X)</span><br><span class="line">inputs = to_onehot(X, vocab_size)</span><br><span class="line">print(len(inputs), inputs[<span class="number">0</span>].shape)</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.</span> <span class="number">1.</span> <span class="number">2.</span> <span class="number">3.</span> <span class="number">4.</span>]</span><br><span class="line"> [<span class="number">5.</span> <span class="number">6.</span> <span class="number">7.</span> <span class="number">8.</span> <span class="number">9.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x5 @cpu(<span class="number">0</span>)&gt;</span><br><span class="line"><span class="number">5</span> (<span class="number">2</span>, <span class="number">1027</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="模型训练以及预测"><a href="#模型训练以及预测" class="headerlink" title="模型训练以及预测"></a>模型训练以及预测</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_jay_lyrics</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(<span class="string">'./data/jaychou_lyrics.txt.zip'</span>) <span class="keyword">as</span> zin:</span><br><span class="line">        <span class="keyword">with</span> zin.open(<span class="string">'jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            corpus_chars = f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(corpus_chars[:<span class="number">40</span>])</span><br><span class="line"></span><br><span class="line">    corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">    <span class="comment"># 仅使用前1万个字符训练模型</span></span><br><span class="line">    corpus_chars = corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line">    idx_to_char = list(set(corpus_chars))</span><br><span class="line">    char_to_idx = dict([(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)])</span><br><span class="line">    vocab_size = len(char_to_idx)</span><br><span class="line">    print(vocab_size)</span><br><span class="line">    print(char_to_idx)</span><br><span class="line"></span><br><span class="line">    x = [(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)]</span><br><span class="line">    print(x[:<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集中的每个字符转化成索引</span></span><br><span class="line">    corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line">    <span class="comment"># 打印前20个字符机器对应的索引</span></span><br><span class="line">    sample = corpus_indices[:<span class="number">20</span>]</span><br><span class="line">    print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line">    print(<span class="string">'indices:'</span>, sample)</span><br><span class="line">    <span class="keyword">return</span> idx_to_char, char_to_idx, corpus_indices, vocab_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># num_steps: 6</span></span><br><span class="line"><span class="comment"># corpus_indices:</span></span><br><span class="line"><span class="comment"># [0,1,2,...,29]</span></span><br><span class="line"><span class="comment"># [[0,...,5],[6,..,11],[12,...,17],[18,...,23],[24,...,29]]</span></span><br><span class="line"><span class="comment"># example_indices: [0,1,2,3,4]</span></span><br><span class="line"><span class="comment"># shuffle example_indices: [...]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    <span class="comment"># 减1是因为输出的索引是相应输入的索引加1</span></span><br><span class="line">    <span class="comment"># [0,1,2,3]输出标签应为[1,2,3,4]</span></span><br><span class="line">    <span class="comment"># 为了防止溢出</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps</span><br><span class="line">    print(num_examples)</span><br><span class="line">    epoch_size = num_examples // batch_size</span><br><span class="line">    example_indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(example_indices)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回从pos开始的长为num_steps的序列</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span><span class="params">(pos)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        <span class="comment"># 每次读取batch_size个随机样本</span></span><br><span class="line">        i = i * batch_size</span><br><span class="line">        batch_indices = example_indices[i:i + batch_size]</span><br><span class="line">        X = [_data(j * num_steps) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j * num_steps + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> nd.array(X, ctx), nd.array(Y, ctx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    corpus_indices = nd.array(corpus_indices, ctx=ctx)</span><br><span class="line">    data_len = len(corpus_indices)</span><br><span class="line">    batch_len = data_len // batch_size</span><br><span class="line">    indices = corpus_indices[<span class="number">0</span>:batch_size * batch_len].reshape((batch_size, batch_len))</span><br><span class="line">    print(indices)</span><br><span class="line">    epoch_size = (batch_len - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i:i + num_steps]</span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>:i + num_steps + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_onehot</span><span class="params">(X, size)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [nd.one_hot(x, size) <span class="keyword">for</span> x <span class="keyword">in</span> X.T]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ctx = mx.gpu()</span><br><span class="line">        _ = nd.zeros((<span class="number">1</span>,), ctx=ctx)</span><br><span class="line">    <span class="keyword">except</span> mx.base.MXNetError:</span><br><span class="line">        ctx = mx.cpu()</span><br><span class="line">    <span class="keyword">return</span> ctx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># my_seq = range(30)</span></span><br><span class="line"><span class="comment"># for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):</span></span><br><span class="line"><span class="comment">#     print('X: ', X, '\nY:', Y, '\n')</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> nd.random.normal(scale=<span class="number">0.01</span>, shape=shape, ctx=ctx)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    W_xh = _one((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = _one((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = nd.zeros(num_hiddens, ctx=ctx)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = nd.zeros(num_outputs, ctx=ctx)</span><br><span class="line">    <span class="comment"># 附上梯度</span></span><br><span class="line">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.attach_grad()</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rnn_state</span><span class="params">(batch_size, num_hiddens, ctx)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn</span><span class="params">(inputs, state, params)</span>:</span></span><br><span class="line">    <span class="comment"># inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h)</span><br><span class="line">        Y = nd.dot(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H, )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn</span><span class="params">(prefix, num_chars, rnn, params, init_rnn_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx)</span>:</span></span><br><span class="line">    state = init_rnn_state(<span class="number">1</span>, num_hiddens, ctx)</span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars+len(prefix) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 将上一实践步的输出作为当前时间步的输入</span></span><br><span class="line">        X = to_onehot(nd.array([output[<span class="number">-1</span>]], ctx=ctx), vocab_size)</span><br><span class="line">        <span class="comment"># 计算输出和更新隐藏状态</span></span><br><span class="line">        Y, state = rnn(X, state, params)</span><br><span class="line">        <span class="comment"># 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix)<span class="number">-1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t+<span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(int(Y[<span class="number">0</span>].argmax(axis=<span class="number">1</span>).asscalar()))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(params, theta, ctx)</span>:</span></span><br><span class="line">    norm = nd.array(<span class="number">0</span>, ctx)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        norm += (param.grad**<span class="number">2</span>).sum()</span><br><span class="line">    norm = norm.sqrt().asscalar()</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        <span class="comment"># print("param:", param)</span></span><br><span class="line">        param[:] = param - lr * param.grad / batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn</span><span class="params">(rnn, get_params, init_rnn_state, num_hiddens,</span></span></span><br><span class="line"><span class="function"><span class="params">                          vocab_size, ctx, corpus_indices, idx_to_char,</span></span></span><br><span class="line"><span class="function"><span class="params">                          char_to_idx, is_random_iter, num_epochs, num_steps,</span></span></span><br><span class="line"><span class="function"><span class="params">                          lr, clipping_theta, batch_size, pred_peroid,</span></span></span><br><span class="line"><span class="function"><span class="params">                          pred_len, prefixes)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_random_iter:</span><br><span class="line">        data_iter_fn = data_iter_random</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data_iter_fn = data_iter_consecutive</span><br><span class="line"></span><br><span class="line">    params = get_params</span><br><span class="line">    loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_random_iter:</span><br><span class="line">            state = init_rnn_state(batch_size, num_hiddens, ctx)</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)</span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> is_random_iter:</span><br><span class="line">                state = init_rnn_state(batch_size, num_hiddens, ctx)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 否则需要使用detach函数从计算图分离隐藏状态</span></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                inputs = to_onehot(X, vocab_size)</span><br><span class="line">                <span class="comment"># outputs有num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">                outputs, state = rnn(inputs, state, params)</span><br><span class="line">                <span class="comment"># 拼接之后形状为(num_steps*batch_size, vocab_size)</span></span><br><span class="line">                outputs = nd.concat(*outputs, dim=<span class="number">0</span>)</span><br><span class="line">                <span class="comment"># Y的形状是(batch_size, num_steps)，转置后再变成长度为</span></span><br><span class="line">                <span class="comment"># batch * num_steps的向量，这样跟输出的行一一对应</span></span><br><span class="line">                y = Y.T.reshape((<span class="number">-1</span>,))</span><br><span class="line">                <span class="comment"># 使用交叉熵损失计算平均分类误差</span></span><br><span class="line">                l = loss(outputs, y).mean()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(params, clipping_theta, ctx)  <span class="comment"># 裁剪梯度</span></span><br><span class="line">            sgd(params, lr, <span class="number">1</span>)  <span class="comment"># 因为误差已经去过均值，梯度不用再做平均，所以这里为1</span></span><br><span class="line">            l_sum += l.asscalar() * y.size</span><br><span class="line">            n += y.size</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_peroid == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum/n), time.time()-start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">' -'</span>, predict_rnn(</span><br><span class="line">                    prefix, pred_len, rnn, params, init_rnn_state,</span><br><span class="line">                    num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">idx_to_char, char_to_idx, corpus_indices, vocab_size = load_data_jay_lyrics()</span><br><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">ctx = try_gpu()</span><br><span class="line">print(<span class="string">'will use'</span>, ctx)</span><br><span class="line">X = nd.arange(<span class="number">10</span>).reshape((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">state = init_rnn_state(X.shape[<span class="number">0</span>], num_hiddens, ctx)</span><br><span class="line">inputs = to_onehot(X.as_in_context(ctx), vocab_size)</span><br><span class="line">print(inputs)</span><br><span class="line"></span><br><span class="line">params = get_params()</span><br><span class="line">outputs, state_new = rnn(inputs, state, params)</span><br><span class="line">print(len(outputs), outputs[<span class="number">0</span>].shape, state_new[<span class="number">0</span>].shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">predict_txt = predict_rnn(<span class="string">'分开'</span>, <span class="number">10</span>, rnn, params, init_rnn_state, num_hiddens, vocab_size,</span><br><span class="line">            ctx, idx_to_char, char_to_idx)</span><br><span class="line"></span><br><span class="line">print(predict_txt)</span><br></pre></td></tr></tbody></table></figure><p><strong>测试</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">will use cpu(<span class="number">0</span>)</span><br><span class="line">[<span class="number">22</span>:<span class="number">52</span>:<span class="number">44</span>] c:\jenkins\workspace\mxnet-tag\mxnet\src\imperative\./imperative_utils.h:<span class="number">91</span>: GPU support <span class="keyword">is</span> disabled. Compile MXNet <span class="keyword">with</span> USE_CUDA=<span class="number">1</span> to enable GPU support.</span><br><span class="line">[</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x1027 @cpu(<span class="number">0</span>)&gt;,</span><br><span class="line">[[<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x1027 @cpu(<span class="number">0</span>)&gt;,</span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x1027 @cpu(<span class="number">0</span>)&gt;,</span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x1027 @cpu(<span class="number">0</span>)&gt;,</span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> ... <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">&lt;NDArray <span class="number">2</span>x1027 @cpu(<span class="number">0</span>)&gt;]</span><br><span class="line"><span class="number">5</span> (<span class="number">2</span>, <span class="number">1027</span>) (<span class="number">2</span>, <span class="number">256</span>)</span><br><span class="line">分开缝武干运欢选文峡颁铜</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_jay_lyrics</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(<span class="string">'./data/jaychou_lyrics.txt.zip'</span>) <span class="keyword">as</span> zin:</span><br><span class="line">        <span class="keyword">with</span> zin.open(<span class="string">'jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            corpus_chars = f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(corpus_chars[:<span class="number">40</span>])</span><br><span class="line"></span><br><span class="line">    corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">    <span class="comment"># 仅使用前1万个字符训练模型</span></span><br><span class="line">    corpus_chars = corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line">    idx_to_char = list(set(corpus_chars))</span><br><span class="line">    char_to_idx = dict([(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)])</span><br><span class="line">    vocab_size = len(char_to_idx)</span><br><span class="line">    print(vocab_size)</span><br><span class="line">    print(char_to_idx)</span><br><span class="line"></span><br><span class="line">    x = [(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)]</span><br><span class="line">    print(x[:<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集中的每个字符转化成索引</span></span><br><span class="line">    corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line">    <span class="comment"># 打印前20个字符机器对应的索引</span></span><br><span class="line">    sample = corpus_indices[:<span class="number">20</span>]</span><br><span class="line">    print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line">    print(<span class="string">'indices:'</span>, sample)</span><br><span class="line">    <span class="keyword">return</span> idx_to_char, char_to_idx, corpus_indices, vocab_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># num_steps: 6</span></span><br><span class="line"><span class="comment"># corpus_indices:</span></span><br><span class="line"><span class="comment"># [0,1,2,...,29]</span></span><br><span class="line"><span class="comment"># [[0,...,5],[6,..,11],[12,...,17],[18,...,23],[24,...,29]]</span></span><br><span class="line"><span class="comment"># example_indices: [0,1,2,3,4]</span></span><br><span class="line"><span class="comment"># shuffle example_indices: [...]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    <span class="comment"># 减1是因为输出的索引是相应输入的索引加1</span></span><br><span class="line">    <span class="comment"># [0,1,2,3]输出标签应为[1,2,3,4]</span></span><br><span class="line">    <span class="comment"># 为了防止溢出</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># print(num_examples)</span></span><br><span class="line">    epoch_size = num_examples // batch_size</span><br><span class="line">    example_indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(example_indices)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回从pos开始的长为num_steps的序列</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span><span class="params">(pos)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        <span class="comment"># 每次读取batch_size个随机样本</span></span><br><span class="line">        i = i * batch_size</span><br><span class="line">        batch_indices = example_indices[i:i + batch_size]</span><br><span class="line">        X = [_data(j * num_steps) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j * num_steps + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> nd.array(X, ctx), nd.array(Y, ctx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    corpus_indices = nd.array(corpus_indices, ctx=ctx)</span><br><span class="line">    data_len = len(corpus_indices)</span><br><span class="line">    batch_len = data_len // batch_size</span><br><span class="line">    indices = corpus_indices[<span class="number">0</span>:batch_size * batch_len].reshape((batch_size, batch_len))</span><br><span class="line">    <span class="comment"># print(indices)</span></span><br><span class="line">    epoch_size = (batch_len - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i:i + num_steps]</span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>:i + num_steps + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_onehot</span><span class="params">(X, size)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [nd.one_hot(x, size) <span class="keyword">for</span> x <span class="keyword">in</span> X.T]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ctx = mx.gpu()</span><br><span class="line">        _ = nd.zeros((<span class="number">1</span>,), ctx=ctx)</span><br><span class="line">    <span class="keyword">except</span> mx.base.MXNetError:</span><br><span class="line">        ctx = mx.cpu()</span><br><span class="line">    <span class="keyword">return</span> ctx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># my_seq = range(30)</span></span><br><span class="line"><span class="comment"># for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):</span></span><br><span class="line"><span class="comment">#     print('X: ', X, '\nY:', Y, '\n')</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> nd.random.normal(scale=<span class="number">0.01</span>, shape=shape, ctx=ctx)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    W_xh = _one((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = _one((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = nd.zeros(num_hiddens, ctx=ctx)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = nd.zeros(num_outputs, ctx=ctx)</span><br><span class="line">    <span class="comment"># 附上梯度</span></span><br><span class="line">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.attach_grad()</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rnn_state</span><span class="params">(batch_size, num_hiddens, ctx)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn</span><span class="params">(inputs, state, params)</span>:</span></span><br><span class="line">    <span class="comment"># inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h)</span><br><span class="line">        Y = nd.dot(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H,)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn</span><span class="params">(prefix, num_chars, rnn, params, init_rnn_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx)</span>:</span></span><br><span class="line">    state = init_rnn_state(<span class="number">1</span>, num_hiddens, ctx)</span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 将上一实践步的输出作为当前时间步的输入</span></span><br><span class="line">        X = to_onehot(nd.array([output[<span class="number">-1</span>]], ctx=ctx), vocab_size)</span><br><span class="line">        <span class="comment"># 计算输出和更新隐藏状态</span></span><br><span class="line">        Y, state = rnn(X, state, params)</span><br><span class="line">        <span class="comment"># 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(int(Y[<span class="number">0</span>].argmax(axis=<span class="number">1</span>).asscalar()))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(params, theta, ctx)</span>:</span></span><br><span class="line">    norm = nd.array([<span class="number">0</span>], ctx)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        norm += (param.grad ** <span class="number">2</span>).sum()</span><br><span class="line">    norm = norm.sqrt().asscalar()</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        <span class="comment"># print("param:", param)</span></span><br><span class="line">        param[:] = param - lr * param.grad / batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn</span><span class="params">(rnn, get_params, init_rnn_state, num_hiddens,</span></span></span><br><span class="line"><span class="function"><span class="params">                          vocab_size, ctx, corpus_indices, idx_to_char,</span></span></span><br><span class="line"><span class="function"><span class="params">                          char_to_idx, is_random_iter, num_epochs, num_steps,</span></span></span><br><span class="line"><span class="function"><span class="params">                          lr, clipping_theta, batch_size, pred_peroid,</span></span></span><br><span class="line"><span class="function"><span class="params">                          pred_len, prefixes)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_random_iter:</span><br><span class="line">        data_iter_fn = data_iter_random</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data_iter_fn = data_iter_consecutive</span><br><span class="line"></span><br><span class="line">    params = get_params()</span><br><span class="line">    loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_random_iter:</span><br><span class="line">            state = init_rnn_state(batch_size, num_hiddens, ctx)</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)</span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> is_random_iter:</span><br><span class="line">                state = init_rnn_state(batch_size, num_hiddens, ctx)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 否则需要使用detach函数从计算图分离隐藏状态</span></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                inputs = to_onehot(X, vocab_size)</span><br><span class="line">                <span class="comment"># outputs有num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">                outputs, state = rnn(inputs, state, params)</span><br><span class="line">                <span class="comment"># 拼接之后形状为(num_steps*batch_size, vocab_size)</span></span><br><span class="line">                outputs = nd.concat(*outputs, dim=<span class="number">0</span>)</span><br><span class="line">                <span class="comment"># Y的形状是(batch_size, num_steps)，转置后再变成长度为</span></span><br><span class="line">                <span class="comment"># batch * num_steps的向量，这样跟输出的行一一对应</span></span><br><span class="line">                y = Y.T.reshape((<span class="number">-1</span>,))</span><br><span class="line">                <span class="comment"># 使用交叉熵损失计算平均分类误差</span></span><br><span class="line">                l = loss(outputs, y).mean()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(params, clipping_theta, ctx)  <span class="comment"># 裁剪梯度</span></span><br><span class="line">            sgd(params, lr, <span class="number">1</span>)  <span class="comment"># 因为误差已经去过均值，梯度不用再做平均</span></span><br><span class="line">            l_sum += l.asscalar() * y.size</span><br><span class="line">            n += y.size</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_peroid == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">' -'</span>, predict_rnn(</span><br><span class="line">                    prefix, pred_len, rnn, params, init_rnn_state,</span><br><span class="line">                    num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">idx_to_char, char_to_idx, corpus_indices, vocab_size = load_data_jay_lyrics()</span><br><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">ctx = try_gpu()</span><br><span class="line">print(<span class="string">'will use'</span>, ctx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># X = nd.arange(10).reshape((2, 5))</span></span><br><span class="line"><span class="comment"># state = init_rnn_state(X.shape[0], num_hiddens, ctx)</span></span><br><span class="line"><span class="comment"># inputs = to_onehot(X.as_in_context(ctx), vocab_size)</span></span><br><span class="line"><span class="comment"># print(inputs)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># params = get_params()</span></span><br><span class="line"><span class="comment"># outputs, state_new = rnn(inputs, state, params)</span></span><br><span class="line"><span class="comment"># print(len(outputs), outputs[0].shape, state_new[0].shape)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># predict_txt = predict_rnn('分开', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,</span></span><br><span class="line"><span class="comment">#             ctx, idx_to_char, char_to_idx)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># print(predict_txt)</span></span><br><span class="line"></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_peroid, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class="line">                      vocab_size, ctx, corpus_indices, idx_to_char,</span><br><span class="line">                      char_to_idx, <span class="literal">False</span>, num_epochs, num_steps, lr,</span><br><span class="line">                      clipping_theta, batch_size, pred_peroid, pred_len, prefixes)</span><br></pre></td></tr></tbody></table></figure><p><strong>随机采样训练</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">epoch <span class="number">50</span>, perplexity <span class="number">72.632877</span>, time <span class="number">3.05</span> sec</span><br><span class="line"> - 分开 我不么 一颗两 三颗四 三颗四 三颗四 三颗四 三颗四 三颗四 三颗四 三颗四 三颗四 三颗四 三</span><br><span class="line"> - 不分开 我有就 一颗两 三颗四 三颗四 三颗四 三颗四 三颗四 三颗四 三颗四 三颗四 三颗四 三颗四 三</span><br><span class="line">epoch <span class="number">100</span>, perplexity <span class="number">10.219814</span>, time <span class="number">3.04</span> sec</span><br><span class="line"> - 分开 一只用 一步两步三步四步望著天 看星星 一颗两颗三步四步望著天 看星星 一颗两颗三步四步望著天 看</span><br><span class="line"> - 不分开吗 我不能再生你 世知不觉 你已经离开棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮</span><br><span class="line">epoch <span class="number">150</span>, perplexity <span class="number">2.869053</span>, time <span class="number">4.22</span> sec</span><br><span class="line"> - 分开 一只两颗 沙碎上 在数我抬起 一定看停留义 为 随底开都防边 我这想这担我 选你这种队 一样过人</span><br><span class="line"> - 不分开吗 我后能爸 你打我早 这样透 痛数我抬起头 有话去对医药箱说 别怪我 别怪我 说你怎么 每实伦中的</span><br><span class="line">epoch <span class="number">200</span>, perplexity <span class="number">1.593117</span>, time <span class="number">3.06</span> sec</span><br><span class="line"> - 分开不会见因悲  却过云层坦堡 但只了你和汉堡 我想要你的微笑每天都能看到  我知道这里很美但家乡的你更</span><br><span class="line"> - 不分开吗把的胖女巫 用拉丁文念咒语啦啦呜 她养的黑猫笑起来像哭 啦啦啦呜 再来的外屋前红 茶说就我妈绕事</span><br><span class="line">epoch <span class="number">250</span>, perplexity <span class="number">1.309647</span>, time <span class="number">3.67</span> sec</span><br><span class="line"> - 分开球 不因在这样的寻瓣酱 我对著黑白照片开始想像 爸和妈当年的模样 说著一口吴侬软语的姑娘缓缓走了外滩</span><br><span class="line"> - 不分开吗 然后将过去 慢慢温习 让我爱上你 那场悲剧 是你完美演出的一场戏 宁愿心碎哭泣 再狠狠忘记 你爱</span><br></pre></td></tr></tbody></table></figure><p><strong>相邻采样训练</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">epoch <span class="number">50</span>, perplexity <span class="number">63.569066</span>, time <span class="number">3.13</span> sec</span><br><span class="line"> - 分开 我想要这爱 有不的美 全人了双截  哼知你 别子我有 我想要这想 有不的美 全人了人 我有多的可写</span><br><span class="line"> - 不分开 我想要这爱 有不的美 全人了双截  哼知你 别子我有 我想要这想 有不的美 全人了人 我有多的可写</span><br><span class="line">epoch <span class="number">100</span>, perplexity <span class="number">7.389908</span>, time <span class="number">3.13</span> sec</span><br><span class="line"> - 分开 一颗我 别怪我 别你怎么都对我 甩开球我满腔的怒火 我想揍你已经很久 别想躲 说你眼睛看着我 别发</span><br><span class="line"> - 不分开觉 你成我 别怪么 它什好睛的片 还地 老子再久了吧? 败不你的黑色幽默 不要 这什么我 戒小说动防</span><br><span class="line">epoch <span class="number">150</span>, perplexity <span class="number">2.130088</span>, time <span class="number">3.54</span> sec</span><br><span class="line"> - 分开 一颗我 印打我 三过怎人面对的怒盒里藏 一场承受年我 一场线最队 除非它乌鸦抢了它的窝 它在灌木丛</span><br><span class="line"> - 不分开觉 你真经很开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生</span><br><span class="line">epoch <span class="number">200</span>, perplexity <span class="number">1.327236</span>, time <span class="number">3.40</span> sec</span><br><span class="line"> - 分开 一候我 谁打我 别你怎么面对我 甩开球我满腔的怒火 我想揍你已经很久 别想躲 说你眼睛看着我 别发</span><br><span class="line"> - 不分开觉 你真经很开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生</span><br><span class="line">epoch <span class="number">250</span>, perplexity <span class="number">1.188282</span>, time <span class="number">3.09</span> sec</span><br><span class="line"> - 分开 一候我 谁怪神枪手 巫师 他念念 有词的 对酋长下诅咒 还我骷髅头 这故事 告诉我 印地安的传说</span><br><span class="line"> - 不分开觉 你想经离开我 不知不觉 我跟了这节奏 后知后觉 又过了觉 迷迷蒙蒙 你给的梦 出现裂缝 隐隐作痛</span><br></pre></td></tr></tbody></table></figure><h2 id="循环神经网络的简洁实现"><a href="#循环神经网络的简洁实现" class="headerlink" title="循环神经网络的简洁实现"></a>循环神经网络的简洁实现</h2><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss, nn, rnn</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, init, nd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_jay_lyrics</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(<span class="string">'./data/jaychou_lyrics.txt.zip'</span>) <span class="keyword">as</span> zin:</span><br><span class="line">        <span class="keyword">with</span> zin.open(<span class="string">'jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            corpus_chars = f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(corpus_chars[:<span class="number">40</span>])</span><br><span class="line"></span><br><span class="line">    corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">    <span class="comment"># 仅使用前1万个字符训练模型</span></span><br><span class="line">    corpus_chars = corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line">    idx_to_char = list(set(corpus_chars))</span><br><span class="line">    char_to_idx = dict([(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)])</span><br><span class="line">    vocab_size = len(char_to_idx)</span><br><span class="line">    print(vocab_size)</span><br><span class="line">    print(char_to_idx)</span><br><span class="line"></span><br><span class="line">    x = [(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)]</span><br><span class="line">    print(x[:<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集中的每个字符转化成索引</span></span><br><span class="line">    corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line">    <span class="comment"># 打印前20个字符机器对应的索引</span></span><br><span class="line">    sample = corpus_indices[:<span class="number">20</span>]</span><br><span class="line">    print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line">    print(<span class="string">'indices:'</span>, sample)</span><br><span class="line">    <span class="keyword">return</span> idx_to_char, char_to_idx, corpus_indices, vocab_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">corpus_indices, char_to_idx, idx_to_char, vocab_size = load_data_jay_lyrics()</span><br><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line">rnn_layer = rnn.RNN(num_hiddens)</span><br><span class="line">rnn_layer.initialize()</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line"><span class="comment"># (隐藏层个数，批量大小，隐藏单元个数)</span></span><br><span class="line">state = rnn_layer.begin_state(batch_size=batch_size)</span><br><span class="line">print(state[<span class="number">0</span>].shape)</span><br><span class="line"></span><br><span class="line">num_steps = <span class="number">35</span></span><br><span class="line"><span class="comment"># (时间步数，批量大小，输入个数)</span></span><br><span class="line">X = nd.random.uniform(shape=(num_steps, batch_size, vocab_size))</span><br><span class="line"><span class="comment"># (时间步数，批量大小，隐藏单元个数)</span></span><br><span class="line">Y, state_new = rnn_layer(X, state)</span><br><span class="line">print(Y.shape, len(state_new), state_new[<span class="number">0</span>].shape)</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">256</span>)</span><br><span class="line">(<span class="number">35</span>, <span class="number">2</span>, <span class="number">256</span>) <span class="number">1</span> (<span class="number">1</span>, <span class="number">2</span>, <span class="number">256</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss, nn, rnn</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, init, nd, gluon</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_jay_lyrics</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(<span class="string">'./data/jaychou_lyrics.txt.zip'</span>) <span class="keyword">as</span> zin:</span><br><span class="line">        <span class="keyword">with</span> zin.open(<span class="string">'jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            corpus_chars = f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(corpus_chars[:<span class="number">40</span>])</span><br><span class="line"></span><br><span class="line">    corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">    <span class="comment"># 仅使用前1万个字符训练模型</span></span><br><span class="line">    corpus_chars = corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line">    idx_to_char = list(set(corpus_chars))</span><br><span class="line">    char_to_idx = dict([(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)])</span><br><span class="line">    vocab_size = len(char_to_idx)</span><br><span class="line">    print(vocab_size)</span><br><span class="line">    print(char_to_idx)</span><br><span class="line"></span><br><span class="line">    x = [(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)]</span><br><span class="line">    print(x[:<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集中的每个字符转化成索引</span></span><br><span class="line">    corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line">    <span class="comment"># 打印前20个字符机器对应的索引</span></span><br><span class="line">    sample = corpus_indices[:<span class="number">20</span>]</span><br><span class="line">    print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line">    print(<span class="string">'indices:'</span>, sample)</span><br><span class="line">    <span class="keyword">return</span> idx_to_char, char_to_idx, corpus_indices, vocab_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_layer, vocab_size, **kwargs)</span>:</span></span><br><span class="line">        super(RNNModel, self).__init__(**kwargs)</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.dense = nn.Dense(vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, state)</span>:</span></span><br><span class="line">        <span class="comment"># 将输入转置成(num_steps, batch_size)后获取one-hot向量表示</span></span><br><span class="line">        <span class="comment"># print(inputs.shape)</span></span><br><span class="line">        X = nd.one_hot(inputs.T, self.vocab_size)</span><br><span class="line">        <span class="comment"># print(X.shape)</span></span><br><span class="line">        Y, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># 全连接层会首先将Y的形状编程(num_steps*batch_size, num_hiddens)，</span></span><br><span class="line">        <span class="comment"># 它的输出为(num_steps*batch_size, vocab_size)</span></span><br><span class="line">        output = self.dense(Y.reshape((<span class="number">-1</span>, Y.shape[<span class="number">-1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span><span class="params">(self, *args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.rnn.begin_state(*args, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn_gluon</span><span class="params">(prefix, num_chars, model, vocab_size, ctx, idx_to_char, char_to_idx)</span>:</span></span><br><span class="line">    <span class="comment"># 使用model的成员函数来初始化隐藏状态</span></span><br><span class="line">    state = model.begin_state(batch_size=<span class="number">1</span>, ctx=ctx)</span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 将上一实践步的输出作为当前时间步的输入</span></span><br><span class="line">        X = nd.array([output[<span class="number">-1</span>]], ctx=ctx).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># print(X.shape)</span></span><br><span class="line">        <span class="comment"># 计算输出和更新隐藏状态</span></span><br><span class="line">        Y, state = model(X, state)</span><br><span class="line">        <span class="comment"># print(Y.shape)</span></span><br><span class="line">        <span class="comment"># 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(int(Y.argmax(axis=<span class="number">1</span>).asscalar()))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ctx = mx.gpu()</span><br><span class="line">        _ = nd.zeros((<span class="number">1</span>,), ctx=ctx)</span><br><span class="line">    <span class="keyword">except</span> mx.base.MXNetError:</span><br><span class="line">        ctx = mx.cpu()</span><br><span class="line">    <span class="keyword">return</span> ctx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># num_steps: 6</span></span><br><span class="line"><span class="comment"># corpus_indices:</span></span><br><span class="line"><span class="comment"># [0,1,2,...,29]</span></span><br><span class="line"><span class="comment"># [[0,...,5],[6,..,11],[12,...,17],[18,...,23],[24,...,29]]</span></span><br><span class="line"><span class="comment"># example_indices: [0,1,2,3,4]</span></span><br><span class="line"><span class="comment"># shuffle example_indices: [...]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    <span class="comment"># 减1是因为输出的索引是相应输入的索引加1</span></span><br><span class="line">    <span class="comment"># [0,1,2,3]输出标签应为[1,2,3,4]</span></span><br><span class="line">    <span class="comment"># 为了防止溢出</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># print(num_examples)</span></span><br><span class="line">    epoch_size = num_examples // batch_size</span><br><span class="line">    example_indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(example_indices)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回从pos开始的长为num_steps的序列</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span><span class="params">(pos)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        <span class="comment"># 每次读取batch_size个随机样本</span></span><br><span class="line">        i = i * batch_size</span><br><span class="line">        batch_indices = example_indices[i:i + batch_size]</span><br><span class="line">        X = [_data(j * num_steps) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j * num_steps + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> nd.array(X, ctx), nd.array(Y, ctx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    corpus_indices = nd.array(corpus_indices, ctx=ctx)</span><br><span class="line">    data_len = len(corpus_indices)</span><br><span class="line">    batch_len = data_len // batch_size</span><br><span class="line">    indices = corpus_indices[<span class="number">0</span>:batch_size * batch_len].reshape((batch_size, batch_len))</span><br><span class="line">    <span class="comment"># print(indices)</span></span><br><span class="line">    epoch_size = (batch_len - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i:i + num_steps]</span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>:i + num_steps + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(params, theta, ctx)</span>:</span></span><br><span class="line">    norm = nd.array([<span class="number">0</span>], ctx)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        norm += (param.grad ** <span class="number">2</span>).sum()</span><br><span class="line">    norm = norm.sqrt().asscalar()</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn_gluon</span><span class="params">(model, num_hiddens, vocab_size, ctx,</span></span></span><br><span class="line"><span class="function"><span class="params">                                corpus_indices, idx_to_char, char_to_idx,</span></span></span><br><span class="line"><span class="function"><span class="params">                                num_epochs, num_steps, lr, clipping_theta,</span></span></span><br><span class="line"><span class="function"><span class="params">                                batch_size, pred_peroid, pred_len, prefixes)</span>:</span></span><br><span class="line">    loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line">    model.initialize(ctx=ctx, force_reinit=<span class="literal">True</span>, init=init.Normal(<span class="number">0.01</span>))</span><br><span class="line">    trainer = gluon.Trainer(model.collect_params(), <span class="string">'sgd'</span>,</span><br><span class="line">                            {<span class="string">'learning_rate'</span>: lr, <span class="string">'momentum'</span>: <span class="number">0</span>, <span class="string">'wd'</span>: <span class="number">0</span>})</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx)</span><br><span class="line">        state = model.begin_state(batch_size=batch_size, ctx=ctx)</span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                s.detach()</span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                output, state = model(X, state)</span><br><span class="line">                y = Y.T.reshape((<span class="number">-1</span>,))</span><br><span class="line">                l = loss(output, y).mean()</span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="comment"># 梯度裁剪</span></span><br><span class="line">            params = [p.data() <span class="keyword">for</span> p <span class="keyword">in</span> model.collect_params().values()]</span><br><span class="line">            grad_clipping(params, clipping_theta, ctx)</span><br><span class="line">            trainer.step(<span class="number">1</span>)  <span class="comment"># 因为已经误差取过均值，梯度不用再做平均</span></span><br><span class="line">            l_sum += l.asscalar() * y.size</span><br><span class="line">            n += y.size</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_peroid == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">'-'</span>, predict_rnn_gluon(prefix, pred_len,</span><br><span class="line">                                             model, vocab_size, idx_to_char, char_to_idx))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ctx = try_gpu()</span><br><span class="line">idx_to_char, char_to_idx, corpus_indices, vocab_size = load_data_jay_lyrics()</span><br><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line">num_steps = <span class="number">35</span></span><br><span class="line">rnn_layer = rnn.RNN(num_hiddens)</span><br><span class="line">rnn_layer.initialize()</span><br><span class="line">model = RNNModel(rnn_layer, vocab_size)</span><br><span class="line">model.initialize(force_reinit=<span class="literal">True</span>, ctx=ctx)</span><br><span class="line"></span><br><span class="line">num_epochs, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line">train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,</span><br><span class="line">                            corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                            num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                            batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></tbody></table></figure><h2 id="门控循环神经网络GRU"><a href="#门控循环神经网络GRU" class="headerlink" title="门控循环神经网络GRU"></a>门控循环神经网络GRU</h2><h3 id="从零开始实现"><a href="#从零开始实现" class="headerlink" title="从零开始实现"></a>从零开始实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd, autograd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> rnn, loss <span class="keyword">as</span> gloss</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_jay_lyrics</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(<span class="string">'./data/jaychou_lyrics.txt.zip'</span>) <span class="keyword">as</span> zin:</span><br><span class="line">        <span class="keyword">with</span> zin.open(<span class="string">'jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            corpus_chars = f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(corpus_chars[:<span class="number">40</span>])</span><br><span class="line"></span><br><span class="line">    corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">    <span class="comment"># 仅使用前1万个字符训练模型</span></span><br><span class="line">    corpus_chars = corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line">    idx_to_char = list(set(corpus_chars))</span><br><span class="line">    char_to_idx = dict([(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)])</span><br><span class="line">    vocab_size = len(char_to_idx)</span><br><span class="line">    print(vocab_size)</span><br><span class="line">    print(char_to_idx)</span><br><span class="line"></span><br><span class="line">    x = [(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)]</span><br><span class="line">    print(x[:<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集中的每个字符转化成索引</span></span><br><span class="line">    corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line">    <span class="comment"># 打印前20个字符机器对应的索引</span></span><br><span class="line">    sample = corpus_indices[:<span class="number">20</span>]</span><br><span class="line">    print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line">    print(<span class="string">'indices:'</span>, sample)</span><br><span class="line">    <span class="keyword">return</span> idx_to_char, char_to_idx, corpus_indices, vocab_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ctx = mx.gpu()</span><br><span class="line">        _ = nd.zeros((<span class="number">1</span>,), ctx=ctx)</span><br><span class="line">    <span class="keyword">except</span> mx.base.MXNetError:</span><br><span class="line">        ctx = mx.cpu()</span><br><span class="line">    <span class="keyword">return</span> ctx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> nd.random.normal(scale=<span class="number">0.01</span>, shape=shape, ctx=ctx)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> _one((num_inputs, num_hiddens)), _one((num_hiddens, num_hiddens)), nd.zeros(num_hiddens, ctx=ctx)</span><br><span class="line"></span><br><span class="line">    W_xz, W_hz, b_z = _three()  <span class="comment"># 更新门参数</span></span><br><span class="line">    W_xr, W_hr, b_r = _three()  <span class="comment"># 重置门参数</span></span><br><span class="line">    W_xh, W_hh, b_h = _three()  <span class="comment"># 候选隐藏状态参数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = nd.zeros(num_outputs, ctx=ctx)</span><br><span class="line">    <span class="comment"># 附上梯度</span></span><br><span class="line">    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.attach_grad()</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_gru_state</span><span class="params">(batch_size, num_hiddens, ctx)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gru</span><span class="params">(inputs, state, params)</span>:</span></span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = nd.sigmoid(nd.dot(X, W_xz) + nd.dot(H, W_hz) + b_z)</span><br><span class="line">        R = nd.sigmoid(nd.dot(X, W_xr) + nd.dot(H, W_hr) + b_r)</span><br><span class="line">        H_tilda = nd.tanh(nd.dot(X, W_xh) + nd.dot(R * H, W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = nd.dot(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H,)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_onehot</span><span class="params">(X, size)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [nd.one_hot(x, size) <span class="keyword">for</span> x <span class="keyword">in</span> X.T]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(params, theta, ctx)</span>:</span></span><br><span class="line">    norm = nd.array([<span class="number">0</span>], ctx)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        norm += (param.grad ** <span class="number">2</span>).sum()</span><br><span class="line">    norm = norm.sqrt().asscalar()</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        <span class="comment"># print("param:", param)</span></span><br><span class="line">        param[:] = param - lr * param.grad / batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># num_steps: 6</span></span><br><span class="line"><span class="comment"># corpus_indices:</span></span><br><span class="line"><span class="comment"># [0,1,2,...,29]</span></span><br><span class="line"><span class="comment"># [[0,...,5],[6,..,11],[12,...,17],[18,...,23],[24,...,29]]</span></span><br><span class="line"><span class="comment"># example_indices: [0,1,2,3,4]</span></span><br><span class="line"><span class="comment"># shuffle example_indices: [...]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    <span class="comment"># 减1是因为输出的索引是相应输入的索引加1</span></span><br><span class="line">    <span class="comment"># [0,1,2,3]输出标签应为[1,2,3,4]</span></span><br><span class="line">    <span class="comment"># 为了防止溢出</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># print(num_examples)</span></span><br><span class="line">    epoch_size = num_examples // batch_size</span><br><span class="line">    example_indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(example_indices)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回从pos开始的长为num_steps的序列</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span><span class="params">(pos)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        <span class="comment"># 每次读取batch_size个随机样本</span></span><br><span class="line">        i = i * batch_size</span><br><span class="line">        batch_indices = example_indices[i:i + batch_size]</span><br><span class="line">        X = [_data(j * num_steps) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j * num_steps + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> nd.array(X, ctx), nd.array(Y, ctx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    corpus_indices = nd.array(corpus_indices, ctx=ctx)</span><br><span class="line">    data_len = len(corpus_indices)</span><br><span class="line">    batch_len = data_len // batch_size</span><br><span class="line">    indices = corpus_indices[<span class="number">0</span>:batch_size * batch_len].reshape((batch_size, batch_len))</span><br><span class="line">    <span class="comment"># print(indices)</span></span><br><span class="line">    epoch_size = (batch_len - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i:i + num_steps]</span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>:i + num_steps + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn</span><span class="params">(prefix, num_chars, rnn, params, init_rnn_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx)</span>:</span></span><br><span class="line">    state = init_rnn_state(<span class="number">1</span>, num_hiddens, ctx)</span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 将上一实践步的输出作为当前时间步的输入</span></span><br><span class="line">        X = to_onehot(nd.array([output[<span class="number">-1</span>]], ctx=ctx), vocab_size)</span><br><span class="line">        <span class="comment"># 计算输出和更新隐藏状态</span></span><br><span class="line">        Y, state = rnn(X, state, params)</span><br><span class="line">        <span class="comment"># 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(int(Y[<span class="number">0</span>].argmax(axis=<span class="number">1</span>).asscalar()))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn</span><span class="params">(rnn, get_params, init_rnn_state, num_hiddens,</span></span></span><br><span class="line"><span class="function"><span class="params">                          vocab_size, ctx, corpus_indices, idx_to_char,</span></span></span><br><span class="line"><span class="function"><span class="params">                          char_to_idx, is_random_iter, num_epochs, num_steps,</span></span></span><br><span class="line"><span class="function"><span class="params">                          lr, clipping_theta, batch_size, pred_peroid,</span></span></span><br><span class="line"><span class="function"><span class="params">                          pred_len, prefixes)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_random_iter:</span><br><span class="line">        data_iter_fn = data_iter_random</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data_iter_fn = data_iter_consecutive</span><br><span class="line"></span><br><span class="line">    params = get_params()</span><br><span class="line">    loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_random_iter:</span><br><span class="line">            state = init_rnn_state(batch_size, num_hiddens, ctx)</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)</span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> is_random_iter:</span><br><span class="line">                state = init_rnn_state(batch_size, num_hiddens, ctx)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 否则需要使用detach函数从计算图分离隐藏状态</span></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                inputs = to_onehot(X, vocab_size)</span><br><span class="line">                <span class="comment"># outputs有num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">                outputs, state = rnn(inputs, state, params)</span><br><span class="line">                <span class="comment"># 拼接之后形状为(num_steps*batch_size, vocab_size)</span></span><br><span class="line">                outputs = nd.concat(*outputs, dim=<span class="number">0</span>)</span><br><span class="line">                <span class="comment"># Y的形状是(batch_size, num_steps)，转置后再变成长度为</span></span><br><span class="line">                <span class="comment"># batch * num_steps的向量，这样跟输出的行一一对应</span></span><br><span class="line">                y = Y.T.reshape((<span class="number">-1</span>,))</span><br><span class="line">                <span class="comment"># 使用交叉熵损失计算平均分类误差</span></span><br><span class="line">                l = loss(outputs, y).mean()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(params, clipping_theta, ctx)  <span class="comment"># 裁剪梯度</span></span><br><span class="line">            sgd(params, lr, <span class="number">1</span>)  <span class="comment"># 因为误差已经去过均值，梯度不用再做平均</span></span><br><span class="line">            l_sum += l.asscalar() * y.size</span><br><span class="line">            n += y.size</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_peroid == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">' -'</span>, predict_rnn(</span><br><span class="line">                    prefix, pred_len, rnn, params, init_rnn_state,</span><br><span class="line">                    num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">idx_to_char, char_to_idx, corpus_indices, vocab_size = load_data_jay_lyrics()</span><br><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">ctx = try_gpu()</span><br><span class="line"></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line">train_and_predict_rnn(gru, get_params, init_gru_state, num_hiddens,</span><br><span class="line">                      vocab_size, ctx, corpus_indices, idx_to_char,</span><br><span class="line">                      char_to_idx, <span class="literal">False</span>, num_epochs, num_steps, lr,</span><br><span class="line">                      clipping_theta, batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">epoch <span class="number">40</span>, perplexity <span class="number">152.503488</span>, time <span class="number">3.45</span> sec</span><br><span class="line"> - 分开 我想你的让我不想想想想想你想你想想想想你想你想想想想你想你想想想想你想你想想想想你想你想想想想你想</span><br><span class="line"> - 不分开 我想你的让我不想想想想想你想你想想想想你想你想想想想你想你想想想想你想你想想想想你想你想想想想你想</span><br><span class="line">epoch <span class="number">80</span>, perplexity <span class="number">32.785765</span>, time <span class="number">3.46</span> sec</span><br><span class="line"> - 分开 一直我 别子我 别你的手 快果我有 你不了 我不要觉 我不要再想 我不要再想 我不要再想 我不要再</span><br><span class="line"> - 不分开 我想要这样 我不要再想 我不能再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我</span><br><span class="line">epoch <span class="number">120</span>, perplexity <span class="number">5.800570</span>, time <span class="number">3.85</span> sec</span><br><span class="line"> - 分开 我想要这样牵着你的手不放开 爱可不可以简简单单没有伤害 你 靠着我的肩膀 你 在我胸口睡著 像这样</span><br><span class="line"> - 不分开 你是我怕开睡是不知不想多  我知道这里很美但像乡的你更美剧过我想要你 我想我的睡有 你 在我胸口睡</span><br><span class="line">epoch <span class="number">160</span>, perplexity <span class="number">1.822375</span>, time <span class="number">3.63</span> sec</span><br><span class="line"> - 分开 我想要你的微笑每天都能看到  我知道这里很美但家乡的你更美走过我只想要你 陪我去吃汉堡  说穿了其</span><br><span class="line"> - 不分开 整个过离开离我不开 我不能受力 我不要再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我</span><br></pre></td></tr></tbody></table></figure><h3 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss, nn, rnn</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, init, nd, gluon</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_jay_lyrics</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(<span class="string">'./data/jaychou_lyrics.txt.zip'</span>) <span class="keyword">as</span> zin:</span><br><span class="line">        <span class="keyword">with</span> zin.open(<span class="string">'jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            corpus_chars = f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="comment"># print(corpus_chars[:40])</span></span><br><span class="line"></span><br><span class="line">    corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">    <span class="comment"># 仅使用前1万个字符训练模型</span></span><br><span class="line">    corpus_chars = corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line">    idx_to_char = list(set(corpus_chars))</span><br><span class="line">    char_to_idx = dict([(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)])</span><br><span class="line">    vocab_size = len(char_to_idx)</span><br><span class="line">    <span class="comment"># print(vocab_size)</span></span><br><span class="line">    <span class="comment"># print(char_to_idx)</span></span><br><span class="line"></span><br><span class="line">    x = [(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)]</span><br><span class="line">    <span class="comment"># print(x[:20])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集中的每个字符转化成索引</span></span><br><span class="line">    corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line">    <span class="comment"># 打印前20个字符机器对应的索引</span></span><br><span class="line">    sample = corpus_indices[:<span class="number">20</span>]</span><br><span class="line">    print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line">    print(<span class="string">'indices:'</span>, sample)</span><br><span class="line">    <span class="keyword">return</span> idx_to_char, char_to_idx, corpus_indices, vocab_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_layer, vocab_size, **kwargs)</span>:</span></span><br><span class="line">        super(RNNModel, self).__init__(**kwargs)</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.dense = nn.Dense(vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, state)</span>:</span></span><br><span class="line">        <span class="comment"># 将输入转置成(num_steps, batch_size)后获取one-hot向量表示</span></span><br><span class="line">        <span class="comment"># print(inputs.shape)</span></span><br><span class="line">        X = nd.one_hot(inputs.T, self.vocab_size)</span><br><span class="line">        <span class="comment"># print(X.shape)</span></span><br><span class="line">        Y, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># 全连接层会首先将Y的形状编程(num_steps*batch_size, num_hiddens)，</span></span><br><span class="line">        <span class="comment"># 它的输出为(num_steps*batch_size, vocab_size)</span></span><br><span class="line">        output = self.dense(Y.reshape((<span class="number">-1</span>, Y.shape[<span class="number">-1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span><span class="params">(self, *args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.rnn.begin_state(*args, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn_gluon</span><span class="params">(prefix, num_chars, model, vocab_size, ctx, idx_to_char, char_to_idx)</span>:</span></span><br><span class="line">    <span class="comment"># 使用model的成员函数来初始化隐藏状态</span></span><br><span class="line">    state = model.begin_state(batch_size=<span class="number">1</span>, ctx=ctx)</span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 将上一实践步的输出作为当前时间步的输入</span></span><br><span class="line">        X = nd.array([output[<span class="number">-1</span>]], ctx=ctx).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># print(X.shape)</span></span><br><span class="line">        <span class="comment"># 计算输出和更新隐藏状态</span></span><br><span class="line">        Y, state = model(X, state)</span><br><span class="line">        <span class="comment"># print(Y.shape)</span></span><br><span class="line">        <span class="comment"># 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(int(Y.argmax(axis=<span class="number">1</span>).asscalar()))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ctx = mx.gpu()</span><br><span class="line">        _ = nd.zeros((<span class="number">1</span>,), ctx=ctx)</span><br><span class="line">    <span class="keyword">except</span> mx.base.MXNetError:</span><br><span class="line">        ctx = mx.cpu()</span><br><span class="line">    <span class="keyword">return</span> ctx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># num_steps: 6</span></span><br><span class="line"><span class="comment"># corpus_indices:</span></span><br><span class="line"><span class="comment"># [0,1,2,...,29]</span></span><br><span class="line"><span class="comment"># [[0,...,5],[6,..,11],[12,...,17],[18,...,23],[24,...,29]]</span></span><br><span class="line"><span class="comment"># example_indices: [0,1,2,3,4]</span></span><br><span class="line"><span class="comment"># shuffle example_indices: [...]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    <span class="comment"># 减1是因为输出的索引是相应输入的索引加1</span></span><br><span class="line">    <span class="comment"># [0,1,2,3]输出标签应为[1,2,3,4]</span></span><br><span class="line">    <span class="comment"># 为了防止溢出</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># print(num_examples)</span></span><br><span class="line">    epoch_size = num_examples // batch_size</span><br><span class="line">    example_indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(example_indices)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回从pos开始的长为num_steps的序列</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span><span class="params">(pos)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        <span class="comment"># 每次读取batch_size个随机样本</span></span><br><span class="line">        i = i * batch_size</span><br><span class="line">        batch_indices = example_indices[i:i + batch_size]</span><br><span class="line">        X = [_data(j * num_steps) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j * num_steps + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> nd.array(X, ctx), nd.array(Y, ctx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    corpus_indices = nd.array(corpus_indices, ctx=ctx)</span><br><span class="line">    data_len = len(corpus_indices)</span><br><span class="line">    batch_len = data_len // batch_size</span><br><span class="line">    indices = corpus_indices[<span class="number">0</span>:batch_size * batch_len].reshape((batch_size, batch_len))</span><br><span class="line">    <span class="comment"># print(indices)</span></span><br><span class="line">    epoch_size = (batch_len - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i:i + num_steps]</span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>:i + num_steps + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(params, theta, ctx)</span>:</span></span><br><span class="line">    norm = nd.array([<span class="number">0</span>], ctx)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        norm += (param.grad ** <span class="number">2</span>).sum()</span><br><span class="line">    norm = norm.sqrt().asscalar()</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn_gluon</span><span class="params">(model, num_hiddens, vocab_size, ctx,</span></span></span><br><span class="line"><span class="function"><span class="params">                                corpus_indices, idx_to_char, char_to_idx,</span></span></span><br><span class="line"><span class="function"><span class="params">                                num_epochs, num_steps, lr, clipping_theta,</span></span></span><br><span class="line"><span class="function"><span class="params">                                batch_size, pred_peroid, pred_len, prefixes)</span>:</span></span><br><span class="line">    loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line">    model.initialize(ctx=ctx, force_reinit=<span class="literal">True</span>, init=init.Normal(<span class="number">0.01</span>))</span><br><span class="line">    trainer = gluon.Trainer(model.collect_params(), <span class="string">'sgd'</span>,</span><br><span class="line">                            {<span class="string">'learning_rate'</span>: lr, <span class="string">'momentum'</span>: <span class="number">0</span>, <span class="string">'wd'</span>: <span class="number">0</span>})</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx)</span><br><span class="line">        state = model.begin_state(batch_size=batch_size, ctx=ctx)</span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                s.detach()</span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                output, state = model(X, state)</span><br><span class="line">                y = Y.T.reshape((<span class="number">-1</span>,))</span><br><span class="line">                l = loss(output, y).mean()</span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="comment"># 梯度裁剪</span></span><br><span class="line">            params = [p.data() <span class="keyword">for</span> p <span class="keyword">in</span> model.collect_params().values()]</span><br><span class="line">            grad_clipping(params, clipping_theta, ctx)</span><br><span class="line">            trainer.step(<span class="number">1</span>)  <span class="comment"># 因为已经误差取过均值，梯度不用再做平均</span></span><br><span class="line">            l_sum += l.asscalar() * y.size</span><br><span class="line">            n += y.size</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_peroid == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">'-'</span>, predict_rnn_gluon(prefix, pred_len,</span><br><span class="line">                                             model, vocab_size, ctx, idx_to_char, char_to_idx))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ctx = try_gpu()</span><br><span class="line">idx_to_char, char_to_idx, corpus_indices, vocab_size = load_data_jay_lyrics()</span><br><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line">num_steps = <span class="number">35</span></span><br><span class="line">gru_layer = rnn.GRU(num_hiddens)</span><br><span class="line">gru_layer.initialize()</span><br><span class="line">model = RNNModel(gru_layer, vocab_size)</span><br><span class="line">model.initialize(force_reinit=<span class="literal">True</span>, ctx=ctx)</span><br><span class="line"></span><br><span class="line">num_epochs, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line">train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,</span><br><span class="line">                            corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                            num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                            batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">想要有直升机</span><br><span class="line">想要和你飞到宇宙去</span><br><span class="line">想要和你融化在一起</span><br><span class="line">融化在宇宙里</span><br><span class="line">我每天每天每</span><br><span class="line">chars: 想要有直升机 想要和你飞到宇宙去 想要和</span><br><span class="line">indices: [<span class="number">977</span>, <span class="number">410</span>, <span class="number">954</span>, <span class="number">551</span>, <span class="number">770</span>, <span class="number">182</span>, <span class="number">305</span>, <span class="number">977</span>, <span class="number">410</span>, <span class="number">52</span>, <span class="number">928</span>, <span class="number">419</span>, <span class="number">360</span>, <span class="number">950</span>, <span class="number">804</span>, <span class="number">705</span>, <span class="number">305</span>, <span class="number">977</span>, <span class="number">410</span>, <span class="number">52</span>]</span><br><span class="line">epoch <span class="number">50</span>, perplexity <span class="number">112.876464</span>, time <span class="number">3.66</span> sec</span><br><span class="line">- 分开 我想你 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我</span><br><span class="line">- 不分开 我想你 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我</span><br><span class="line">epoch <span class="number">100</span>, perplexity <span class="number">12.570226</span>, time <span class="number">3.29</span> sec</span><br><span class="line">- 分开 我想就这样里堡 但你在我满腔的怒火 我想揍你已经很久 想想想你的微笑 让你在我遇见 让没有没有 我</span><br><span class="line">- 不分开 我爱你的爱写在西元前 深埋在美索不达米亚平原 用楔形文年的世斑鸠 我感的让我疯狂的可爱女人 坏坏的</span><br><span class="line">epoch <span class="number">150</span>, perplexity <span class="number">1.831173</span>, time <span class="number">3.69</span> sec</span><br><span class="line">- 分开 我想带你已经很久 别想躲 说你眼睛看着我 别发抖 快给我抬起头 有话去对医药箱说 别怪我 别怪我</span><br><span class="line">- 不分开 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生活</span><br><span class="line">epoch <span class="number">200</span>, perplexity <span class="number">1.067530</span>, time <span class="number">3.59</span> sec</span><br><span class="line">- 分开 我想轻的话模笑样没在看到 你给我 这里我一起 悲化我都做得到 但那个人已经不是我 上海一九四三 泛</span><br><span class="line">- 不分开 平过我怕你泪痛 我想要你的微笑每天都能看到  我知道这里很美但家乡的你更美原来我只想要你 陪我去吃</span><br><span class="line">epoch <span class="number">250</span>, perplexity <span class="number">1.030577</span>, time <span class="number">3.22</span> sec</span><br><span class="line">- 分开 我想轻声斯嵩山 想要是你笑到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每天在想想想想著你</span><br><span class="line">- 不分开觉  杵过云层 我试著努力向你奔跑 爱才送到 你却已在别人怀抱 就是开不了口让她知道 我一定会呵护著</span><br></pre></td></tr></tbody></table></figure><h2 id="长短期记忆循环神经网络LSTM"><a href="#长短期记忆循环神经网络LSTM" class="headerlink" title="长短期记忆循环神经网络LSTM"></a>长短期记忆循环神经网络LSTM</h2><h3 id="LSTM原生实现"><a href="#LSTM原生实现" class="headerlink" title="LSTM原生实现"></a>LSTM原生实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd, autograd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> rnn, loss <span class="keyword">as</span> gloss</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_jay_lyrics</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(<span class="string">'./data/jaychou_lyrics.txt.zip'</span>) <span class="keyword">as</span> zin:</span><br><span class="line">        <span class="keyword">with</span> zin.open(<span class="string">'jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            corpus_chars = f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(corpus_chars[:<span class="number">40</span>])</span><br><span class="line"></span><br><span class="line">    corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">    <span class="comment"># 仅使用前1万个字符训练模型</span></span><br><span class="line">    corpus_chars = corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line">    idx_to_char = list(set(corpus_chars))</span><br><span class="line">    char_to_idx = dict([(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)])</span><br><span class="line">    vocab_size = len(char_to_idx)</span><br><span class="line">    print(vocab_size)</span><br><span class="line">    print(char_to_idx)</span><br><span class="line"></span><br><span class="line">    x = [(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)]</span><br><span class="line">    print(x[:<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集中的每个字符转化成索引</span></span><br><span class="line">    corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line">    <span class="comment"># 打印前20个字符机器对应的索引</span></span><br><span class="line">    sample = corpus_indices[:<span class="number">20</span>]</span><br><span class="line">    print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line">    print(<span class="string">'indices:'</span>, sample)</span><br><span class="line">    <span class="keyword">return</span> idx_to_char, char_to_idx, corpus_indices, vocab_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ctx = mx.gpu()</span><br><span class="line">        _ = nd.zeros((<span class="number">1</span>,), ctx=ctx)</span><br><span class="line">    <span class="keyword">except</span> mx.base.MXNetError:</span><br><span class="line">        ctx = mx.cpu()</span><br><span class="line">    <span class="keyword">return</span> ctx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> nd.random.normal(scale=<span class="number">0.01</span>, shape=shape, ctx=ctx)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> _one((num_inputs, num_hiddens)), _one((num_hiddens, num_hiddens)), nd.zeros(num_hiddens, ctx=ctx)</span><br><span class="line"></span><br><span class="line">    W_xi, W_hi, b_i = _three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = _three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = _three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = _three()  <span class="comment"># 候选记忆细胞参数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = nd.zeros(num_outputs, ctx=ctx)</span><br><span class="line">    <span class="comment"># 附上梯度</span></span><br><span class="line">    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,</span><br><span class="line">              W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.attach_grad()</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_gru_state</span><span class="params">(batch_size, num_hiddens, ctx)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx), nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm</span><span class="params">(inputs, state, params)</span>:</span></span><br><span class="line">    W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q = params</span><br><span class="line">    H, C = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        I = nd.sigmoid(nd.dot(X, W_xi) + nd.dot(H, W_hi) + b_i)</span><br><span class="line">        F = nd.sigmoid(nd.dot(X, W_xf) + nd.dot(H, W_hf) + b_f)</span><br><span class="line">        O = nd.sigmoid(nd.dot(X, W_xo) + nd.dot(H, W_ho) + b_o)</span><br><span class="line">        C_tilda = nd.tanh(nd.dot(X, W_xc) + nd.dot(H, W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * C.tanh()</span><br><span class="line">        Y = nd.dot(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H, C)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_onehot</span><span class="params">(X, size)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [nd.one_hot(x, size) <span class="keyword">for</span> x <span class="keyword">in</span> X.T]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(params, theta, ctx)</span>:</span></span><br><span class="line">    norm = nd.array([<span class="number">0</span>], ctx)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        norm += (param.grad ** <span class="number">2</span>).sum()</span><br><span class="line">    norm = norm.sqrt().asscalar()</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        <span class="comment"># print("param:", param)</span></span><br><span class="line">        param[:] = param - lr * param.grad / batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># num_steps: 6</span></span><br><span class="line"><span class="comment"># corpus_indices:</span></span><br><span class="line"><span class="comment"># [0,1,2,...,29]</span></span><br><span class="line"><span class="comment"># [[0,...,5],[6,..,11],[12,...,17],[18,...,23],[24,...,29]]</span></span><br><span class="line"><span class="comment"># example_indices: [0,1,2,3,4]</span></span><br><span class="line"><span class="comment"># shuffle example_indices: [...]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    <span class="comment"># 减1是因为输出的索引是相应输入的索引加1</span></span><br><span class="line">    <span class="comment"># [0,1,2,3]输出标签应为[1,2,3,4]</span></span><br><span class="line">    <span class="comment"># 为了防止溢出</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># print(num_examples)</span></span><br><span class="line">    epoch_size = num_examples // batch_size</span><br><span class="line">    example_indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(example_indices)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回从pos开始的长为num_steps的序列</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span><span class="params">(pos)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        <span class="comment"># 每次读取batch_size个随机样本</span></span><br><span class="line">        i = i * batch_size</span><br><span class="line">        batch_indices = example_indices[i:i + batch_size]</span><br><span class="line">        X = [_data(j * num_steps) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j * num_steps + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> nd.array(X, ctx), nd.array(Y, ctx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    corpus_indices = nd.array(corpus_indices, ctx=ctx)</span><br><span class="line">    data_len = len(corpus_indices)</span><br><span class="line">    batch_len = data_len // batch_size</span><br><span class="line">    indices = corpus_indices[<span class="number">0</span>:batch_size * batch_len].reshape((batch_size, batch_len))</span><br><span class="line">    <span class="comment"># print(indices)</span></span><br><span class="line">    epoch_size = (batch_len - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i:i + num_steps]</span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>:i + num_steps + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn</span><span class="params">(prefix, num_chars, rnn, params, init_rnn_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx)</span>:</span></span><br><span class="line">    state = init_rnn_state(<span class="number">1</span>, num_hiddens, ctx)</span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 将上一实践步的输出作为当前时间步的输入</span></span><br><span class="line">        X = to_onehot(nd.array([output[<span class="number">-1</span>]], ctx=ctx), vocab_size)</span><br><span class="line">        <span class="comment"># 计算输出和更新隐藏状态</span></span><br><span class="line">        Y, state = rnn(X, state, params)</span><br><span class="line">        <span class="comment"># 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(int(Y[<span class="number">0</span>].argmax(axis=<span class="number">1</span>).asscalar()))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn</span><span class="params">(rnn, get_params, init_rnn_state, num_hiddens,</span></span></span><br><span class="line"><span class="function"><span class="params">                          vocab_size, ctx, corpus_indices, idx_to_char,</span></span></span><br><span class="line"><span class="function"><span class="params">                          char_to_idx, is_random_iter, num_epochs, num_steps,</span></span></span><br><span class="line"><span class="function"><span class="params">                          lr, clipping_theta, batch_size, pred_peroid,</span></span></span><br><span class="line"><span class="function"><span class="params">                          pred_len, prefixes)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_random_iter:</span><br><span class="line">        data_iter_fn = data_iter_random</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data_iter_fn = data_iter_consecutive</span><br><span class="line"></span><br><span class="line">    params = get_params()</span><br><span class="line">    loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_random_iter:</span><br><span class="line">            state = init_rnn_state(batch_size, num_hiddens, ctx)</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)</span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> is_random_iter:</span><br><span class="line">                state = init_rnn_state(batch_size, num_hiddens, ctx)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 否则需要使用detach函数从计算图分离隐藏状态</span></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                inputs = to_onehot(X, vocab_size)</span><br><span class="line">                <span class="comment"># outputs有num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">                outputs, state = rnn(inputs, state, params)</span><br><span class="line">                <span class="comment"># 拼接之后形状为(num_steps*batch_size, vocab_size)</span></span><br><span class="line">                outputs = nd.concat(*outputs, dim=<span class="number">0</span>)</span><br><span class="line">                <span class="comment"># Y的形状是(batch_size, num_steps)，转置后再变成长度为</span></span><br><span class="line">                <span class="comment"># batch * num_steps的向量，这样跟输出的行一一对应</span></span><br><span class="line">                y = Y.T.reshape((<span class="number">-1</span>,))</span><br><span class="line">                <span class="comment"># 使用交叉熵损失计算平均分类误差</span></span><br><span class="line">                l = loss(outputs, y).mean()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(params, clipping_theta, ctx)  <span class="comment"># 裁剪梯度</span></span><br><span class="line">            sgd(params, lr, <span class="number">1</span>)  <span class="comment"># 因为误差已经去过均值，梯度不用再做平均</span></span><br><span class="line">            l_sum += l.asscalar() * y.size</span><br><span class="line">            n += y.size</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_peroid == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">' -'</span>, predict_rnn(</span><br><span class="line">                    prefix, pred_len, rnn, params, init_rnn_state,</span><br><span class="line">                    num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">idx_to_char, char_to_idx, corpus_indices, vocab_size = load_data_jay_lyrics()</span><br><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">ctx = try_gpu()</span><br><span class="line"></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line">train_and_predict_rnn(lstm, get_params, init_gru_state, num_hiddens,</span><br><span class="line">                      vocab_size, ctx, corpus_indices, idx_to_char,</span><br><span class="line">                      char_to_idx, <span class="literal">False</span>, num_epochs, num_steps, lr,</span><br><span class="line">                      clipping_theta, batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></tbody></table></figure><h3 id="LSTM简洁实现"><a href="#LSTM简洁实现" class="headerlink" title="LSTM简洁实现"></a>LSTM简洁实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss, nn, rnn</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, init, nd, gluon</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_jay_lyrics</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(<span class="string">'./data/jaychou_lyrics.txt.zip'</span>) <span class="keyword">as</span> zin:</span><br><span class="line">        <span class="keyword">with</span> zin.open(<span class="string">'jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            corpus_chars = f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="comment"># print(corpus_chars[:40])</span></span><br><span class="line"></span><br><span class="line">    corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">    <span class="comment"># 仅使用前1万个字符训练模型</span></span><br><span class="line">    corpus_chars = corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line">    idx_to_char = list(set(corpus_chars))</span><br><span class="line">    char_to_idx = dict([(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)])</span><br><span class="line">    vocab_size = len(char_to_idx)</span><br><span class="line">    <span class="comment"># print(vocab_size)</span></span><br><span class="line">    <span class="comment"># print(char_to_idx)</span></span><br><span class="line"></span><br><span class="line">    x = [(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)]</span><br><span class="line">    <span class="comment"># print(x[:20])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集中的每个字符转化成索引</span></span><br><span class="line">    corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line">    <span class="comment"># 打印前20个字符机器对应的索引</span></span><br><span class="line">    sample = corpus_indices[:<span class="number">20</span>]</span><br><span class="line">    print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line">    print(<span class="string">'indices:'</span>, sample)</span><br><span class="line">    <span class="keyword">return</span> idx_to_char, char_to_idx, corpus_indices, vocab_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_layer, vocab_size, **kwargs)</span>:</span></span><br><span class="line">        super(RNNModel, self).__init__(**kwargs)</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.dense = nn.Dense(vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, state)</span>:</span></span><br><span class="line">        <span class="comment"># 将输入转置成(num_steps, batch_size)后获取one-hot向量表示</span></span><br><span class="line">        <span class="comment"># print(inputs.shape)</span></span><br><span class="line">        X = nd.one_hot(inputs.T, self.vocab_size)</span><br><span class="line">        <span class="comment"># print(X.shape)</span></span><br><span class="line">        Y, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># 全连接层会首先将Y的形状编程(num_steps*batch_size, num_hiddens)，</span></span><br><span class="line">        <span class="comment"># 它的输出为(num_steps*batch_size, vocab_size)</span></span><br><span class="line">        output = self.dense(Y.reshape((<span class="number">-1</span>, Y.shape[<span class="number">-1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span><span class="params">(self, *args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.rnn.begin_state(*args, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn_gluon</span><span class="params">(prefix, num_chars, model, vocab_size, ctx, idx_to_char, char_to_idx)</span>:</span></span><br><span class="line">    <span class="comment"># 使用model的成员函数来初始化隐藏状态</span></span><br><span class="line">    state = model.begin_state(batch_size=<span class="number">1</span>, ctx=ctx)</span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 将上一实践步的输出作为当前时间步的输入</span></span><br><span class="line">        X = nd.array([output[<span class="number">-1</span>]], ctx=ctx).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># print(X.shape)</span></span><br><span class="line">        <span class="comment"># 计算输出和更新隐藏状态</span></span><br><span class="line">        Y, state = model(X, state)</span><br><span class="line">        <span class="comment"># print(Y.shape)</span></span><br><span class="line">        <span class="comment"># 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(int(Y.argmax(axis=<span class="number">1</span>).asscalar()))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ctx = mx.gpu()</span><br><span class="line">        _ = nd.zeros((<span class="number">1</span>,), ctx=ctx)</span><br><span class="line">    <span class="keyword">except</span> mx.base.MXNetError:</span><br><span class="line">        ctx = mx.cpu()</span><br><span class="line">    <span class="keyword">return</span> ctx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># num_steps: 6</span></span><br><span class="line"><span class="comment"># corpus_indices:</span></span><br><span class="line"><span class="comment"># [0,1,2,...,29]</span></span><br><span class="line"><span class="comment"># [[0,...,5],[6,..,11],[12,...,17],[18,...,23],[24,...,29]]</span></span><br><span class="line"><span class="comment"># example_indices: [0,1,2,3,4]</span></span><br><span class="line"><span class="comment"># shuffle example_indices: [...]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    <span class="comment"># 减1是因为输出的索引是相应输入的索引加1</span></span><br><span class="line">    <span class="comment"># [0,1,2,3]输出标签应为[1,2,3,4]</span></span><br><span class="line">    <span class="comment"># 为了防止溢出</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># print(num_examples)</span></span><br><span class="line">    epoch_size = num_examples // batch_size</span><br><span class="line">    example_indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(example_indices)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回从pos开始的长为num_steps的序列</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span><span class="params">(pos)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        <span class="comment"># 每次读取batch_size个随机样本</span></span><br><span class="line">        i = i * batch_size</span><br><span class="line">        batch_indices = example_indices[i:i + batch_size]</span><br><span class="line">        X = [_data(j * num_steps) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j * num_steps + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> nd.array(X, ctx), nd.array(Y, ctx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    corpus_indices = nd.array(corpus_indices, ctx=ctx)</span><br><span class="line">    data_len = len(corpus_indices)</span><br><span class="line">    batch_len = data_len // batch_size</span><br><span class="line">    indices = corpus_indices[<span class="number">0</span>:batch_size * batch_len].reshape((batch_size, batch_len))</span><br><span class="line">    <span class="comment"># print(indices)</span></span><br><span class="line">    epoch_size = (batch_len - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i:i + num_steps]</span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>:i + num_steps + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(params, theta, ctx)</span>:</span></span><br><span class="line">    norm = nd.array([<span class="number">0</span>], ctx)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        norm += (param.grad ** <span class="number">2</span>).sum()</span><br><span class="line">    norm = norm.sqrt().asscalar()</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn_gluon</span><span class="params">(model, num_hiddens, vocab_size, ctx,</span></span></span><br><span class="line"><span class="function"><span class="params">                                corpus_indices, idx_to_char, char_to_idx,</span></span></span><br><span class="line"><span class="function"><span class="params">                                num_epochs, num_steps, lr, clipping_theta,</span></span></span><br><span class="line"><span class="function"><span class="params">                                batch_size, pred_peroid, pred_len, prefixes)</span>:</span></span><br><span class="line">    loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line">    model.initialize(ctx=ctx, force_reinit=<span class="literal">True</span>, init=init.Normal(<span class="number">0.01</span>))</span><br><span class="line">    trainer = gluon.Trainer(model.collect_params(), <span class="string">'sgd'</span>,</span><br><span class="line">                            {<span class="string">'learning_rate'</span>: lr, <span class="string">'momentum'</span>: <span class="number">0</span>, <span class="string">'wd'</span>: <span class="number">0</span>})</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx)</span><br><span class="line">        state = model.begin_state(batch_size=batch_size, ctx=ctx)</span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                s.detach()</span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                output, state = model(X, state)</span><br><span class="line">                y = Y.T.reshape((<span class="number">-1</span>,))</span><br><span class="line">                l = loss(output, y).mean()</span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="comment"># 梯度裁剪</span></span><br><span class="line">            params = [p.data() <span class="keyword">for</span> p <span class="keyword">in</span> model.collect_params().values()]</span><br><span class="line">            grad_clipping(params, clipping_theta, ctx)</span><br><span class="line">            trainer.step(<span class="number">1</span>)  <span class="comment"># 因为已经误差取过均值，梯度不用再做平均</span></span><br><span class="line">            l_sum += l.asscalar() * y.size</span><br><span class="line">            n += y.size</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_peroid == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">'-'</span>, predict_rnn_gluon(prefix, pred_len,</span><br><span class="line">                                             model, vocab_size, ctx, idx_to_char, char_to_idx))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ctx = try_gpu()</span><br><span class="line">idx_to_char, char_to_idx, corpus_indices, vocab_size = load_data_jay_lyrics()</span><br><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line">num_steps = <span class="number">35</span></span><br><span class="line">lstm_layer = rnn.LSTM(num_hiddens)</span><br><span class="line">lstm_layer.initialize()</span><br><span class="line">model = RNNModel(lstm_layer, vocab_size)</span><br><span class="line">model.initialize(force_reinit=<span class="literal">True</span>, ctx=ctx)</span><br><span class="line"></span><br><span class="line">num_epochs, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line">train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,</span><br><span class="line">                            corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                            num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                            batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></tbody></table></figure><h2 id="深度循环神经网络DRNN"><a href="#深度循环神经网络DRNN" class="headerlink" title="深度循环神经网络DRNN"></a>深度循环神经网络DRNN</h2><h3 id="DRNN原生实现"><a href="#DRNN原生实现" class="headerlink" title="DRNN原生实现"></a>DRNN原生实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_jay_lyrics</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(<span class="string">'./data/jaychou_lyrics.txt.zip'</span>) <span class="keyword">as</span> zin:</span><br><span class="line">        <span class="keyword">with</span> zin.open(<span class="string">'jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            corpus_chars = f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(corpus_chars[:<span class="number">40</span>])</span><br><span class="line"></span><br><span class="line">    corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">    <span class="comment"># 仅使用前1万个字符训练模型</span></span><br><span class="line">    corpus_chars = corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line">    idx_to_char = list(set(corpus_chars))</span><br><span class="line">    char_to_idx = dict([(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)])</span><br><span class="line">    vocab_size = len(char_to_idx)</span><br><span class="line">    print(vocab_size)</span><br><span class="line">    print(char_to_idx)</span><br><span class="line"></span><br><span class="line">    x = [(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)]</span><br><span class="line">    print(x[:<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集中的每个字符转化成索引</span></span><br><span class="line">    corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line">    <span class="comment"># 打印前20个字符机器对应的索引</span></span><br><span class="line">    sample = corpus_indices[:<span class="number">20</span>]</span><br><span class="line">    print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line">    print(<span class="string">'indices:'</span>, sample)</span><br><span class="line">    <span class="keyword">return</span> idx_to_char, char_to_idx, corpus_indices, vocab_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># num_steps: 6</span></span><br><span class="line"><span class="comment"># corpus_indices:</span></span><br><span class="line"><span class="comment"># [0,1,2,...,29]</span></span><br><span class="line"><span class="comment"># [[0,...,5],[6,..,11],[12,...,17],[18,...,23],[24,...,29]]</span></span><br><span class="line"><span class="comment"># example_indices: [0,1,2,3,4]</span></span><br><span class="line"><span class="comment"># shuffle example_indices: [...]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    <span class="comment"># 减1是因为输出的索引是相应输入的索引加1</span></span><br><span class="line">    <span class="comment"># [0,1,2,3]输出标签应为[1,2,3,4]</span></span><br><span class="line">    <span class="comment"># 为了防止溢出</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># print(num_examples)</span></span><br><span class="line">    epoch_size = num_examples // batch_size</span><br><span class="line">    example_indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(example_indices)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回从pos开始的长为num_steps的序列</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span><span class="params">(pos)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        <span class="comment"># 每次读取batch_size个随机样本</span></span><br><span class="line">        i = i * batch_size</span><br><span class="line">        batch_indices = example_indices[i:i + batch_size]</span><br><span class="line">        X = [_data(j * num_steps) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j * num_steps + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> nd.array(X, ctx), nd.array(Y, ctx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    corpus_indices = nd.array(corpus_indices, ctx=ctx)</span><br><span class="line">    data_len = len(corpus_indices)</span><br><span class="line">    batch_len = data_len // batch_size</span><br><span class="line">    indices = corpus_indices[<span class="number">0</span>:batch_size * batch_len].reshape((batch_size, batch_len))</span><br><span class="line">    <span class="comment"># print(indices)</span></span><br><span class="line">    epoch_size = (batch_len - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i:i + num_steps]</span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>:i + num_steps + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_onehot</span><span class="params">(X, size)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [nd.one_hot(x, size) <span class="keyword">for</span> x <span class="keyword">in</span> X.T]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ctx = mx.gpu()</span><br><span class="line">        _ = nd.zeros((<span class="number">1</span>,), ctx=ctx)</span><br><span class="line">    <span class="keyword">except</span> mx.base.MXNetError:</span><br><span class="line">        ctx = mx.cpu()</span><br><span class="line">    <span class="keyword">return</span> ctx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># my_seq = range(30)</span></span><br><span class="line"><span class="comment"># for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):</span></span><br><span class="line"><span class="comment">#     print('X: ', X, '\nY:', Y, '\n')</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">(ith_layer, num_layers)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> nd.random.normal(scale=<span class="number">0.01</span>, shape=shape, ctx=ctx)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    <span class="keyword">if</span> ith_layer == <span class="number">0</span>:</span><br><span class="line">        W_xh = _one((num_inputs, num_hiddens))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        W_xh = _one((num_hiddens, num_hiddens))</span><br><span class="line">    W_hh = _one((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = nd.zeros(num_hiddens, ctx=ctx)</span><br><span class="line">    <span class="keyword">if</span> ith_layer == num_layers - <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 输出层参数</span></span><br><span class="line">        W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">        b_q = nd.zeros(num_outputs, ctx=ctx)</span><br><span class="line">        <span class="comment"># 附上梯度</span></span><br><span class="line">        params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        params = [W_xh, W_hh, b_h]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.attach_grad()</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rnn_state</span><span class="params">(batch_size, num_hiddens, ctx)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn</span><span class="params">(inputs, states, params)</span>:</span></span><br><span class="line">    <span class="comment"># inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(params)):</span><br><span class="line">            <span class="comment"># W_xh, W_hh, b_h, W_hq, b_q = None, None, None, None, None</span></span><br><span class="line">            <span class="keyword">if</span> i != len(params) - <span class="number">1</span>:</span><br><span class="line">                W_xh, W_hh, b_h = params[i]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                W_xh, W_hh, b_h, W_hq, b_q = params[i]</span><br><span class="line">            H, = states[i]</span><br><span class="line">            <span class="comment"># preH = None</span></span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                preH, = states[i - <span class="number">1</span>]</span><br><span class="line">                H = nd.tanh(nd.dot(preH, W_xh) + nd.dot(H, W_hh) + b_h)</span><br><span class="line">            states[i] = (H,)</span><br><span class="line">            <span class="keyword">if</span> i == len(params) - <span class="number">1</span>:</span><br><span class="line">                Y = nd.dot(H, W_hq) + b_q</span><br><span class="line">                outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, states</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn</span><span class="params">(prefix, num_chars, rnn, params, init_rnn_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx)</span>:</span></span><br><span class="line">    states = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(params)):</span><br><span class="line">        states.append(init_rnn_state(<span class="number">1</span>, num_hiddens, ctx))</span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 将上一实践步的输出作为当前时间步的输入</span></span><br><span class="line">        X = to_onehot(nd.array([output[<span class="number">-1</span>]], ctx=ctx), vocab_size)</span><br><span class="line">        <span class="comment"># 计算输出和更新隐藏状态</span></span><br><span class="line">        Y, states = rnn(X, states, params)</span><br><span class="line">        <span class="comment"># 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(int(Y[<span class="number">0</span>].argmax(axis=<span class="number">1</span>).asscalar()))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(params, theta, ctx)</span>:</span></span><br><span class="line">    norm = nd.array([<span class="number">0</span>], ctx)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        norm += (param.grad ** <span class="number">2</span>).sum()</span><br><span class="line">    norm = norm.sqrt().asscalar()</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        <span class="comment"># print("param:", param)</span></span><br><span class="line">        param[:] = param - lr * param.grad / batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn</span><span class="params">(rnn, num_layers, get_params, init_rnn_state, num_hiddens,</span></span></span><br><span class="line"><span class="function"><span class="params">                          vocab_size, ctx, corpus_indices, idx_to_char,</span></span></span><br><span class="line"><span class="function"><span class="params">                          char_to_idx, is_random_iter, num_epochs, num_steps,</span></span></span><br><span class="line"><span class="function"><span class="params">                          lr, clipping_theta, batch_size, pred_peroid,</span></span></span><br><span class="line"><span class="function"><span class="params">                          pred_len, prefixes)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_random_iter:</span><br><span class="line">        data_iter_fn = data_iter_random</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data_iter_fn = data_iter_consecutive</span><br><span class="line"></span><br><span class="line">    params = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">        params.append(get_params(i, num_layers))</span><br><span class="line">    loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_random_iter:</span><br><span class="line">            states = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">                states.append(init_rnn_state(batch_size, num_hiddens, ctx))</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)</span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> is_random_iter:</span><br><span class="line">                states = []</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">                    states.append(init_rnn_state(batch_size, num_hiddens, ctx))</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 否则需要使用detach函数从计算图分离隐藏状态</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(len(states)):</span><br><span class="line">                    <span class="keyword">for</span> s <span class="keyword">in</span> states[i]:</span><br><span class="line">                        s.detach()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                inputs = to_onehot(X, vocab_size)</span><br><span class="line">                <span class="comment"># outputs有num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class="line">                outputs, states = rnn(inputs, states, params)</span><br><span class="line">                <span class="comment"># 拼接之后形状为(num_steps*batch_size, vocab_size)</span></span><br><span class="line">                outputs = nd.concat(*outputs, dim=<span class="number">0</span>)</span><br><span class="line">                <span class="comment"># Y的形状是(batch_size, num_steps)，转置后再变成长度为</span></span><br><span class="line">                <span class="comment"># batch * num_steps的向量，这样跟输出的行一一对应</span></span><br><span class="line">                y = Y.T.reshape((<span class="number">-1</span>,))</span><br><span class="line">                <span class="comment"># 使用交叉熵损失计算平均分类误差</span></span><br><span class="line">                l = loss(outputs, y).mean()</span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">                grad_clipping(params[i], clipping_theta, ctx)  <span class="comment"># 裁剪梯度</span></span><br><span class="line">                sgd(params[i], lr, <span class="number">1</span>)  <span class="comment"># 因为误差已经去过均值，梯度不用再做平均</span></span><br><span class="line">            l_sum += l.asscalar() * y.size</span><br><span class="line">            n += y.size</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_peroid == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">' -'</span>, predict_rnn(</span><br><span class="line">                    prefix, pred_len, rnn, params, init_rnn_state,</span><br><span class="line">                    num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">idx_to_char, char_to_idx, corpus_indices, vocab_size = load_data_jay_lyrics()</span><br><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">ctx = try_gpu()</span><br><span class="line">print(<span class="string">'will use'</span>, ctx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># X = nd.arange(10).reshape((2, 5))</span></span><br><span class="line"><span class="comment"># state = init_rnn_state(X.shape[0], num_hiddens, ctx)</span></span><br><span class="line"><span class="comment"># inputs = to_onehot(X.as_in_context(ctx), vocab_size)</span></span><br><span class="line"><span class="comment"># print(inputs)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># params = get_params()</span></span><br><span class="line"><span class="comment"># outputs, state_new = rnn(inputs, state, params)</span></span><br><span class="line"><span class="comment"># print(len(outputs), outputs[0].shape, state_new[0].shape)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># predict_txt = predict_rnn('分开', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,</span></span><br><span class="line"><span class="comment">#             ctx, idx_to_char, char_to_idx)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># print(predict_txt)</span></span><br><span class="line"></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_peroid, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line">num_layers = <span class="number">3</span></span><br><span class="line">train_and_predict_rnn(rnn, num_layers, get_params, init_rnn_state, num_hiddens,</span><br><span class="line">                      vocab_size, ctx, corpus_indices, idx_to_char,</span><br><span class="line">                      char_to_idx, <span class="literal">False</span>, num_epochs, num_steps, lr,</span><br><span class="line">                      clipping_theta, batch_size, pred_peroid, pred_len, prefixes)</span><br><span class="line"><span class="comment"># params = []</span></span><br><span class="line"><span class="comment"># for i in range(num_layers):</span></span><br><span class="line"><span class="comment">#     params.append(get_params(i, num_layers))</span></span><br><span class="line"><span class="comment"># predict_rnn(prefixes[0], pred_len, rnn, params, init_rnn_state,</span></span><br><span class="line"><span class="comment"># num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx)</span></span><br></pre></td></tr></tbody></table></figure><h3 id="DRNN简洁实现"><a href="#DRNN简洁实现" class="headerlink" title="DRNN简洁实现"></a>DRNN简洁实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss, nn, rnn</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, init, nd, gluon</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_jay_lyrics</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(<span class="string">'./data/jaychou_lyrics.txt.zip'</span>) <span class="keyword">as</span> zin:</span><br><span class="line">        <span class="keyword">with</span> zin.open(<span class="string">'jaychou_lyrics.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            corpus_chars = f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(corpus_chars[:<span class="number">40</span>])</span><br><span class="line"></span><br><span class="line">    corpus_chars = corpus_chars.replace(<span class="string">'\n'</span>, <span class="string">' '</span>).replace(<span class="string">'\r'</span>, <span class="string">' '</span>)</span><br><span class="line">    <span class="comment"># 仅使用前1万个字符训练模型</span></span><br><span class="line">    corpus_chars = corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br><span class="line"></span><br><span class="line">    idx_to_char = list(set(corpus_chars))</span><br><span class="line">    char_to_idx = dict([(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)])</span><br><span class="line">    vocab_size = len(char_to_idx)</span><br><span class="line">    print(vocab_size)</span><br><span class="line">    print(char_to_idx)</span><br><span class="line"></span><br><span class="line">    x = [(char, i) <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(idx_to_char)]</span><br><span class="line">    print(x[:<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练集中的每个字符转化成索引</span></span><br><span class="line">    corpus_indices = [char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line">    <span class="comment"># 打印前20个字符机器对应的索引</span></span><br><span class="line">    sample = corpus_indices[:<span class="number">20</span>]</span><br><span class="line">    print(<span class="string">'chars:'</span>, <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line">    print(<span class="string">'indices:'</span>, sample)</span><br><span class="line">    <span class="keyword">return</span> idx_to_char, char_to_idx, corpus_indices, vocab_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_layer, vocab_size, **kwargs)</span>:</span></span><br><span class="line">        super(RNNModel, self).__init__(**kwargs)</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.dense = nn.Dense(vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, state)</span>:</span></span><br><span class="line">        <span class="comment"># 将输入转置成(num_steps, batch_size)后获取one-hot向量表示</span></span><br><span class="line">        <span class="comment"># print(inputs.shape)</span></span><br><span class="line">        X = nd.one_hot(inputs.T, self.vocab_size)</span><br><span class="line">        <span class="comment"># print(X.shape)</span></span><br><span class="line">        Y, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># 全连接层会首先将Y的形状编程(num_steps*batch_size, num_hiddens)，</span></span><br><span class="line">        <span class="comment"># 它的输出为(num_steps*batch_size, vocab_size)</span></span><br><span class="line">        output = self.dense(Y.reshape((<span class="number">-1</span>, Y.shape[<span class="number">-1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span><span class="params">(self, *args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.rnn.begin_state(*args, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn_gluon</span><span class="params">(prefix, num_chars, model, vocab_size, ctx, idx_to_char, char_to_idx)</span>:</span></span><br><span class="line">    <span class="comment"># 使用model的成员函数来初始化隐藏状态</span></span><br><span class="line">    state = model.begin_state(batch_size=<span class="number">1</span>, ctx=ctx)</span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 将上一实践步的输出作为当前时间步的输入</span></span><br><span class="line">        X = nd.array([output[<span class="number">-1</span>]], ctx=ctx).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># print(X.shape)</span></span><br><span class="line">        <span class="comment"># 计算输出和更新隐藏状态</span></span><br><span class="line">        Y, state = model(X, state)</span><br><span class="line">        <span class="comment"># print(Y.shape)</span></span><br><span class="line">        <span class="comment"># 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(int(Y.argmax(axis=<span class="number">1</span>).asscalar()))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ctx = mx.gpu()</span><br><span class="line">        _ = nd.zeros((<span class="number">1</span>,), ctx=ctx)</span><br><span class="line">    <span class="keyword">except</span> mx.base.MXNetError:</span><br><span class="line">        ctx = mx.cpu()</span><br><span class="line">    <span class="keyword">return</span> ctx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># num_steps: 6</span></span><br><span class="line"><span class="comment"># corpus_indices:</span></span><br><span class="line"><span class="comment"># [0,1,2,...,29]</span></span><br><span class="line"><span class="comment"># [[0,...,5],[6,..,11],[12,...,17],[18,...,23],[24,...,29]]</span></span><br><span class="line"><span class="comment"># example_indices: [0,1,2,3,4]</span></span><br><span class="line"><span class="comment"># shuffle example_indices: [...]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_random</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    <span class="comment"># 减1是因为输出的索引是相应输入的索引加1</span></span><br><span class="line">    <span class="comment"># [0,1,2,3]输出标签应为[1,2,3,4]</span></span><br><span class="line">    <span class="comment"># 为了防止溢出</span></span><br><span class="line">    num_examples = (len(corpus_indices) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># print(num_examples)</span></span><br><span class="line">    epoch_size = num_examples // batch_size</span><br><span class="line">    example_indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(example_indices)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回从pos开始的长为num_steps的序列</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data</span><span class="params">(pos)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corpus_indices[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        <span class="comment"># 每次读取batch_size个随机样本</span></span><br><span class="line">        i = i * batch_size</span><br><span class="line">        batch_indices = example_indices[i:i + batch_size]</span><br><span class="line">        X = [_data(j * num_steps) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y = [_data(j * num_steps + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> nd.array(X, ctx), nd.array(Y, ctx)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span><span class="params">(corpus_indices, batch_size, num_steps, ctx=None)</span>:</span></span><br><span class="line">    corpus_indices = nd.array(corpus_indices, ctx=ctx)</span><br><span class="line">    data_len = len(corpus_indices)</span><br><span class="line">    batch_len = data_len // batch_size</span><br><span class="line">    indices = corpus_indices[<span class="number">0</span>:batch_size * batch_len].reshape((batch_size, batch_len))</span><br><span class="line">    <span class="comment"># print(indices)</span></span><br><span class="line">    epoch_size = (batch_len - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i:i + num_steps]</span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>:i + num_steps + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span><span class="params">(params, theta, ctx)</span>:</span></span><br><span class="line">    norm = nd.array([<span class="number">0</span>], ctx)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        norm += (param.grad ** <span class="number">2</span>).sum()</span><br><span class="line">    norm = norm.sqrt().asscalar()</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn_gluon</span><span class="params">(model, num_hiddens, vocab_size, ctx,</span></span></span><br><span class="line"><span class="function"><span class="params">                                corpus_indices, idx_to_char, char_to_idx,</span></span></span><br><span class="line"><span class="function"><span class="params">                                num_epochs, num_steps, lr, clipping_theta,</span></span></span><br><span class="line"><span class="function"><span class="params">                                batch_size, pred_peroid, pred_len, prefixes)</span>:</span></span><br><span class="line">    loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line">    model.initialize(ctx=ctx, force_reinit=<span class="literal">True</span>, init=init.Normal(<span class="number">0.01</span>))</span><br><span class="line">    trainer = gluon.Trainer(model.collect_params(), <span class="string">'sgd'</span>,</span><br><span class="line">                            {<span class="string">'learning_rate'</span>: lr, <span class="string">'momentum'</span>: <span class="number">0</span>, <span class="string">'wd'</span>: <span class="number">0</span>})</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx)</span><br><span class="line">        state = model.begin_state(batch_size=batch_size, ctx=ctx)</span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                s.detach()</span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                output, state = model(X, state)</span><br><span class="line">                y = Y.T.reshape((<span class="number">-1</span>,))</span><br><span class="line">                l = loss(output, y).mean()</span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="comment"># 梯度裁剪</span></span><br><span class="line">            params = [p.data() <span class="keyword">for</span> p <span class="keyword">in</span> model.collect_params().values()]</span><br><span class="line">            grad_clipping(params, clipping_theta, ctx)</span><br><span class="line">            trainer.step(<span class="number">1</span>)  <span class="comment"># 因为已经误差取过均值，梯度不用再做平均</span></span><br><span class="line">            l_sum += l.asscalar() * y.size</span><br><span class="line">            n += y.size</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_peroid == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">'-'</span>, predict_rnn_gluon(prefix, pred_len,</span><br><span class="line">                                             model, vocab_size, ctx, idx_to_char, char_to_idx))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ctx = try_gpu()</span><br><span class="line">idx_to_char, char_to_idx, corpus_indices, vocab_size = load_data_jay_lyrics()</span><br><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line">num_steps = <span class="number">35</span></span><br><span class="line"><span class="comment"># rnn_layer = rnn.RNN(num_hiddens)</span></span><br><span class="line"><span class="comment"># rnn_layer.initialize()</span></span><br><span class="line">rnn_layers = rnn.RNN(num_hiddens, num_layers=<span class="number">3</span>)</span><br><span class="line">model = RNNModel(rnn_layers, vocab_size)</span><br><span class="line">model.initialize(force_reinit=<span class="literal">True</span>, ctx=ctx)</span><br><span class="line"></span><br><span class="line">num_epochs, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line">train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,</span><br><span class="line">                            corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                            num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                            batch_size, pred_period, pred_len, prefixes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># x = nd.arange(20).reshape((2, 2, 5))</span></span><br><span class="line"><span class="comment"># print(x.shape)</span></span><br><span class="line"><span class="comment"># print(x)</span></span><br><span class="line"><span class="comment"># print(x.reshape((-1, 5)))</span></span><br></pre></td></tr></tbody></table></figure></div><div class="article-info article-info-index"><div class="article-tag tagcloud"><i class="icon-price-tags icon"></i><ul class="article-tag-list"><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag article-tag-list-link color3">ML</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag article-tag-list-link color3">DL</a></li></ul></div><div class="share-btn share-icons tooltip-left"><div class="tooltip tooltip-east"><span class="tooltip-item"><a href="javascript:;" class="share-sns share-outer"><i class="icon icon-share"></i></a></span><span class="tooltip-content"><div class="share-wrap"><div class="share-icons"><a class="weibo share-sns" href="javascript:;" data-type="weibo"><i class="icon icon-weibo"></i></a><a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin"><i class="icon icon-weixin"></i></a><a class="qq share-sns" href="javascript:;" data-type="qq"><i class="icon icon-qq"></i></a><a class="douban share-sns" href="javascript:;" data-type="douban"><i class="icon icon-douban"></i></a><a class="qzone share-sns" href="javascript:;" data-type="qzone"><i class="icon icon-qzone"></i></a><a class="facebook share-sns" href="javascript:;" data-type="facebook"><i class="icon icon-facebook"></i></a><a class="twitter share-sns" href="javascript:;" data-type="twitter"><i class="icon icon-twitter"></i></a><a class="google share-sns" href="javascript:;" data-type="google"><i class="icon icon-google"></i></a></div></div></span></div></div><div class="page-modal wx-share js-wx-box"><a class="close js-modal-close" href="javascript:;"><i class="icon icon-close"></i></a><p>扫一扫，分享到微信</p><div class="wx-qrcode"> <img src="http://s.jiathis.com/qrcode.php?url=https://lourisxu.github.io/2020/03/25/7-deep-learning-recurrent-neural-networks.html/" alt="微信分享二维码"></div></div><div class="mask js-mask"></div><div class="clearfix"></div></div></div></article><nav id="article-nav"><a href="/2020/04/05/nodejs-study-notes.html/" id="article-nav-newer" class="article-nav-link-wrap"><i class="icon-circle-left"></i><div class="article-nav-title"> Node.js学习笔记</div></a><a href="/2020/03/21/librosaaudio-processing-library-learning.html/" id="article-nav-older" class="article-nav-link-wrap"><div class="article-nav-title">librosa--音频处理库学习</div><i class="icon-circle-right"></i></a></nav><aside class="wrap-side-operation"><div class="mod-side-operation"><div class="jump-container" id="js-jump-container" style="display:none"><a href="javascript:void(0)" class="mod-side-operation__jump-to-top"><i class="icon-font icon-back"></i></a><div id="js-jump-plan-container" class="jump-plan-container" style="top:-11px"><i class="icon-font icon-plane jump-plane"></i></div></div><div class="toc-container tooltip-left"><i class="icon-font icon-category"></i><div class="tooltip tooltip-east"><span class="tooltip-item"></span><span class="tooltip-content"><div class="toc-article"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#循环神经网络"><span class="toc-number">1.</span> <span class="toc-text">循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#含隐藏状态的循环神经网络"><span class="toc-number">1.1.</span> <span class="toc-text">含隐藏状态的循环神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#语言模型数据集（周杰伦专辑歌词）"><span class="toc-number">1.2.</span> <span class="toc-text">语言模型数据集（周杰伦专辑歌词）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#随机采样"><span class="toc-number">1.2.1.</span> <span class="toc-text">随机采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#相邻采样"><span class="toc-number">1.2.2.</span> <span class="toc-text">相邻采样</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#循环神经网络从零开始实现"><span class="toc-number">1.3.</span> <span class="toc-text">循环神经网络从零开始实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#one-hot向量"><span class="toc-number">1.3.1.</span> <span class="toc-text">one_hot向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型训练以及预测"><span class="toc-number">1.3.2.</span> <span class="toc-text">模型训练以及预测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#循环神经网络的简洁实现"><span class="toc-number">1.4.</span> <span class="toc-text">循环神经网络的简洁实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#测试"><span class="toc-number">1.4.1.</span> <span class="toc-text">测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练"><span class="toc-number">1.4.2.</span> <span class="toc-text">训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#门控循环神经网络GRU"><span class="toc-number">1.5.</span> <span class="toc-text">门控循环神经网络GRU</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#从零开始实现"><span class="toc-number">1.5.1.</span> <span class="toc-text">从零开始实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#简洁实现"><span class="toc-number">1.5.2.</span> <span class="toc-text">简洁实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#长短期记忆循环神经网络LSTM"><span class="toc-number">1.6.</span> <span class="toc-text">长短期记忆循环神经网络LSTM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM原生实现"><span class="toc-number">1.6.1.</span> <span class="toc-text">LSTM原生实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM简洁实现"><span class="toc-number">1.6.2.</span> <span class="toc-text">LSTM简洁实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#深度循环神经网络DRNN"><span class="toc-number">1.7.</span> <span class="toc-text">深度循环神经网络DRNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#DRNN原生实现"><span class="toc-number">1.7.1.</span> <span class="toc-text">DRNN原生实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DRNN简洁实现"><span class="toc-number">1.7.2.</span> <span class="toc-text">DRNN简洁实现</span></a></li></ol></li></ol></li></ol></div></span></div></div></div></aside></div></div></div><footer id="footer"><div class="outer"><div id="footer-info"><div class="footer-left"> &copy; 2025 Louris <span style="font-size:smaller">Hosted by <a href="https://coding.net/" target="_blank" rel="noopener" style="font-weight:700">Coding Pages</a>/ <a href="https://gitee.com/" target="_blank" rel="noopener" style="font-weight:700">Gitee Pages</a>/ <a href="https://github.com/" target="_blank" rel="noopener" style="font-weight:700">Github Pages</a></span></div><div class="footer-right"> <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten</div></div></div></footer></div><script>var yiliaConfig={mathjax:!0,isHome:!1,isPost:!0,isArchive:!1,isTag:!1,isCategory:!1,open_in_new:!1,toc_hide_index:!1,root:"/",innerArchive:!0,showTags:!1}</script><script>!function(r){function e(t){if(i[t])return i[t].exports;var n=i[t]={exports:{},id:t,loaded:!1};return r[t].call(n.exports,n,n.exports,e),n.loaded=!0,n.exports}var i={};e.m=r,e.c=i,e.p="./",e(0)}([function(t,n,r){r(203),t.exports=r(199)},function(t,n,r){var d=r(3),y=r(49),g=r(26),m=r(27),b=r(46),x="prototype",S=function(t,n,r){var e,i,o,u,c=t&S.F,f=t&S.G,a=t&S.S,s=t&S.P,l=t&S.B,h=f?d:a?d[n]||(d[n]={}):(d[n]||{})[x],v=f?y:y[n]||(y[n]={}),p=v[x]||(v[x]={});for(e in f&&(r=n),r)o=((i=!c&&h&&void 0!==h[e])?h:r)[e],u=l&&i?b(o,d):s&&"function"==typeof o?b(Function.call,o):o,h&&m(h,e,o,t&S.U),v[e]!=o&&g(v,e,u),s&&p[e]!=o&&(p[e]=o)};d.core=y,S.F=1,S.G=2,S.S=4,S.P=8,S.B=16,S.W=32,S.U=64,S.R=128,t.exports=S},function(t,n,r){var e=r(5);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n,r){var e=r(128)("wks"),i=r(78),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(170),o=r(53),u=Object.defineProperty;n.f=r(9)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(52),i=Math.min;t.exports=function(t){return 0<t?i(e(t),9007199254740991):0}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(59),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(95),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(6).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(50);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r=t.exports={version:"2.5.1"};"number"==typeof __e&&(__e=r)},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(10),i=r(74);t.exports=r(9)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var o=r(3),u=r(26),c=r(25),f=r(78)("src"),e="toString",i=Function[e],a=(""+i).split(e);r(49).inspectSource=function(t){return i.call(t)},(t.exports=function(t,n,r,e){var i="function"==typeof r;i&&(c(r,"name")||u(r,"name",n)),t[n]!==r&&(i&&(c(r,f)||u(r,f,t[n]?""+t[n]:a.join(String(n)))),t===o?t[n]=r:e?t[n]?t[n]=r:u(t,n,r):(delete t[n],u(t,n,r)))})(Function.prototype,e,function(){return"function"==typeof this&&this[f]||i.call(this)})},function(t,n,r){var e=r(1),i=r(4),u=r(50),c=/"/g,o=function(t,n,r,e){var i=String(u(t)),o="<"+n;return""!==r&&(o+=" "+r+'="'+String(e).replace(c,"&quot;")+'"'),o+">"+i+"</"+n+">"};t.exports=function(n,t){var r={};r[n]=t(o),e(e.P+e.F*i(function(){var t=""[n]('"');return t!==t.toLowerCase()||3<t.split('"').length}),"String",r)}},function(t,n,r){var e=r(64),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(116),i=r(74),o=r(32),u=r(53),c=r(25),f=r(170),a=Object.getOwnPropertyDescriptor;n.f=r(9)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(25),i=r(17),o=r(149)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(115),i=r(50);t.exports=function(t){return e(i(t))}},function(t,n){t.exports=function(t){if(null==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(16)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(6),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(0<t?e:r)(t)}},function(t,n,r){var i=r(21);t.exports=function(t,n){if(!i(t))return t;var r,e;if(n&&"function"==typeof(r=t.toString)&&!i(e=r.call(t)))return e;if("function"==typeof(r=t.valueOf)&&!i(e=r.call(t)))return e;if(!n&&"function"==typeof(r=t.toString)&&!i(e=r.call(t)))return e;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(6),i=r(24),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(16)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var o=r(19);t.exports=function(e,i,t){if(o(e),void 0===i)return e;switch(t){case 1:return function(t){return e.call(i,t)};case 2:return function(t,n){return e.call(i,t,n)};case 3:return function(t,n,r){return e.call(i,t,n,r)}}return function(){return e.apply(i,arguments)}}},function(t,n,r){"use strict";var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var b=r(46),x=r(115),S=r(17),w=r(11),e=r(134);t.exports=function(l,t){var h=1==l,v=2==l,p=3==l,d=4==l,y=6==l,g=5==l||y,m=t||e;return function(t,n,r){for(var e,i,o=S(t),u=x(o),c=b(n,r,3),f=w(u.length),a=0,s=h?m(t,f):v?m(t,0):void 0;a<f;a++)if((g||a in u)&&(i=c(e=u[a],a,o),l))if(h)s[a]=i;else if(i)switch(l){case 3:return!0;case 5:return e;case 6:return a;case 2:s.push(e)}else if(d)return!1;return y?-1:p||d?d:s}}},function(t,n){var r=t.exports={version:"2.5.1"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if(null==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var i=r(1),o=r(49),u=r(4);t.exports=function(t,n){var r=(o.Object||{})[t]||Object[t],e={};e[t]=n(r),i(i.S+i.F*u(function(){r(1)}),"Object",e)}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(0<t?e:r)(t)}},function(t,n,r){var i=r(5);t.exports=function(t,n){if(!i(t))return t;var r,e;if(n&&"function"==typeof(r=t.toString)&&!i(e=r.call(t)))return e;if("function"==typeof(r=t.valueOf)&&!i(e=r.call(t)))return e;if(!n&&"function"==typeof(r=t.toString)&&!i(e=r.call(t)))return e;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var d=r(6),y=r(24),g=r(92),m=r(13),b="prototype",x=function(t,n,r){var e,i,o,u=t&x.F,c=t&x.G,f=t&x.S,a=t&x.P,s=t&x.B,l=t&x.W,h=c?y:y[n]||(y[n]={}),v=h[b],p=c?d:f?d[n]:(d[n]||{})[b];for(e in c&&(r=n),r)(i=!u&&p&&void 0!==p[e])&&e in h||(o=i?p[e]:r[e],h[e]=c&&"function"!=typeof p[e]?r[e]:s&&i?g(o,d):l&&p[e]==o?function(e){var t=function(t,n,r){if(this instanceof e){switch(arguments.length){case 0:return new e;case 1:return new e(t);case 2:return new e(t,n)}return new e(t,n,r)}return e.apply(this,arguments)};return t[b]=e[b],t}(o):a&&"function"==typeof o?g(Function.call,o):o,a&&((h.virtual||(h.virtual={}))[e]=o,t&x.R&&v&&!v[e]&&m(v,e,o)))};x.F=1,x.G=2,x.S=4,x.P=8,x.B=16,x.W=32,x.U=64,x.R=128,t.exports=x},function(t,n,r){var o=r(191),e=r(1),i=r(128)("metadata"),u=i.store||(i.store=new(r(194))),c=function(t,n,r){var e=u.get(t);if(!e){if(!r)return;u.set(t,e=new o)}var i=e.get(n);if(!i){if(!r)return;e.set(n,i=new o)}return i};t.exports={store:u,map:c,has:function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},get:function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},set:function(t,n,r,e){c(r,e,!0).set(t,n)},keys:function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},key:function(t){return void 0===t||"symbol"==typeof t?t:String(t)},exp:function(t){e(e.S,"Reflect",t)}}},function(t,n,r){"use strict";if(r(9)){var g=r(70),m=r(3),b=r(4),x=r(1),S=r(130),e=r(155),h=r(46),w=r(68),i=r(74),_=r(26),o=r(75),u=r(52),O=r(11),E=r(189),c=r(77),f=r(53),a=r(25),M=r(114),P=r(5),v=r(17),p=r(141),j=r(71),F=r(31),A=r(72).f,d=r(157),s=r(78),l=r(7),y=r(48),L=r(117),N=r(129),T=r(158),I=r(80),k=r(123),R=r(76),C=r(133),D=r(162),G=r(10),W=r(30),U=G.f,V=W.f,B=m.RangeError,q=m.TypeError,z=m.Uint8Array,H="ArrayBuffer",K="Shared"+H,J="BYTES_PER_ELEMENT",Y="prototype",$=Array[Y],X=e.ArrayBuffer,Q=e.DataView,Z=y(0),tt=y(2),nt=y(3),rt=y(4),et=y(5),it=y(6),ot=L(!0),ut=L(!1),ct=T.values,ft=T.keys,at=T.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,mt=l("iterator"),bt=l("toStringTag"),xt=s("typed_constructor"),St=s("def_constructor"),wt=S.CONSTR,_t=S.TYPED,Ot=S.VIEW,Et="Wrong length!",Mt=y(1,function(t,n){return Lt(N(t,t[St]),n)}),Pt=b(function(){return 1===new z(new Uint16Array([1]).buffer)[0]}),jt=!!z&&!!z[Y].set&&b(function(){new z(1).set({})}),Ft=function(t,n){var r=u(t);if(r<0||r%n)throw B("Wrong offset!");return r},At=function(t){if(P(t)&&_t in t)return t;throw q(t+" is not a typed array!")},Lt=function(t,n){if(!(P(t)&&xt in t))throw q("It is not a typed array constructor!");return new t(n)},Nt=function(t,n){return Tt(N(t,t[St]),n)},Tt=function(t,n){for(var r=0,e=n.length,i=Lt(t,e);r<e;)i[r]=n[r++];return i},It=function(t,n,r){U(t,n,{get:function(){return this._d[r]}})},kt=function(t){var n,r,e,i,o,u,c=v(t),f=arguments.length,a=1<f?arguments[1]:void 0,s=void 0!==a,l=d(c);if(null!=l&&!p(l)){for(u=l.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(s&&2<f&&(a=h(a,arguments[2],2)),n=0,r=O(c.length),i=Lt(this,r);n<r;n++)i[n]=s?a(c[n],n):c[n];return i},Rt=function(){for(var t=0,n=arguments.length,r=Lt(this,n);t<n;)r[t]=arguments[t++];return r},Ct=!!z&&b(function(){gt.call(new z(1))}),Dt=function(){return gt.apply(Ct?dt.call(At(this)):At(this),arguments)},Gt={copyWithin:function(t,n){return D.call(At(this),t,n,2<arguments.length?arguments[2]:void 0)},every:function(t){return rt(At(this),t,1<arguments.length?arguments[1]:void 0)},fill:function(t){return C.apply(At(this),arguments)},filter:function(t){return Nt(this,tt(At(this),t,1<arguments.length?arguments[1]:void 0))},find:function(t){return et(At(this),t,1<arguments.length?arguments[1]:void 0)},findIndex:function(t){return it(At(this),t,1<arguments.length?arguments[1]:void 0)},forEach:function(t){Z(At(this),t,1<arguments.length?arguments[1]:void 0)},indexOf:function(t){return ut(At(this),t,1<arguments.length?arguments[1]:void 0)},includes:function(t){return ot(At(this),t,1<arguments.length?arguments[1]:void 0)},join:function(t){return vt.apply(At(this),arguments)},lastIndexOf:function(t){return st.apply(At(this),arguments)},map:function(t){return Mt(At(this),t,1<arguments.length?arguments[1]:void 0)},reduce:function(t){return lt.apply(At(this),arguments)},reduceRight:function(t){return ht.apply(At(this),arguments)},reverse:function(){for(var t,n=this,r=At(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(At(this),t,1<arguments.length?arguments[1]:void 0)},sort:function(t){return pt.call(At(this),t)},subarray:function(t,n){var r=At(this),e=r.length,i=c(t,e);return new(N(r,r[St]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,O((void 0===n?e:c(n,e))-i))}},Wt=function(t,n){return Nt(this,dt.call(At(this),t,n))},Ut=function(t){At(this);var n=Ft(arguments[1],1),r=this.length,e=v(t),i=O(e.length),o=0;if(r<i+n)throw B(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(At(this))},keys:function(){return ft.call(At(this))},values:function(){return ct.call(At(this))}},Bt=function(t,n){return P(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return Bt(t,n=f(n,!0))?i(2,t[n]):V(t,n)},zt=function(t,n,r){return!(Bt(t,n=f(n,!0))&&P(r)&&a(r,"value"))||a(r,"get")||a(r,"set")||r.configurable||a(r,"writable")&&!r.writable||a(r,"enumerable")&&!r.enumerable?U(t,n,r):(t[n]=r.value,t)};wt||(W.f=qt,G.f=zt),x(x.S+x.F*!wt,"Object",{getOwnPropertyDescriptor:qt,defineProperty:zt}),b(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Ht=o({},Gt);o(Ht,Vt),_(Ht,mt,Vt.values),o(Ht,{slice:Wt,set:Ut,constructor:function(){},toString:yt,toLocaleString:Dt}),It(Ht,"buffer","b"),It(Ht,"byteOffset","o"),It(Ht,"byteLength","l"),It(Ht,"length","e"),U(Ht,bt,{get:function(){return this[_t]}}),t.exports=function(t,l,n,o){var h=t+((o=!!o)?"Clamped":"")+"Array",r="get"+t,u="set"+t,v=m[h],c=v||{},e=v&&F(v),i=!v||!S.ABV,f={},a=v&&v[Y],p=function(t,i){U(t,i,{get:function(){return t=i,(n=this._d).v[r](t*l+n.o,Pt);var t,n},set:function(t){return n=i,r=t,e=this._d,o&&(r=(r=Math.round(r))<0?0:255<r?255:255&r),void e.v[u](n*l+e.o,r,Pt);var n,r,e},enumerable:!0})};i?(v=n(function(t,n,r,e){w(t,v,h,"_d");var i,o,u,c,f=0,a=0;if(P(n)){if(!(n instanceof X||(c=M(n))==H||c==K))return _t in n?Tt(v,n):kt.call(v,n);i=n,a=Ft(r,l);var s=n.byteLength;if(void 0===e){if(s%l)throw B(Et);if((o=s-a)<0)throw B(Et)}else if((o=O(e)*l)+a>s)throw B(Et);u=o/l}else u=E(n),i=new X(o=u*l);for(_(t,"_d",{b:i,o:a,l:o,e:u,v:new Q(i)});f<u;)p(t,f++)}),a=v[Y]=j(Ht),_(a,"constructor",v)):b(function(){v(1)})&&b(function(){new v(-1)})&&k(function(t){new v,new v(null),new v(1.5),new v(t)},!0)||(v=n(function(t,n,r,e){var i;return w(t,v,h),P(n)?n instanceof X||(i=M(n))==H||i==K?void 0!==e?new c(n,Ft(r,l),e):void 0!==r?new c(n,Ft(r,l)):new c(n):_t in n?Tt(v,n):kt.call(v,n):new c(E(n))}),Z(e!==Function.prototype?A(c).concat(A(e)):A(c),function(t){t in v||_(v,t,c[t])}),v[Y]=a,g||(a.constructor=v));var s=a[mt],d=!!s&&("values"==s.name||null==s.name),y=Vt.values;_(v,xt,!0),_(a,_t,h),_(a,Ot,!0),_(a,St,v),(o?new v(1)[bt]==h:bt in a)||U(a,bt,{get:function(){return h}}),f[h]=v,x(x.G+x.W+x.F*(v!=c),f),x(x.S,h,{BYTES_PER_ELEMENT:l}),x(x.S+x.F*b(function(){c.of.call(v,1)}),h,{from:kt,of:Rt}),J in a||_(a,J,l),x(x.P,h,Gt),R(h),x(x.P+x.F*jt,h,{set:Ut}),x(x.P+x.F*!d,h,Vt),g||a.toString==yt||(a.toString=yt),x(x.P+x.F*b(function(){new v(1).slice()}),h,{slice:Wt}),x(x.P+x.F*(b(function(){return[1,2].toLocaleString()!=new v([1,2]).toLocaleString()})||!b(function(){a.toLocaleString.call([1,2])})),h,{toLocaleString:Dt}),I[h]=d?s:y,g||d||_(a,mt,y)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(6).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(58)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var b=r(36),x=r(54),S=r(65),w=r(13),_=r(8),O=r(35),E=r(97),M=r(38),P=r(103),j=r(16)("iterator"),F=!([].keys&&"next"in[].keys()),A="values",L=function(){return this};t.exports=function(t,n,r,e,i,o,u){E(r,n,e);var c,f,a,s=function(t){if(!F&&t in p)return p[t];switch(t){case"keys":case A:return function(){return new r(this,t)}}return function(){return new r(this,t)}},l=n+" Iterator",h=i==A,v=!1,p=t.prototype,d=p[j]||p["@@iterator"]||i&&p[i],y=d||s(i),g=i?h?s("entries"):y:void 0,m="Array"==n&&p.entries||d;if(m&&(a=P(m.call(new t)))!==Object.prototype&&a.next&&(M(a,l,!0),b||_(a,j)||w(a,j,L)),h&&d&&d.name!==A&&(v=!0,y=function(){return d.call(this)}),b&&!u||!F&&!v&&p[j]||w(p,j,y),O[n]=y,O[l]=L,i)if(c={values:h?y:s(A),keys:o?y:s("keys"),entries:g},u)for(f in c)f in p||S(p,f,c[f]);else x(x.P+x.F*(F||v),n,c);return c}},function(t,n,e){var i=e(20),o=e(100),u=e(34),c=e(39)("IE_PROTO"),f=function(){},a="prototype",s=function(){var t,n=e(58)("iframe"),r=u.length;for(n.style.display="none",e(94).appendChild(n),n.src="javascript:",(t=n.contentWindow.document).open(),t.write("<script>document.F=Object<\/script>"),t.close(),s=t.F;r--;)delete s[a][u[r]];return s()};t.exports=Object.create||function(t,n){var r;return null!==t?(f[a]=i(t),r=new f,f[a]=null,r[c]=t):r=s(),void 0===n?r:o(r,n)}},function(t,n,r){var e=r(64),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var u=r(8),c=r(15),f=r(91)(!1),a=r(39)("IE_PROTO");t.exports=function(t,n){var r,e=c(t),i=0,o=[];for(r in e)r!=a&&u(e,r)&&o.push(r);for(;n.length>i;)u(e,r=n[i++])&&(~f(o,r)||o.push(r));return o}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;null==i[e]&&r(26)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(78)("meta"),i=r(5),o=r(25),u=r(10).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=t.exports={KEY:e,NEED:!1,fastKey:function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},getWeak:function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},onFreeze:function(t){return a&&l.NEED&&f(t)&&!o(t,e)&&s(t),t}}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n,r){var h=r(46),v=r(173),p=r(141),d=r(2),y=r(11),g=r(157),m={},b={};(n=t.exports=function(t,n,r,e,i){var o,u,c,f,a=i?function(){return t}:g(t),s=h(r,e,n?2:1),l=0;if("function"!=typeof a)throw TypeError(t+" is not iterable!");if(p(a)){for(o=y(t.length);l<o;l++)if((f=n?s(d(u=t[l])[0],u[1]):s(t[l]))===m||f===b)return f}else for(c=a.call(t);!(u=c.next()).done;)if((f=v(c,s,u.value,n))===m||f===b)return f}).BREAK=m,n.RETURN=b},function(t,n){t.exports=!1},function(t,n,e){var i=e(2),o=e(179),u=e(137),c=e(149)("IE_PROTO"),f=function(){},a="prototype",s=function(){var t,n=e(136)("iframe"),r=u.length;for(n.style.display="none",e(139).appendChild(n),n.src="javascript:",(t=n.contentWindow.document).open(),t.write("<script>document.F=Object<\/script>"),t.close(),s=t.F;r--;)delete s[a][u[r]];return s()};t.exports=Object.create||function(t,n){var r;return null!==t?(f[a]=i(t),r=new f,f[a]=null,r[c]=t):r=s(),void 0===n?r:o(r,n)}},function(t,n,r){var e=r(181),i=r(137).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(181),i=r(137);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n,r){var i=r(27);t.exports=function(t,n,r){for(var e in n)i(t,e,n[e],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(9),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(52),i=Math.max,o=Math.min;t.exports=function(t,n){return(t=e(t))<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports={}},function(t,n,r){var e=r(10).f,i=r(25),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var u=r(1),e=r(50),c=r(4),f=r(153),i="["+f+"]",o=RegExp("^"+i+i+"*"),a=RegExp(i+i+"*$"),s=function(t,n,r){var e={},i=c(function(){return!!f[t]()||"​"!="​"[t]()}),o=e[t]=i?n(l):f[t];r&&(e[r]=o),u(u.P+u.F*i,"String",e)},l=s.trim=function(t,n){return t=String(e(t)),1&n&&(t=t.replace(o,"")),2&n&&(t=t.replace(a,"")),t};t.exports=s},function(t,n,r){var e=r(5);t.exports=function(t,n){if(!e(t)||t._t!==n)throw TypeError("Incompatible receiver, "+n+" required!");return t}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){t.exports={default:r(88),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=e(r(85)),o=e(r(84)),u="function"==typeof o.default&&"symbol"==typeof i.default?function(t){return typeof t}:function(t){return t&&"function"==typeof o.default&&t.constructor===o.default&&t!==o.default.prototype?"symbol":typeof t};n.default="function"==typeof o.default&&"symbol"===u(i.default)?function(t){return void 0===t?"undefined":u(t)}:function(t){return t&&"function"==typeof o.default&&t.constructor===o.default&&t!==o.default.prototype?"symbol":void 0===t?"undefined":u(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(24).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var f=r(15),a=r(106),s=r(105);t.exports=function(c){return function(t,n,r){var e,i=f(t),o=a(i.length),u=s(r,o);if(c&&n!=n){for(;u<o;)if((e=i[u++])!=e)return!0}else for(;u<o;u++)if((c||u in i)&&i[u]===n)return c||u||0;return!c&&-1}}},function(t,n,r){var o=r(89);t.exports=function(e,i,t){if(o(e),void 0===i)return e;switch(t){case 1:return function(t){return e.call(i,t)};case 2:return function(t,n){return e.call(i,t,n)};case 3:return function(t,n,r){return e.call(i,t,n,r)}}return function(){return e.apply(i,arguments)}}},function(t,n,r){var c=r(29),f=r(63),a=r(37);t.exports=function(t){var n=c(t),r=f.f;if(r)for(var e,i=r(t),o=a.f,u=0;i.length>u;)o.call(t,e=i[u++])&&n.push(e);return n}},function(t,n,r){var e=r(6).document;t.exports=e&&e.documentElement},function(t,n,r){var e=r(57);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(57);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(61),i=r(22),o=r(38),u={};r(13)(u,r(16)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=t.exports={KEY:e,NEED:!1,fastKey:function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},getWeak:function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},onFreeze:function(t){return a&&l.NEED&&f(t)&&!o(t,e)&&s(t),t}}},function(t,n,r){var u=r(14),c=r(20),f=r(29);t.exports=r(12)?Object.defineProperties:function(t,n){c(t);for(var r,e=f(n),i=e.length,o=0;o<i;)u.f(t,r=e[o++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(15),u=r(42),c=r(8),f=r(59),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(15),i=r(62).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[];t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?function(t){try{return i(t)}catch(t){return u.slice()}}(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(79),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var f=r(41),a=r(33);t.exports=function(c){return function(t,n){var r,e,i=String(a(t)),o=f(n),u=i.length;return o<0||u<=o?c?"":void 0:(r=i.charCodeAt(o))<55296||56319<r||o+1===u||(e=i.charCodeAt(o+1))<56320||57343<e?c?i.charAt(o):r:c?i.slice(o,o+2):e-56320+(r-55296<<10)+65536}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return(t=e(t))<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return 0<t?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(90),i=r(98),o=r(35),u=r(15);t.exports=r(60)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):i(0,"keys"==n?r:"values"==n?t[r]:[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(60)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(6),u=r(8),i=r(12),o=r(54),c=r(65),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(16),p=r(44),d=r(43),y=r(93),g=r(96),m=r(20),b=r(15),x=r(42),S=r(22),w=r(61),_=r(102),O=r(101),E=r(14),M=r(29),P=O.f,j=E.f,F=_.f,A=e.Symbol,L=e.JSON,N=L&&L.stringify,T="prototype",I=v("_hidden"),k=v("toPrimitive"),R={}.propertyIsEnumerable,C=s("symbol-registry"),D=s("symbols"),G=s("op-symbols"),W=Object[T],U="function"==typeof A,V=e.QObject,B=!V||!V[T]||!V[T].findChild,q=i&&a(function(){return 7!=w(j({},"a",{get:function(){return j(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=P(W,n);e&&delete W[n],j(t,n,r),e&&t!==W&&j(W,n,e)}:j,z=function(t){var n=D[t]=w(A[T]);return n._k=t,n},H=U&&"symbol"==typeof A.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof A},K=function(t,n,r){return t===W&&K(G,n,r),m(t),n=x(n,!0),m(r),u(D,n)?(r.enumerable?(u(t,I)&&t[I][n]&&(t[I][n]=!1),r=w(r,{enumerable:S(0,!1)})):(u(t,I)||j(t,I,S(1,{})),t[I][n]=!0),q(t,n,r)):j(t,n,r)},J=function(t,n){m(t);for(var r,e=y(n=b(n)),i=0,o=e.length;i<o;)K(t,r=e[i++],n[r]);return t},Y=function(t){var n=R.call(this,t=x(t,!0));return!(this===W&&u(D,t)&&!u(G,t))&&(!(n||!u(this,t)||!u(D,t)||u(this,I)&&this[I][t])||n)},$=function(t,n){if(t=b(t),n=x(n,!0),t!==W||!u(D,n)||u(G,n)){var r=P(t,n);return!r||!u(D,n)||u(t,I)&&t[I][n]||(r.enumerable=!0),r}},X=function(t){for(var n,r=F(b(t)),e=[],i=0;r.length>i;)u(D,n=r[i++])||n==I||n==f||e.push(n);return e},Q=function(t){for(var n,r=t===W,e=F(r?G:b(t)),i=[],o=0;e.length>o;)!u(D,n=e[o++])||r&&!u(W,n)||i.push(D[n]);return i};U||(c((A=function(){if(this instanceof A)throw TypeError("Symbol is not a constructor!");var n=h(0<arguments.length?arguments[0]:void 0),r=function(t){this===W&&r.call(G,t),u(this,I)&&u(this[I],n)&&(this[I][n]=!1),q(this,n,S(1,t))};return i&&B&&q(W,n,{configurable:!0,set:r}),z(n)})[T],"toString",function(){return this._k}),O.f=$,E.f=K,r(62).f=_.f=X,r(37).f=Y,r(63).f=Q,i&&!r(36)&&c(W,"propertyIsEnumerable",Y,!0),p.f=function(t){return z(v(t))}),o(o.G+o.W+o.F*!U,{Symbol:A});for(var Z="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),tt=0;Z.length>tt;)v(Z[tt++]);for(var nt=M(v.store),rt=0;nt.length>rt;)d(nt[rt++]);o(o.S+o.F*!U,"Symbol",{for:function(t){return u(C,t+="")?C[t]:C[t]=A(t)},keyFor:function(t){if(!H(t))throw TypeError(t+" is not a symbol!");for(var n in C)if(C[n]===t)return n},useSetter:function(){B=!0},useSimple:function(){B=!1}}),o(o.S+o.F*!U,"Object",{create:function(t,n){return void 0===n?w(t):J(w(t),n)},defineProperty:K,defineProperties:J,getOwnPropertyDescriptor:$,getOwnPropertyNames:X,getOwnPropertySymbols:Q}),L&&o(o.S+o.F*(!U||a(function(){var t=A();return"[null]"!=N([t])||"{}"!=N({a:t})||"{}"!=N(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!H(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return"function"==typeof(n=e[1])&&(r=n),!r&&g(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!H(n))return n}),e[1]=n,N.apply(L,e)}}}),A[T][k]||r(13)(A[T],k,A[T].valueOf),l(A,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(6),i=r(13),o=r(35),u=r(16)("toStringTag"),c="CSSRuleList,CSSStyleDeclaration,CSSValueList,ClientRectList,DOMRectList,DOMStringList,DOMTokenList,DataTransferItemList,FileList,HTMLAllCollection,HTMLCollection,HTMLFormElement,HTMLSelectElement,MediaList,MimeTypeArray,NamedNodeMap,NodeList,PaintRequestList,Plugin,PluginArray,SVGLengthList,SVGNumberList,SVGPathSegList,SVGPointList,SVGStringList,SVGTransformList,SourceBufferList,StyleSheetList,TextTrackCueList,TextTrackList,TouchList".split(","),f=0;f<c.length;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var i=r(45),o=r(7)("toStringTag"),u="Arguments"==i(function(){return arguments}());t.exports=function(t){var n,r,e;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=function(t,n){try{return t[n]}catch(t){}}(n=Object(t),o))?r:u?i(n):"Object"==(e=i(n))&&"function"==typeof n.callee?"Arguments":e}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var f=r(32),a=r(11),s=r(77);t.exports=function(c){return function(t,n,r){var e,i=f(t),o=a(i.length),u=s(r,o);if(c&&n!=n){for(;u<o;)if((e=i[u++])!=e)return!0}else for(;u<o;u++)if((c||u in i)&&i[u]===n)return c||u||0;return!c&&-1}}},function(t,n,r){"use strict";var g=r(3),m=r(1),b=r(27),x=r(75),S=r(67),w=r(69),_=r(68),O=r(5),E=r(4),M=r(123),P=r(81),j=r(140);t.exports=function(e,t,n,r,i,o){var u=g[e],c=u,f=i?"set":"add",a=c&&c.prototype,s={},l=function(t){var r=a[t];b(a,t,"delete"==t?function(t){return!(o&&!O(t))&&r.call(this,0===t?0:t)}:"has"==t?function(t){return!(o&&!O(t))&&r.call(this,0===t?0:t)}:"get"==t?function(t){return o&&!O(t)?void 0:r.call(this,0===t?0:t)}:"add"==t?function(t){return r.call(this,0===t?0:t),this}:function(t,n){return r.call(this,0===t?0:t,n),this})};if("function"==typeof c&&(o||a.forEach&&!E(function(){(new c).entries().next()}))){var h=new c,v=h[f](o?{}:-0,1)!=h,p=E(function(){h.has(1)}),d=M(function(t){new c(t)}),y=!o&&E(function(){for(var t=new c,n=5;n--;)t[f](n,n);return!t.has(-0)});d||(((c=t(function(t,n){_(t,c,e);var r=j(new u,t,c);return null!=n&&w(n,i,r[f],r),r})).prototype=a).constructor=c),(p||y)&&(l("delete"),l("has"),i&&l("get")),(y||v)&&l(f),o&&a.clear&&delete a.clear}else c=r.getConstructor(t,e,i,f),x(c.prototype,n),S.NEED=!0;return P(c,e),s[e]=c,m(m.G+m.W+m.F*(c!=u),s),o||r.setStrong(c,e,i),c}},function(t,n,r){"use strict";var c=r(26),f=r(27),a=r(4),s=r(50),l=r(7);t.exports=function(n,t,r){var e=l(n),i=r(s,e,""[n]),o=i[0],u=i[1];a(function(){var t={};return t[e]=function(){return 7},7!=""[n](t)})&&(f(String.prototype,n,o),c(RegExp.prototype,e,2==t?function(t,n){return u.call(t,this,n)}:function(t){return u.call(t,this)}))}},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){var e=r(5),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var o=r(7)("iterator"),u=!1;try{var e=[7][o]();e.return=function(){u=!0},Array.from(e,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!u)return!1;var r=!1;try{var e=[7],i=e[o]();i.next=function(){return{done:r=!0}},e[o]=function(){return i},t(e)}catch(t){}return r}},function(t,n,r){"use strict";t.exports=r(70)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){"use strict";var e=r(1),u=r(19),c=r(46),f=r(69);t.exports=function(t){e(e.S,t,{from:function(t){var n,r,e,i,o=arguments[1];return u(this),(n=void 0!==o)&&u(o),null==t?new this:(r=[],n?(e=0,i=c(o,arguments[2],2),f(t,!1,function(t){r.push(i(t,e++))})):f(t,!1,r.push,r),new this(r))}})}},function(t,n,r){"use strict";var e=r(1);t.exports=function(t){e(e.S,t,{of:function(){for(var t=arguments.length,n=Array(t);t--;)n[t]=arguments[t];return new this(n)}})}},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){var i=r(2),o=r(19),u=r(7)("species");t.exports=function(t,n){var r,e=i(t).constructor;return void 0===e||null==(r=i(e)[u])?n:o(r)}},function(t,n,r){for(var e,i=r(3),o=r(26),u=r(78),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r,e={versions:(r=window.navigator.userAgent,{trident:-1<r.indexOf("Trident"),presto:-1<r.indexOf("Presto"),webKit:-1<r.indexOf("AppleWebKit"),gecko:-1<r.indexOf("Gecko")&&-1==r.indexOf("KHTML"),mobile:!!r.match(/AppleWebKit.*Mobile.*/),ios:!!r.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:-1<r.indexOf("Android")||-1<r.indexOf("Linux"),iPhone:-1<r.indexOf("iPhone")||-1<r.indexOf("Mac"),iPad:-1<r.indexOf("iPad"),webApp:-1==r.indexOf("Safari"),weixin:-1==r.indexOf("MicroMessenger")})};t.exports=e},function(t,n,r){"use strict";var e,l=(e=r(86))&&e.__esModule?e:{default:e},h=function(){function n(t,n,r){return n||r?String.fromCharCode(n||r):o[t]||t}function r(t){return s[t]}var e=/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,i=/['<> "&]/g,o={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},u=/\u00a0/g,c=/<br\s*\/?>/gi,f=/\r?\n/g,a=/\s/g,s={};for(var t in o)s[o[t]]=t;return o["&apos;"]="'",s["'"]="&#39;",{encode:function(t){return t?(""+t).replace(i,r).replace(f,"<br/>").replace(a,"&nbsp;"):""},decode:function(t){return t?(""+t).replace(c,"\n").replace(e,n).replace(u," "):""},encodeBase16:function(t){if(!t)return t;for(var n=[],r=0,e=(t+="").length;r<e;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;for(var n=[],r=0,e=(t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")})).length;r<e;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;for(var n=[],r=0,e=(t+="").length;r<e;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;n<r;n++)t[n]=h.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,l.default)(t)))for(var e in t)t[e]=h.encodeObject(t[e]);else if("string"==typeof t)return h.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=h},function(t,n,r){"use strict";var c=r(17),f=r(77),a=r(11);t.exports=function(t){for(var n=c(this),r=a(n.length),e=arguments.length,i=f(1<e?arguments[1]:void 0,r),o=2<e?arguments[2]:void 0,u=void 0===o?r:f(o,r);i<u;)n[i++]=t;return n}},function(t,n,r){var e=r(211);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(10),i=r(74);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(5),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(n){var r=/./;try{"/./"[n](r)}catch(t){try{return r[e]=!1,!"/./"[n](r)}catch(n){}}return!0}},function(t,n,r){var e=r(3).document;t.exports=e&&e.documentElement},function(t,n,r){var o=r(5),u=r(148).set;t.exports=function(t,n,r){var e,i=n.constructor;return i!==r&&"function"==typeof i&&(e=i.prototype)!==r.prototype&&o(e)&&u&&u(t,e),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){"use strict";var e=r(71),i=r(74),o=r(81),u={};r(26)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var b=r(70),x=r(1),S=r(27),w=r(26),_=r(25),O=r(80),E=r(142),M=r(81),P=r(31),j=r(7)("iterator"),F=!([].keys&&"next"in[].keys()),A="values",L=function(){return this};t.exports=function(t,n,r,e,i,o,u){E(r,n,e);var c,f,a,s=function(t){if(!F&&t in p)return p[t];switch(t){case"keys":case A:return function(){return new r(this,t)}}return function(){return new r(this,t)}},l=n+" Iterator",h=i==A,v=!1,p=t.prototype,d=p[j]||p["@@iterator"]||i&&p[i],y=d||s(i),g=i?h?s("entries"):y:void 0,m="Array"==n&&p.entries||d;if(m&&(a=P(m.call(new t)))!==Object.prototype&&a.next&&(M(a,l,!0),b||_(a,j)||w(a,j,L)),h&&d&&d.name!==A&&(v=!0,y=function(){return d.call(this)}),b&&!u||!F&&!v&&p[j]||w(p,j,y),O[n]=y,O[l]=L,i)if(c={values:h?y:s(A),keys:o?y:s("keys"),entries:g},u)for(f in c)f in p||S(p,f,c[f]);else x(x.P+x.F*(F||v),n,c);return c}},function(t,n){var r=Math.expm1;t.exports=!r||22025.465794806718<r(10)||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:-1e-6<t&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var c=r(3),f=r(154).set,a=c.MutationObserver||c.WebKitMutationObserver,s=c.process,l=c.Promise,h="process"==r(45)(s);t.exports=function(){var r,e,i,t=function(){var t,n;for(h&&(t=s.domain)&&t.exit();r;){n=r.fn,r=r.next;try{n()}catch(t){throw r?i():e=void 0,t}}e=void 0,t&&t.enter()};if(h)i=function(){s.nextTick(t)};else if(a){var n=!0,o=document.createTextNode("");new a(t).observe(o,{characterData:!0}),i=function(){o.data=n=!n}}else if(l&&l.resolve){var u=l.resolve();i=function(){u.then(t)}}else i=function(){f.call(c,t)};return function(t){var n={fn:t,next:void 0};e&&(e.next=n),r||(r=n,i()),e=n}}},function(t,n,r){"use strict";function e(t){var r,e;this.promise=new t(function(t,n){if(void 0!==r||void 0!==e)throw TypeError("Bad Promise constructor");r=t,e=n}),this.resolve=i(r),this.reject=i(e)}var i=r(19);t.exports.f=function(t){return new e(t)}},function(t,n,i){var r=i(5),e=i(2),o=function(t,n){if(e(t),!r(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,r,e){try{(e=i(46)(Function.call,i(30).f(Object.prototype,"__proto__").set,2))(t,[]),r=!(t instanceof Array)}catch(t){r=!0}return function(t,n){return o(t,n),r?t.__proto__=n:e(t,n),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(128)("keys"),i=r(78);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var f=r(52),a=r(50);t.exports=function(c){return function(t,n){var r,e,i=String(a(t)),o=f(n),u=i.length;return o<0||u<=o?c?"":void 0:(r=i.charCodeAt(o))<55296||56319<r||o+1===u||(e=i.charCodeAt(o+1))<56320||57343<e?c?i.charAt(o):r:c?i.slice(o,o+2):e-56320+(r-55296<<10)+65536}}},function(t,n,r){var e=r(122),i=r(50);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var i=r(52),o=r(50);t.exports=function(t){var n=String(o(this)),r="",e=i(t);if(e<0||e==1/0)throw RangeError("Count can't be negative");for(;0<e;(e>>>=1)&&(n+=n))1&e&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(46),c=r(171),f=r(139),a=r(136),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=s.Dispatch,y=0,g={},m="onreadystatechange",b=function(){var t=+this;if(g.hasOwnProperty(t)){var n=g[t];delete g[t],n()}},x=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return g[++y]=function(){c("function"==typeof t?t:Function(t),n)},e(y),y},v=function(t){delete g[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:d&&d.now?e=function(t){d.now(u(b,t,1))}:p?(o=(i=new p).port2,i.port1.onmessage=x,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",x,!1)):e=m in a("script")?function(t){f.appendChild(a("script"))[m]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";function e(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?W(2,-24)-W(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for((t=G(t))!=t||t===C?(i=t!=t?1:0,e=f):(e=U(V(t)/B),t*(o=W(2,-e))<1&&(e--,o*=2),2<=(t+=1<=e+a?s/o:s*W(2,1-a))*o&&(e++,o/=2),f<=e+a?(i=0,e=f):1<=e+a?(i=(t*o-1)*W(2,n),e+=a):(i=t*W(2,a-1)*W(2,n),e=0));8<=n;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;0<c;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u}function i(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;0<c;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;0<c;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-C:C;e+=W(2,n),s-=u}return(a?-1:1)*e*W(2,s-n)}function o(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]}function u(t){return[255&t]}function c(t){return[255&t,t>>8&255]}function f(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]}function a(t){return e(t,52,8)}function s(t){return e(t,23,4)}function l(t,n,r){M(t[L],n,{get:function(){return this[r]}})}function h(t,n,r,e){var i=O(+r);if(i+n>t[K])throw R(N);var o=t[H]._b,u=i+t[J],c=o.slice(u,u+n);return e?c:c.reverse()}function v(t,n,r,e,i,o){var u=O(+r);if(u+n>t[K])throw R(N);for(var c=t[H]._b,f=u+t[J],a=e(+i),s=0;s<n;s++)c[f+s]=a[o?s:n-s-1]}var p=r(3),d=r(9),y=r(70),g=r(130),m=r(26),b=r(75),x=r(4),S=r(68),w=r(52),_=r(11),O=r(189),E=r(72).f,M=r(10).f,P=r(133),j=r(81),F="ArrayBuffer",A="DataView",L="prototype",N="Wrong index!",T=p[F],I=p[A],k=p.Math,R=p.RangeError,C=p.Infinity,D=T,G=k.abs,W=k.pow,U=k.floor,V=k.log,B=k.LN2,q="byteLength",z="byteOffset",H=d?"_b":"buffer",K=d?"_l":q,J=d?"_o":z;if(g.ABV){if(!x(function(){T(1)})||!x(function(){new T(-1)})||x(function(){return new T,new T(1.5),new T(NaN),T.name!=F})){for(var Y,$=(T=function(t){return S(this,T),new D(O(t))})[L]=D[L],X=E(D),Q=0;X.length>Q;)(Y=X[Q++])in T||m(T,Y,D[Y]);y||($.constructor=T)}var Z=new I(new T(2)),tt=I[L].setInt8;Z.setInt8(0,2147483648),Z.setInt8(1,2147483649),!Z.getInt8(0)&&Z.getInt8(1)||b(I[L],{setInt8:function(t,n){tt.call(this,t,n<<24>>24)},setUint8:function(t,n){tt.call(this,t,n<<24>>24)}},!0)}else T=function(t){S(this,T,F);var n=O(t);this._b=P.call(Array(n),0),this[K]=n},I=function(t,n,r){S(this,I,A),S(t,T,A);var e=t[K],i=w(n);if(i<0||e<i)throw R("Wrong offset!");if(e<i+(r=void 0===r?e-i:_(r)))throw R("Wrong length!");this[H]=t,this[J]=i,this[K]=r},d&&(l(T,q,"_l"),l(I,"buffer","_b"),l(I,q,"_l"),l(I,z,"_o")),b(I[L],{getInt8:function(t){return h(this,1,t)[0]<<24>>24},getUint8:function(t){return h(this,1,t)[0]},getInt16:function(t){var n=h(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=h(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return o(h(this,4,t,arguments[1]))},getUint32:function(t){return o(h(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return i(h(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return i(h(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){v(this,1,t,u,n)},setUint8:function(t,n){v(this,1,t,u,n)},setInt16:function(t,n){v(this,2,t,c,n,arguments[2])},setUint16:function(t,n){v(this,2,t,c,n,arguments[2])},setInt32:function(t,n){v(this,4,t,f,n,arguments[2])},setUint32:function(t,n){v(this,4,t,f,n,arguments[2])},setFloat32:function(t,n){v(this,4,t,s,n,arguments[2])},setFloat64:function(t,n){v(this,8,t,a,n,arguments[2])}});j(T,F),j(I,A),m(I[L],g.VIEW,!0),n[F]=T,n[A]=I},function(t,n,r){var e=r(3),i=r(49),o=r(70),u=r(190),c=r(10).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(49).getIteratorMethod=function(t){if(null!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(66),i=r(174),o=r(80),u=r(32);t.exports=r(143)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):i(0,"keys"==n?r:"values"==n?t[r]:[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){t.exports=function(t,n){t.classList?t.classList.add(n):t.className+=" "+n}},function(t,n){t.exports=function(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var a=r(17),s=r(77),l=r(11);t.exports=[].copyWithin||function(t,n){var r=a(this),e=l(r.length),i=s(t,e),o=s(n,e),u=2<arguments.length?arguments[2]:void 0,c=Math.min((void 0===u?e:s(u,e))-o,e-i),f=1;for(o<i&&i<o+c&&(f=-1,o+=c-1,i+=c-1);0<c--;)o in r?r[i]=r[o]:delete r[i],i+=f,o+=f;return r}},function(t,n,r){var e=r(69);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var s=r(19),l=r(17),h=r(115),v=r(11);t.exports=function(t,n,r,e,i){s(n);var o=l(t),u=h(o),c=v(o.length),f=i?c-1:0,a=i?-1:1;if(r<2)for(;;){if(f in u){e=u[f],f+=a;break}if(f+=a,i?f<0:c<=f)throw TypeError("Reduce of empty array with no initial value")}for(;i?0<=f:f<c;f+=a)f in u&&(e=n(e,u[f],f,o));return e}},function(t,n,r){"use strict";var o=r(19),u=r(5),c=r(171),f=[].slice,a={};t.exports=Function.bind||function(n){var r=o(this),e=f.call(arguments,1),i=function(){var t=e.concat(f.call(arguments));return this instanceof i?function(t,n,r){if(!(n in a)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";a[n]=Function("F,a","return new F("+e.join(",")+")")}return a[n](t,r)}(r,t.length,t):c(r,t,n)};return u(r.prototype)&&(i.prototype=r.prototype),i}},function(t,n,r){"use strict";var u=r(10).f,c=r(71),f=r(75),a=r(46),s=r(68),l=r(69),e=r(143),i=r(174),o=r(76),h=r(9),v=r(67).fastKey,p=r(83),d=h?"_s":"size",y=function(t,n){var r,e=v(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,o,r,e){var i=t(function(t,n){s(t,i,o,"_i"),t._t=o,t._i=c(null),t._f=void 0,t._l=void 0,t[d]=0,null!=n&&l(n,r,t[e],t)});return f(i.prototype,{clear:function(){for(var t=p(this,o),n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=p(this,o),r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){p(this,o);for(var n,r=a(t,1<arguments.length?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(p(this,o),t)}}),h&&u(i.prototype,"size",{get:function(){return p(this,o)[d]}}),i},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=v(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,r,n){e(t,r,function(t,n){this._t=p(t,r),this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?i(0,"keys"==n?r.k:"values"==n?r.v:[r.k,r.v]):(t._t=void 0,i(1))},n?"entries":"values",!n,!0),o(r)}}},function(t,n,r){var e=r(114),i=r(163);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var u=r(75),c=r(67).getWeak,i=r(2),f=r(5),a=r(68),s=r(69),e=r(48),l=r(25),h=r(83),o=e(5),v=e(6),p=0,d=function(t){return t._l||(t._l=new y)},y=function(){this.a=[]},g=function(t,n){return o(t.a,function(t){return t[0]===n})};y.prototype={get:function(t){var n=g(this,t);if(n)return n[1]},has:function(t){return!!g(this,t)},set:function(t,n){var r=g(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(n){var t=v(this.a,function(t){return t[0]===n});return~t&&this.a.splice(t,1),!!~t}},t.exports={getConstructor:function(t,r,e,i){var o=t(function(t,n){a(t,o,r,"_i"),t._t=r,t._i=p++,t._l=void 0,null!=n&&s(n,e,t[i],t)});return u(o.prototype,{delete:function(t){if(!f(t))return!1;var n=c(t);return!0===n?d(h(this,r)).delete(t):n&&l(n,this._i)&&delete n[this._i]},has:function(t){if(!f(t))return!1;var n=c(t);return!0===n?d(h(this,r)).has(t):n&&l(n,this._i)}}),o},def:function(t,n,r){var e=c(i(n),!0);return!0===e?d(t).set(n,r):e[t._i]=r,t},ufstore:d}},function(t,n,r){"use strict";var p=r(121),d=r(5),y=r(11),g=r(46),m=r(7)("isConcatSpreadable");t.exports=function t(n,r,e,i,o,u,c,f){for(var a,s,l=o,h=0,v=!!c&&g(c,f,3);h<i;){if(h in e){if(a=v?v(e[h],h,r):e[h],s=!1,d(a)&&(s=void 0!==(s=a[m])?!!s:p(a)),s&&0<u)l=t(n,r,a,y(a.length),l,u-1)-1;else{if(9007199254740991<=l)throw TypeError();n[l]=a}l++}h++}return l}},function(t,n,r){t.exports=!r(9)&&!r(4)(function(){return 7!=Object.defineProperty(r(136)("div"),"a",{get:function(){return 7}}).a})},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(5),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var o=r(2);t.exports=function(t,n,r,e){try{return e?n(o(r)[0],r[1]):n(r)}catch(n){var i=t.return;throw void 0!==i&&o(i.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var o=r(145),e=Math.pow,u=e(2,-52),c=e(2,-23),f=e(2,127)*(2-c),a=e(2,-126);t.exports=Math.fround||function(t){var n,r,e=Math.abs(t),i=o(t);return e<a?i*(e/a/c+1/u-1/u)*a*c:(r=(n=(1+c/u)*e)-(n-e))>f||r!=r?i*(1/0):i*r}},function(t,n){t.exports=Math.log1p||function(t){return-1e-8<(t=+t)&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n){t.exports=Math.scale||function(t,n,r,e,i){return 0===arguments.length||t!=t||n!=n||r!=r||e!=e||i!=i?NaN:t===1/0||t===-1/0?t:(t-n)*(i-e)/(r-n)+e}},function(t,n,r){"use strict";var h=r(73),v=r(125),p=r(116),d=r(17),y=r(115),i=Object.assign;t.exports=!i||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=i({},t)[r]||Object.keys(i({},n)).join("")!=e})?function(t,n){for(var r=d(t),e=arguments.length,i=1,o=v.f,u=p.f;i<e;)for(var c,f=y(arguments[i++]),a=o?h(f).concat(o(f)):h(f),s=a.length,l=0;l<s;)u.call(f,c=a[l++])&&(r[c]=f[c]);return r}:i},function(t,n,r){var u=r(10),c=r(2),f=r(73);t.exports=r(9)?Object.defineProperties:function(t,n){c(t);for(var r,e=f(n),i=e.length,o=0;o<i;)u.f(t,r=e[o++],n[r]);return t}},function(t,n,r){var e=r(32),i=r(72).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[];t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?function(t){try{return i(t)}catch(t){return u.slice()}}(t):i(e(t))}},function(t,n,r){var u=r(25),c=r(32),f=r(117)(!1),a=r(149)("IE_PROTO");t.exports=function(t,n){var r,e=c(t),i=0,o=[];for(r in e)r!=a&&u(e,r)&&o.push(r);for(;n.length>i;)u(e,r=n[i++])&&(~f(o,r)||o.push(r));return o}},function(t,n,r){var f=r(73),a=r(32),s=r(116).f;t.exports=function(c){return function(t){for(var n,r=a(t),e=f(r),i=e.length,o=0,u=[];o<i;)s.call(r,n=e[o++])&&u.push(c?[n,r[n]]:r[n]);return u}}},function(t,n,r){var e=r(72),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(153)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(153),u=/^[-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=function(t){try{return{e:!1,v:t()}}catch(t){return{e:!0,v:t}}}},function(t,n,r){var e=r(2),i=r(5),o=r(147);t.exports=function(t,n){if(e(t),i(n)&&n.constructor===t)return n;var r=o.f(t);return(0,r.resolve)(n),r.promise}},function(t,n,r){var s=r(11),l=r(152),h=r(50);t.exports=function(t,n,r,e){var i=String(h(t)),o=i.length,u=void 0===r?" ":String(r),c=s(n);if(c<=o||""==u)return i;var f=c-o,a=l.call(u,Math.ceil(f/u.length));return a.length>f&&(a=a.slice(0,f)),e?a+i:i+a}},function(t,n,r){var e=r(52),i=r(11);t.exports=function(t){if(void 0===t)return 0;var n=e(t),r=i(n);if(n!==r)throw RangeError("Wrong length!");return r}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(166),i=r(83);t.exports=r(118)("Map",function(t){return function(){return t(this,0<arguments.length?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(i(this,"Map"),t);return n&&n.v},set:function(t,n){return e.def(i(this,"Map"),0===t?0:t,n)}},e,!0)},function(t,n,r){r(9)&&"g"!=/./g.flags&&r(10).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(166),i=r(83);t.exports=r(118)("Set",function(t){return function(){return t(this,0<arguments.length?arguments[0]:void 0)}},{add:function(t){return e.def(i(this,"Set"),t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var o,e=r(48)(0),u=r(27),i=r(67),c=r(178),f=r(168),a=r(5),s=r(4),l=r(83),h="WeakMap",v=i.getWeak,p=Object.isExtensible,d=f.ufstore,y={},g=function(t){return function(){return t(this,0<arguments.length?arguments[0]:void 0)}},m={get:function(t){if(a(t)){var n=v(t);return!0===n?d(l(this,h)).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(l(this,h),t,n)}},b=t.exports=r(118)(h,g,m,f,!0,!0);s(function(){return 7!=(new b).set((Object.freeze||Object)(y),7).get(y)})&&(c((o=f.getConstructor(g,h)).prototype,m),i.NEED=!0,e(["delete","has","get","set"],function(e){var t=b.prototype,i=t[e];u(t,e,function(t,n){if(!a(t)||p(t))return i.call(this,t,n);this._f||(this._f=new o);var r=this._f[e](t,n);return"set"==e?this:r})}))},,,,function(t,n){"use strict";t.exports={init:function(){var t=document.querySelector("#page-nav");t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&lt; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &gt;</a>'),yiliaConfig&&yiliaConfig.open_in_new&&document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")}),yiliaConfig&&yiliaConfig.toc_hide_index&&document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"});var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n,r,e,i){var o=function(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}(t),u=function(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}(t)-n;if(u-r<=i){var c=t.$newDom;c||(c=t.cloneNode(!0),(0,a.default)(t,c),(t.$newDom=c).style.position="fixed",c.style.top=(r||u)+"px",c.style.left=o+"px",c.style.zIndex=e||2,c.style.width="100%",c.style.color="#fff"),c.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var f=t.$newDom;f&&(f.style.visibility="hidden")}}function o(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");i(t,document.body.scrollTop,-63,2,0),i(n,document.body.scrollTop,1,3,0)}var f=e(r(159)),a=e((e(r(160)),r(410))),u=e(r(131)),c=e(r(198)),s=r(132);u.default.versions.mobile&&window.screen.width<800&&(function(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var i=t[r];o=n,u=i.getAttribute("href"),c=/\/|index.html/g,o.replace(c,"")===u.replace(c,"")&&(0,f.default)(i,"active")}var o,u,c}(),document.querySelector("#container").addEventListener("scroll",function(t){o()}),window.addEventListener("scroll",function(t){o()}),o()),(0,s.addLoadEvent)(function(){c.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object.defineProperty(t,n,{writable:!0,configurable:!0,value:r})}if(r(409),r(204),r(207),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0,n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},function(N,t){(function(t){!function(t){"use strict";function o(t,n,r,e){var o,u,c,f,i=n&&n.prototype instanceof h?n:h,a=Object.create(i.prototype),s=new p(e||[]);return a._invoke=(o=t,u=r,c=s,f=_,function(t,n){if(f===E)throw new Error("Generator is already running");if(f===M){if("throw"===t)throw n;return d()}for(c.method=t,c.arg=n;;){var r=c.delegate;if(r){var e=v(r,c);if(e){if(e===P)continue;return e}}if("next"===c.method)c.sent=c._sent=c.arg;else if("throw"===c.method){if(f===_)throw f=M,c.arg;c.dispatchException(c.arg)}else"return"===c.method&&c.abrupt("return",c.arg);f=E;var i=l(o,u,c);if("normal"===i.type){if(f=c.done?M:O,i.arg===P)continue;return{value:i.arg,done:c.done}}"throw"===i.type&&(f=M,c.method="throw",c.arg=i.arg)}}),a}function l(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function h(){}function r(){}function n(){}function e(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function u(c){function f(t,n,r,e){var i=l(c[t],c,n);if("throw"!==i.type){var o=i.arg,u=o.value;return u&&"object"==typeof u&&y.call(u,"__await")?Promise.resolve(u.__await).then(function(t){f("next",t,r,e)},function(t){f("throw",t,r,e)}):Promise.resolve(u).then(function(t){o.value=t,r(o)},e)}e(i.arg)}var n;"object"==typeof t.process&&t.process.domain&&(f=t.process.domain.bind(f)),this._invoke=function(r,e){function t(){return new Promise(function(t,n){f(r,e,t,n)})}return n=n?n.then(t,t):t()}}function v(t,n){var r=t.iterator[n.method];if(r===a){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=a,v(t,n),"throw"===n.method))return P;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return P}var e=l(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,P;var i=e.arg;return i?i.done?(n[t.resultName]=i.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=a),n.delegate=null,P):i:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,P)}function i(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function c(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(i,this),this.reset(!0)}function f(n){if(n){var t=n[m];if(t)return t.call(n);if("function"==typeof n.next)return n;if(!isNaN(n.length)){var r=-1,e=function t(){for(;++r<n.length;)if(y.call(n,r))return t.value=n[r],t.done=!1,t;return t.value=a,t.done=!0,t};return e.next=e}}return{next:d}}function d(){return{value:a,done:!0}}var a,s=Object.prototype,y=s.hasOwnProperty,g="function"==typeof Symbol?Symbol:{},m=g.iterator||"@@iterator",b=g.asyncIterator||"@@asyncIterator",x=g.toStringTag||"@@toStringTag",S="object"==typeof N,w=t.regeneratorRuntime;if(w)S&&(N.exports=w);else{(w=t.regeneratorRuntime=S?N.exports:{}).wrap=o;var _="suspendedStart",O="suspendedYield",E="executing",M="completed",P={},j={};j[m]=function(){return this};var F=Object.getPrototypeOf,A=F&&F(F(f([])));A&&A!==s&&y.call(A,m)&&(j=A);var L=n.prototype=h.prototype=Object.create(j);r.prototype=L.constructor=n,n.constructor=r,n[x]=r.displayName="GeneratorFunction",w.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===r||"GeneratorFunction"===(n.displayName||n.name))},w.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,n):(t.__proto__=n,x in t||(t[x]="GeneratorFunction")),t.prototype=Object.create(L),t},w.awrap=function(t){return{__await:t}},e(u.prototype),u.prototype[b]=function(){return this},w.AsyncIterator=u,w.async=function(t,n,r,e){var i=new u(o(t,n,r,e));return w.isGeneratorFunction(n)?i:i.next().then(function(t){return t.done?t.value:i.next()})},e(L),L[x]="Generator",L[m]=function(){return this},L.toString=function(){return"[object Generator]"},w.keys=function(r){var e=[];for(var t in r)e.push(t);return e.reverse(),function t(){for(;e.length;){var n=e.pop();if(n in r)return t.value=n,t.done=!1,t}return t.done=!0,t}},w.values=f,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=a,this.done=!1,this.delegate=null,this.method="next",this.arg=a,this.tryEntries.forEach(c),!t)for(var n in this)"t"===n.charAt(0)&&y.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=a)},stop:function(){this.done=!0;var t=this.tryEntries[0].completion;if("throw"===t.type)throw t.arg;return this.rval},dispatchException:function(r){function t(t,n){return o.type="throw",o.arg=r,e.next=t,n&&(e.method="next",e.arg=a),!!n}if(this.done)throw r;for(var e=this,n=this.tryEntries.length-1;0<=n;--n){var i=this.tryEntries[n],o=i.completion;if("root"===i.tryLoc)return t("end");if(i.tryLoc<=this.prev){var u=y.call(i,"catchLoc"),c=y.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return t(i.catchLoc,!0);if(this.prev<i.finallyLoc)return t(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return t(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return t(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;0<=r;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&y.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,P):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),P},finish:function(t){for(var n=this.tryEntries.length-1;0<=n;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),c(r),P}},catch:function(t){for(var n=this.tryEntries.length-1;0<=n;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;c(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:f(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=a),P}}}}("object"==typeof t?t:"object"==typeof window?window:"object"==typeof self?self:this)}).call(t,function(){return this}())},,,function(t,n,r){r(217),t.exports=r(49).RegExp.escape},,,,function(t,n,r){var e=r(5),i=r(121),o=r(7)("species");t.exports=function(t){var n;return i(t)&&("function"!=typeof(n=t.constructor)||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){"use strict";var e=r(4),i=Date.prototype.getTime,o=Date.prototype.toISOString,u=function(t){return 9<t?t:"0"+t};t.exports=e(function(){return"0385-07-25T07:06:39.999Z"!=o.call(new Date(-5e13-1))})||!e(function(){o.call(new Date(NaN))})?function(){if(!isFinite(i.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":9999<n?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(99<r?r:"0"+u(r))+"Z"}:o},function(t,n,r){"use strict";var e=r(2),i=r(53);t.exports=function(t){if("string"!==t&&"number"!==t&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),"number"!=t)}},function(t,n,r){var c=r(73),f=r(125),a=r(116);t.exports=function(t){var n=c(t),r=f.f;if(r)for(var e,i=r(t),o=a.f,u=0;i.length>u;)o.call(t,e=i[u++])&&n.push(e);return n}},function(t,n){t.exports=function(n,r){var e=r===Object(r)?function(t){return r[t]}:r;return function(t){return String(t).replace(n,e)}}},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(1),i=r(215)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(162)}),r(66)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(133)}),r(66)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,1<arguments.length?arguments[1]:void 0)}}),r(66)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,1<arguments.length?arguments[1]:void 0)}}),r(66)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var h=r(46),e=r(1),v=r(17),p=r(173),d=r(141),y=r(11),g=r(135),m=r(157);e(e.S+e.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,e,i,o=v(t),u="function"==typeof this?this:Array,c=arguments.length,f=1<c?arguments[1]:void 0,a=void 0!==f,s=0,l=m(o);if(a&&(f=h(f,2<c?arguments[2]:void 0,2)),null==l||u==Array&&d(l))for(r=new u(n=y(o.length));s<n;s++)g(r,s,a?f(o[s],s):o[s]);else for(i=l.call(o),r=new u;!(e=i.next()).done;s++)g(r,s,a?p(i,f,[e.value,s],!0):e.value);return r.length=s,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(121)})},function(t,n,r){"use strict";var e=r(1),i=r(32),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(32),o=r(52),u=r(11),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(1<arguments.length&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);0<=e;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(135);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);t<n;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(164);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(164);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(139),a=r(45),s=r(77),l=r(11),h=[].slice;e(e.P+e.F*r(4)(function(){i&&h.call(i)}),"Array",{slice:function(t,n){var r=l(this.length),e=a(this);if(n=void 0===n?r:n,"Array"==e)return h.call(this,t,n);for(var i=s(t,r),o=s(n,r),u=l(o-i),c=Array(u),f=0;f<u;f++)c[f]="String"==e?this.charAt(i+f):this[i+f];return c}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(19),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(76)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){var e=r(1),i=r(212);e(e.P+e.F*(Date.prototype.toISOString!==i),"Date",{toISOString:i})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(53);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(26)(i,e,r(213))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(27)(e,o,function(){var t=c.call(this);return t==t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(165)})},function(t,n,r){"use strict";var e=r(5),i=r(31),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(10).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(10).f,i=Function.prototype,o=/^\s*function ([^ (]*)/;"name"in i||r(9)&&e(i,"name",{configurable:!0,get:function(){try{return(""+this).match(o)[1]}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(176),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:94906265.62425156<t?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){var e=r(1),i=Math.asinh;e(e.S+e.F*!(i&&0<1/i(0)),"Math",{asinh:function t(n){return isFinite(n=+n)&&0!=n?n<0?-t(-n):Math.log(n+Math.sqrt(n*n+1)):n}})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(145);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(144);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1);e(e.S,"Math",{fround:r(175)})},function(t,n,r){var e=r(1),f=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,i=0,o=0,u=arguments.length,c=0;o<u;)c<(r=f(arguments[o++]))?(i=i*(e=c/r)*e+1,c=r):i+=0<r?(e=r/c)*e:r;return c===1/0?1/0:c*Math.sqrt(i)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)*Math.LOG10E}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(176)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(145)})},function(t,n,r){var e=r(1),i=r(144),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(144),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(0<t?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(25),o=r(45),u=r(140),s=r(53),c=r(4),f=r(72).f,a=r(30).f,l=r(10).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(71)(y))==v,m="trim"in String.prototype,b=function(t){var n=s(t,!1);if("string"==typeof n&&2<n.length){var r,e,i,o=(n=m?n.trim():h(n,3)).charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,c=n.slice(2),f=0,a=c.length;f<a;f++)if((u=c.charCodeAt(f))<48||i<u)return NaN;return parseInt(c,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?c(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(b(n)),r,p):b(n)};for(var x,S=r(9)?f(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),w=0;S.length>w;w++)i(d,x=S[w])&&!i(p,x)&&l(p,x,a(d,x));(p.prototype=y).constructor=p,r(27)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(172),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(184);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(185);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),a=r(52),s=r(161),l=r(152),i=1..toFixed,o=Math.floor,u=[0,0,0,0,0,0],h="Number.toFixed: incorrect invocation!",v=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*u[r],u[r]=e%1e7,e=o(e/1e7)},p=function(t){for(var n=6,r=0;0<=--n;)r+=u[n],u[n]=o(r/t),r=r%t*1e7},d=function(){for(var t=6,n="";0<=--t;)if(""!==n||0===t||0!==u[t]){var r=String(u[t]);n=""===n?r:n+l.call("0",7-r.length)+r}return n},y=function(t,n,r){return 0===n?r:n%2==1?y(t,n-1,r*t):y(t*t,n/2,r)};e(e.P+e.F*(!!i&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){i.call({})})),"Number",{toFixed:function(t){var n,r,e,i,o=s(this,h),u=a(t),c="",f="0";if(u<0||20<u)throw RangeError(h);if(o!=o)return"NaN";if(o<=-1e21||1e21<=o)return String(o);if(o<0&&(c="-",o=-o),1e-21<o)if(r=(n=function(t){for(var n=0,r=o*y(2,69,1);4096<=r;)n+=12,r/=4096;for(;2<=r;)n+=1,r/=2;return n}()-69)<0?o*y(2,-n,1):o/y(2,n,1),r*=4503599627370496,0<(n=52-n)){for(v(0,r),e=u;7<=e;)v(1e7,0),e-=7;for(v(y(10,e,1),0),e=n-1;23<=e;)p(1<<23),e-=23;p(1<<e),v(1,1),p(2),f=d()}else v(0,r),v(1<<-n,0),f=d()+l.call("0",u);return 0<u?c+((i=f.length)<=u?"0."+l.call("0",u-i)+f:f.slice(0,i-u)+"."+f.slice(i-u)):c+f}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(161),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(178)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(71)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(9),"Object",{defineProperties:r(179)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(9),"Object",{defineProperty:r(10).f})},function(t,n,r){var e=r(5),i=r(67).onFreeze;r(51)("freeze",function(n){return function(t){return n&&e(t)?n(i(t)):t}})},function(t,n,r){var e=r(32),i=r(30).f;r(51)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(51)("getOwnPropertyNames",function(){return r(180).f})},function(t,n,r){var e=r(17),i=r(31);r(51)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(5);r(51)("isExtensible",function(n){return function(t){return!!e(t)&&(!n||n(t))}})},function(t,n,r){var e=r(5);r(51)("isFrozen",function(n){return function(t){return!e(t)||!!n&&n(t)}})},function(t,n,r){var e=r(5);r(51)("isSealed",function(n){return function(t){return!e(t)||!!n&&n(t)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(216)})},function(t,n,r){var e=r(17),i=r(73);r(51)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(5),i=r(67).onFreeze;r(51)("preventExtensions",function(n){return function(t){return n&&e(t)?n(i(t)):t}})},function(t,n,r){var e=r(5),i=r(67).onFreeze;r(51)("seal",function(n){return function(t){return n&&e(t)?n(i(t)):t}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(148).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(27)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(184);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(185);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u,c=r(70),f=r(3),a=r(46),s=r(114),l=r(1),h=r(5),v=r(19),p=r(68),d=r(69),y=r(129),g=r(154).set,m=r(146)(),b=r(147),x=r(186),S=r(187),w="Promise",_=f.TypeError,O=f.process,E=f[w],M="process"==s(O),P=function(){},j=i=b.f,F=!!function(){try{var t=E.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(P,P)};return(M||"function"==typeof PromiseRejectionEvent)&&t.then(P)instanceof n}catch(t){}}(),A=function(t){var n;return!(!h(t)||"function"!=typeof(n=t.then))&&n},L=function(a,n){if(!a._n){a._n=!0;var r=a._c;m(function(){for(var c=a._v,f=1==a._s,t=0;r.length>t;)!function(t){var n,r,e=f?t.ok:t.fail,i=t.resolve,o=t.reject,u=t.domain;try{e?(f||(2==a._h&&I(a),a._h=1),!0===e?n=c:(u&&u.enter(),n=e(c),u&&u.exit()),n===t.promise?o(_("Promise-chain cycle")):(r=A(n))?r.call(n,i,o):i(n)):o(c)}catch(t){o(t)}}(r[t++]);a._c=[],a._n=!1,n&&!a._h&&N(a)})}},N=function(o){g.call(f,function(){var t,n,r,e=o._v,i=T(o);if(i&&(t=x(function(){M?O.emit("unhandledRejection",e,o):(n=f.onunhandledrejection)?n({promise:o,reason:e}):(r=f.console)&&r.error&&r.error("Unhandled promise rejection",e)}),o._h=M||T(o)?2:1),o._a=void 0,i&&t.e)throw t.v})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if((n=r[e++]).fail||!T(n.promise))return!1;return!0},I=function(n){g.call(f,function(){var t;M?O.emit("rejectionHandled",n):(t=f.onrejectionhandled)&&t({promise:n,reason:n._v})})},k=function(t){var n=this;n._d||(n._d=!0,(n=n._w||n)._v=t,n._s=2,n._a||(n._a=n._c.slice()),L(n,!0))},R=function(t){var r,e=this;if(!e._d){e._d=!0,e=e._w||e;try{if(e===t)throw _("Promise can't be resolved itself");(r=A(t))?m(function(){var n={_w:e,_d:!1};try{r.call(t,a(R,n,1),a(k,n,1))}catch(t){k.call(n,t)}}):(e._v=t,e._s=1,L(e,!1))}catch(t){k.call({_w:e,_d:!1},t)}}};F||(E=function(t){p(this,E,w,"_h"),v(t),e.call(this);try{t(a(R,this,1),a(k,this,1))}catch(t){k.call(this,t)}},(e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1}).prototype=r(75)(E.prototype,{then:function(t,n){var r=j(y(this,E));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=M?O.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&L(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),o=function(){var t=new e;this.promise=t,this.resolve=a(R,t,1),this.reject=a(k,t,1)},b.f=j=function(t){return t===E||t===u?new o(t):i(t)}),l(l.G+l.W+l.F*!F,{Promise:E}),r(81)(E,w),r(76)(w),u=r(49)[w],l(l.S+l.F*!F,w,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),l(l.S+l.F*(c||!F),w,{resolve:function(t){return S(c&&this===u?E:this,t)}}),l(l.S+l.F*!(F&&r(123)(function(t){E.all(t).catch(P)})),w,{all:function(t){var u=this,n=j(u),c=n.resolve,f=n.reject,r=x(function(){var e=[],i=0,o=1;d(t,!1,function(t){var n=i++,r=!1;e.push(void 0),o++,u.resolve(t).then(function(t){r||(r=!0,e[n]=t,--o||c(e))},f)}),--o||c(e)});return r.e&&f(r.v),n.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=x(function(){d(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i.e&&e(i.v),r.promise}})},function(t,n,r){var e=r(1),o=r(19),u=r(2),c=(r(3).Reflect||{}).apply,f=Function.apply;e(e.S+e.F*!r(4)(function(){c(function(){})}),"Reflect",{apply:function(t,n,r){var e=o(t),i=u(r);return c?c(e,n,i):f.call(e,n,i)}})},function(t,n,r){var e=r(1),c=r(71),f=r(19),a=r(2),s=r(5),i=r(4),l=r(165),h=(r(3).Reflect||{}).construct,v=i(function(){function t(){}return!(h(function(){},[],t)instanceof t)}),p=!i(function(){h(function(){})});e(e.S+e.F*(v||p),"Reflect",{construct:function(t,n){f(t),a(n);var r=arguments.length<3?t:f(arguments[2]);if(p&&!v)return h(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(l.apply(t,e))}var i=r.prototype,o=c(s(i)?i:Object.prototype),u=Function.apply.call(t,o,n);return s(u)?u:o}})},function(t,n,r){var e=r(10),i=r(1),o=r(2),u=r(53);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(30).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(142)(o,"Object",function(){var t,n=this._k;do{if(this._i>=n.length)return{value:void 0,done:!0}}while(!((t=n[this._i++])in this._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(30),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(31),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){var u=r(30),c=r(31),f=r(25),e=r(1),a=r(5),s=r(2);e(e.S,"Reflect",{get:function t(n,r){var e,i,o=arguments.length<3?n:arguments[2];return s(n)===o?n[r]:(e=u.f(n,r))?f(e,"value")?e.value:void 0!==e.get?e.get.call(o):void 0:a(i=c(n))?t(i,r,o):void 0}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(183)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(148);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){var f=r(10),a=r(30),s=r(31),l=r(25),e=r(1),h=r(74),v=r(2),p=r(5);e(e.S,"Reflect",{set:function t(n,r,e){var i,o,u=arguments.length<4?n:arguments[3],c=a.f(v(n),r);if(!c){if(p(o=s(n)))return t(o,r,e,u);c=h(0)}return l(c,"value")?!(!1===c.writable||!p(u)||((i=a.f(u,r)||h(0)).value=e,f.f(u,r,i),0)):void 0!==c.set&&(c.set.call(u,e),!0)}})},function(t,n,r){var e=r(3),o=r(140),i=r(10).f,u=r(72).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(9)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),i=void 0===n;return!r&&e&&t.constructor===a&&i?t:o(p?new s(e&&!i?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&i?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(n){n in a||i(a,n,{configurable:!0,get:function(){return s[n]},set:function(t){s[n]=t}})}(d[y++]);(l.constructor=a).prototype=l,r(27)(e,"RegExp",a)}r(76)("RegExp")},function(t,n,r){r(119)("match",1,function(e,i,t){return[function(t){"use strict";var n=e(this),r=null==t?void 0:t[i];return void 0!==r?r.call(t,n):new RegExp(t)[i](String(n))},t]})},function(t,n,r){r(119)("replace",2,function(i,o,u){return[function(t,n){"use strict";var r=i(this),e=null==t?void 0:t[o];return void 0!==e?e.call(t,r,n):u.call(String(r),t,n)},u]})},function(t,n,r){r(119)("search",1,function(e,i,t){return[function(t){"use strict";var n=e(this),r=null==t?void 0:t[i];return void 0!==r?r.call(t,n):new RegExp(t)[i](String(n))},t]})},function(t,n,r){r(119)("split",2,function(i,o,u){"use strict";var v=r(122),p=u,d=[].push,t="split",y="length",g="lastIndex";if("c"=="abbc"[t](/(b)*/)[1]||4!="test"[t](/(?:)/,-1)[y]||2!="ab"[t](/(?:ab)*/)[y]||4!="."[t](/(.?)(.?)/)[y]||1<"."[t](/()()/)[y]||""[t](/.?/)[y]){var m=void 0===/()??/.exec("")[1];u=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!v(t))return p.call(r,t,n);var e,i,o,u,c,f=[],a=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),s=0,l=void 0===n?4294967295:n>>>0,h=new RegExp(t.source,a+"g");for(m||(e=new RegExp("^"+h.source+"$(?!\\s)",a));(i=h.exec(r))&&!((o=i.index+i[0][y])>s&&(f.push(r.slice(s,i.index)),!m&&1<i[y]&&i[0].replace(e,function(){for(c=1;c<arguments[y]-2;c++)void 0===arguments[c]&&(i[c]=void 0)}),1<i[y]&&i.index<r[y]&&d.apply(f,i.slice(1)),u=i[0][y],s=o,f[y]>=l));)h[g]===i.index&&h[g]++;return s===r[y]?!u&&h.test("")||f.push(""):f.push(r.slice(s)),f[y]>l?f.slice(0,l):f}}else"0"[t](void 0,0)[y]&&(u=function(t,n){return void 0===t&&0===n?[]:p.call(this,t,n)});return[function(t,n){var r=i(this),e=null==t?void 0:t[o];return void 0!==e?e.call(t,r,n):u.call(String(r),t,n)},u]})},function(t,n,r){"use strict";r(192);var e=r(2),i=r(120),o=r(9),u="toString",c=/./[u],f=function(t){r(27)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(28)("anchor",function(n){return function(t){return n(this,"a","name",t)}})},function(t,n,r){"use strict";r(28)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(28)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(28)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(150)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),u=r(11),c=r(151),f="endsWith",a=""[f];e(e.P+e.F*r(138)(f),"String",{endsWith:function(t){var n=c(this,t,f),r=1<arguments.length?arguments[1]:void 0,e=u(n.length),i=void 0===r?e:Math.min(u(r),e),o=String(t);return a?a.call(n,o,i):n.slice(i-o.length,i)===o}})},function(t,n,r){"use strict";r(28)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(28)("fontcolor",function(n){return function(t){return n(this,"font","color",t)}})},function(t,n,r){"use strict";r(28)("fontsize",function(n){return function(t){return n(this,"font","size",t)}})},function(t,n,r){var e=r(1),o=r(77),u=String.fromCharCode,i=String.fromCodePoint;e(e.S+e.F*(!!i&&1!=i.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,i=0;i<e;){if(n=+arguments[i++],o(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?u(n):u(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(151);e(e.P+e.F*r(138)("includes"),"String",{includes:function(t){return!!~i(this,t,"includes").indexOf(t,1<arguments.length?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(28)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(150)(!0);r(143)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(28)("link",function(n){return function(t){return n(this,"a","href",t)}})},function(t,n,r){var e=r(1),u=r(32),c=r(11);e(e.S,"String",{raw:function(t){for(var n=u(t.raw),r=c(n.length),e=arguments.length,i=[],o=0;o<r;)i.push(String(n[o++])),o<e&&i.push(String(arguments[o]));return i.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(152)})},function(t,n,r){"use strict";r(28)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(11),o=r(151),u="startsWith",c=""[u];e(e.P+e.F*r(138)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(1<arguments.length?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(28)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(28)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(28)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),u=r(25),i=r(9),o=r(1),c=r(27),f=r(67).KEY,a=r(4),s=r(128),l=r(81),h=r(78),v=r(7),p=r(190),d=r(156),y=r(214),g=r(121),m=r(2),b=r(32),x=r(53),S=r(74),w=r(71),_=r(180),O=r(30),E=r(10),M=r(73),P=O.f,j=E.f,F=_.f,A=e.Symbol,L=e.JSON,N=L&&L.stringify,T="prototype",I=v("_hidden"),k=v("toPrimitive"),R={}.propertyIsEnumerable,C=s("symbol-registry"),D=s("symbols"),G=s("op-symbols"),W=Object[T],U="function"==typeof A,V=e.QObject,B=!V||!V[T]||!V[T].findChild,q=i&&a(function(){return 7!=w(j({},"a",{get:function(){return j(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=P(W,n);e&&delete W[n],j(t,n,r),e&&t!==W&&j(W,n,e)}:j,z=function(t){var n=D[t]=w(A[T]);return n._k=t,n},H=U&&"symbol"==typeof A.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof A},K=function(t,n,r){return t===W&&K(G,n,r),m(t),n=x(n,!0),m(r),u(D,n)?(r.enumerable?(u(t,I)&&t[I][n]&&(t[I][n]=!1),r=w(r,{enumerable:S(0,!1)})):(u(t,I)||j(t,I,S(1,{})),t[I][n]=!0),q(t,n,r)):j(t,n,r)},J=function(t,n){m(t);for(var r,e=y(n=b(n)),i=0,o=e.length;i<o;)K(t,r=e[i++],n[r]);return t},Y=function(t){var n=R.call(this,t=x(t,!0));return!(this===W&&u(D,t)&&!u(G,t))&&(!(n||!u(this,t)||!u(D,t)||u(this,I)&&this[I][t])||n)},$=function(t,n){if(t=b(t),n=x(n,!0),t!==W||!u(D,n)||u(G,n)){var r=P(t,n);return!r||!u(D,n)||u(t,I)&&t[I][n]||(r.enumerable=!0),r}},X=function(t){for(var n,r=F(b(t)),e=[],i=0;r.length>i;)u(D,n=r[i++])||n==I||n==f||e.push(n);return e},Q=function(t){for(var n,r=t===W,e=F(r?G:b(t)),i=[],o=0;e.length>o;)!u(D,n=e[o++])||r&&!u(W,n)||i.push(D[n]);return i};U||(c((A=function(){if(this instanceof A)throw TypeError("Symbol is not a constructor!");var n=h(0<arguments.length?arguments[0]:void 0),r=function(t){this===W&&r.call(G,t),u(this,I)&&u(this[I],n)&&(this[I][n]=!1),q(this,n,S(1,t))};return i&&B&&q(W,n,{configurable:!0,set:r}),z(n)})[T],"toString",function(){return this._k}),O.f=$,E.f=K,r(72).f=_.f=X,r(116).f=Y,r(125).f=Q,i&&!r(70)&&c(W,"propertyIsEnumerable",Y,!0),p.f=function(t){return z(v(t))}),o(o.G+o.W+o.F*!U,{Symbol:A});for(var Z="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),tt=0;Z.length>tt;)v(Z[tt++]);for(var nt=M(v.store),rt=0;nt.length>rt;)d(nt[rt++]);o(o.S+o.F*!U,"Symbol",{for:function(t){return u(C,t+="")?C[t]:C[t]=A(t)},keyFor:function(t){if(!H(t))throw TypeError(t+" is not a symbol!");for(var n in C)if(C[n]===t)return n},useSetter:function(){B=!0},useSimple:function(){B=!1}}),o(o.S+o.F*!U,"Object",{create:function(t,n){return void 0===n?w(t):J(w(t),n)},defineProperty:K,defineProperties:J,getOwnPropertyDescriptor:$,getOwnPropertyNames:X,getOwnPropertySymbols:Q}),L&&o(o.S+o.F*(!U||a(function(){var t=A();return"[null]"!=N([t])||"{}"!=N({a:t})||"{}"!=N(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!H(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return"function"==typeof(n=e[1])&&(r=n),!r&&g(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!H(n))return n}),e[1]=n,N.apply(L,e)}}}),A[T][k]||r(26)(A[T],k,A[T].valueOf),l(A,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(130),o=r(155),a=r(2),s=r(77),l=r(11),u=r(5),c=r(3).ArrayBuffer,h=r(129),v=o.ArrayBuffer,p=o.DataView,f=i.ABV&&c.isView,d=v.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(c!==v),{ArrayBuffer:v}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return f&&f(t)||u(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new v(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(a(this),t);for(var r=a(this).byteLength,e=s(t,r),i=s(void 0===n?r:n,r),o=new(h(this,v))(l(i-e)),u=new p(this),c=new p(o),f=0;e<i;)c.setUint8(f++,u.getUint8(e++));return o}}),r(76)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(130).ABV,{DataView:r(155).DataView})},function(t,n,r){r(56)("Float32",4,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(56)("Float64",8,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(56)("Int16",2,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(56)("Int32",4,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(56)("Int8",1,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(56)("Uint16",2,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(56)("Uint32",4,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(56)("Uint8",1,function(e){return function(t,n,r){return e(this,t,n,r)}})},function(t,n,r){r(56)("Uint8",1,function(e){return function(t,n,r){return e(this,t,n,r)}},!0)},function(t,n,r){"use strict";var e=r(168),i=r(83);r(118)("WeakSet",function(t){return function(){return t(this,0<arguments.length?arguments[0]:void 0)}},{add:function(t){return e.def(i(this,"WeakSet"),t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(169),o=r(17),u=r(11),c=r(19),f=r(134);e(e.P,"Array",{flatMap:function(t){var n,r,e=o(this);return c(t),n=u(e.length),r=f(e,0),i(r,e,e,n,0,1,t,arguments[1]),r}}),r(66)("flatMap")},function(t,n,r){"use strict";var e=r(1),i=r(169),o=r(17),u=r(11),c=r(52),f=r(134);e(e.P,"Array",{flatten:function(){var t=arguments[0],n=o(this),r=u(n.length),e=f(n,0);return i(e,n,n,r,0,void 0===t?1:c(t)),e}}),r(66)("flatten")},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,1<arguments.length?arguments[1]:void 0)}}),r(66)("includes")},function(t,n,r){var e=r(1),i=r(146)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.G,{global:r(3)})},function(t,n,r){r(126)("Map")},function(t,n,r){r(127)("Map")},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(167)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{clamp:function(t,n,r){return Math.min(r,Math.max(n,t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{DEG_PER_RAD:Math.PI/180})},function(t,n,r){var e=r(1),i=180/Math.PI;e(e.S,"Math",{degrees:function(t){return t*i}})},function(t,n,r){var e=r(1),o=r(177),u=r(175);e(e.S,"Math",{fscale:function(t,n,r,e,i){return u(o(t,n,r,e,i))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=r>>>0;return(n>>>0)+(e>>>0)+((i&o|(i|o)&~(i+o>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=+t,e=+n,i=65535&r,o=65535&e,u=r>>16,c=e>>16,f=(u*o>>>0)+(i*o>>>16);return u*c+(f>>16)+((i*c>>>0)+(65535&f)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=r>>>0;return(n>>>0)-(e>>>0)-((~i&o|~(i^o)&i-o>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{RAD_PER_DEG:180/Math.PI})},function(t,n,r){var e=r(1),i=Math.PI/180;e(e.S,"Math",{radians:function(t){return t*i}})},function(t,n,r){var e=r(1);e(e.S,"Math",{scale:r(177)})},function(t,n,r){var e=r(1);e(e.S,"Math",{signbit:function(t){return(t=+t)!=t?t:0==t?1/t==1/0:0<t}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=+t,e=+n,i=65535&r,o=65535&e,u=r>>>16,c=e>>>16,f=(u*o>>>0)+(i*o>>>16);return u*c+(f>>>16)+((i*c>>>0)+(65535&f)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(19),u=r(10);r(9)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(19),u=r(10);r(9)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(182)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),f=r(183),a=r(32),s=r(30),l=r(135);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r,e=a(t),i=s.f,o=f(e),u={},c=0;o.length>c;)void 0!==(r=i(e,n=o[c++]))&&l(u,n,r);return u}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(53),u=r(31),c=r(30).f;r(9)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(53),u=r(31),c=r(30).f;r(9)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(182)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),o=r(3),u=r(49),i=r(146)(),c=r(7)("observable"),f=r(19),a=r(2),s=r(68),l=r(75),h=r(26),v=r(69),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},m=function(t){g(t)||(t._o=void 0,y(t))},b=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};b.prototype=l({},{unsubscribe:function(){m(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{m(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var S=function(t){s(this,S,"Observable","_f")._f=f(t)};l(S.prototype,{subscribe:function(t){return new b(t,this._f)},forEach:function(e){var i=this;return new(u.Promise||o.Promise)(function(t,n){f(e);var r=i.subscribe({next:function(t){try{return e(t)}catch(t){n(t),r.unsubscribe()}},error:n,complete:t})})}}),l(S,{from:function(t){var n="function"==typeof this?this:S,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return i(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,e=Array(n);t<n;)e[t]=arguments[t++];return new("function"==typeof this?this:S)(function(n){var r=!1;return i(function(){if(!r){for(var t=0;t<e.length;++t)if(n.next(e[t]),r)return;n.complete()}}),function(){r=!0}})}}),h(S.prototype,c,function(){return this}),e(e.G,{Observable:S}),r(76)("Observable")},function(t,n,r){"use strict";var e=r(1),i=r(49),o=r(3),u=r(129),c=r(187);e(e.P+e.R,"Promise",{finally:function(n){var r=u(this,i.Promise||o.Promise),t="function"==typeof n;return this.then(t?function(t){return c(r,n()).then(function(){return t})}:n,t?function(t){return c(r,n()).then(function(){throw t})}:n)}})},function(t,n,r){"use strict";var e=r(1),i=r(147),o=r(186);e(e.S,"Promise",{try:function(t){var n=i.f(this),r=o(t);return(r.e?n.reject:n.resolve)(r.v),n.promise}})},function(t,n,r){var e=r(55),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(55),o=r(2),u=e.key,c=e.map,f=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:u(arguments[2]),e=c(o(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var i=f.get(n);return i.delete(r),!!i.size||f.delete(n)}})},function(t,n,r){var o=r(193),u=r(163),e=r(55),i=r(2),c=r(31),f=e.keys,a=e.key,s=function(t,n){var r=f(t,n),e=c(t);if(null===e)return r;var i=s(e,n);return i.length?r.length?u(new o(r.concat(i))):i:r};e.exp({getMetadataKeys:function(t){return s(i(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(55),i=r(2),o=r(31),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(55),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(55),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(55),i=r(2),o=r(31),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(55),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(55),i=r(2),o=r(19),u=e.key,c=e.set;e.exp({metadata:function(r,e){return function(t,n){c(r,e,(void 0!==n?i:o)(t),u(n))}}})},function(t,n,r){r(126)("Set")},function(t,n,r){r(127)("Set")},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(167)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(150)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(50),o=r(11),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(142)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(188);e(e.P,"String",{padEnd:function(t){return i(this,t,1<arguments.length?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(188);e(e.P,"String",{padStart:function(t){return i(this,t,1<arguments.length?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(156)("asyncIterator")},function(t,n,r){r(156)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){r(126)("WeakMap")},function(t,n,r){r(127)("WeakMap")},function(t,n,r){r(126)("WeakSet")},function(t,n,r){r(127)("WeakSet")},function(t,n,r){for(var e=r(158),i=r(73),o=r(27),u=r(3),c=r(26),f=r(80),a=r(7),s=a("iterator"),l=a("toStringTag"),h=f.Array,v={CSSRuleList:!0,CSSStyleDeclaration:!1,CSSValueList:!1,ClientRectList:!1,DOMRectList:!1,DOMStringList:!1,DOMTokenList:!0,DataTransferItemList:!1,FileList:!1,HTMLAllCollection:!1,HTMLCollection:!1,HTMLFormElement:!1,HTMLSelectElement:!1,MediaList:!0,MimeTypeArray:!1,NamedNodeMap:!1,NodeList:!0,PaintRequestList:!1,Plugin:!1,PluginArray:!1,SVGLengthList:!1,SVGNumberList:!1,SVGPathSegList:!1,SVGPointList:!1,SVGStringList:!1,SVGTransformList:!1,SourceBufferList:!1,StyleSheetList:!0,TextTrackCueList:!1,TextTrackList:!1,TouchList:!1},p=i(v),d=0;d<p.length;d++){var y,g=p[d],m=v[g],b=u[g],x=b&&b.prototype;if(x&&(x[s]||c(x,s,h),x[l]||c(x,l,g),f[g]=h,m))for(y in e)x[y]||o(x,y,e[y],!0)}},function(t,n,r){var e=r(1),i=r(154);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=e.navigator,u=[].slice,c=!!o&&/MSIE .\./.test(o.userAgent),f=function(i){return function(t,n){var r=2<arguments.length,e=!!r&&u.call(arguments,2);return i(r?function(){("function"==typeof t?t:Function(t)).apply(this,e)}:t,n)}};i(i.G+i.B+i.F*c,{setTimeout:f(e.setTimeout),setInterval:f(e.setInterval)})},function(t,n,r){r(337),r(276),r(278),r(277),r(280),r(282),r(287),r(281),r(279),r(289),r(288),r(284),r(285),r(283),r(275),r(286),r(290),r(291),r(243),r(245),r(244),r(293),r(292),r(263),r(273),r(274),r(264),r(265),r(266),r(267),r(268),r(269),r(270),r(271),r(272),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(256),r(257),r(258),r(259),r(260),r(261),r(262),r(324),r(329),r(336),r(327),r(319),r(320),r(325),r(330),r(332),r(315),r(316),r(317),r(318),r(321),r(322),r(323),r(326),r(328),r(331),r(333),r(334),r(335),r(238),r(240),r(239),r(242),r(241),r(227),r(225),r(231),r(228),r(234),r(236),r(224),r(230),r(221),r(235),r(219),r(233),r(232),r(226),r(229),r(218),r(220),r(223),r(222),r(237),r(158),r(309),r(314),r(192),r(310),r(311),r(312),r(313),r(294),r(191),r(193),r(194),r(349),r(338),r(339),r(344),r(347),r(348),r(342),r(345),r(343),r(346),r(340),r(341),r(295),r(296),r(297),r(298),r(299),r(302),r(300),r(301),r(303),r(304),r(305),r(306),r(308),r(307),r(352),r(350),r(351),r(393),r(396),r(395),r(397),r(398),r(394),r(399),r(400),r(374),r(377),r(373),r(371),r(372),r(375),r(376),r(358),r(392),r(357),r(391),r(403),r(405),r(356),r(390),r(402),r(404),r(355),r(401),r(354),r(359),r(360),r(361),r(362),r(363),r(365),r(364),r(366),r(367),r(368),r(370),r(369),r(379),r(380),r(381),r(382),r(384),r(383),r(386),r(385),r(387),r(388),r(389),r(353),r(378),r(408),r(407),r(406),t.exports=r(49)},function(t,n){t.exports=function(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}}])</script><script src="/./main.e8862b.js"></script><script>!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src","/slider.5b7e29.js")}()</script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script><script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)"><div class="tools-nav header-menu"><ul style="width:70%"><li style="width:33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">All articles</a></li><li style="width:33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">Friends</a></li><li style="width:33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">About me</a></li></ul></div><div class="tools-wrap"><section class="tools-section tools-section-all" q-show="innerArchive"><div class="search-wrap"> <input class="search-ipt" q-model="search" type="text" placeholder="find something…"><i class="icon-search icon" q-show="search|isEmptyStr"></i><i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i></div><div class="widget tagcloud search-tag"><p class="search-tag-wording">tag:</p> <label class="search-switch"><input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags"></label><ul class="article-tag-list" q-show="showTags"><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color5">Algorithm</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color4">C++</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color3">DS</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color3">Android</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color5">Tech</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color3">ML</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color3">DL</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color1">Essay</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color4">git</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color5">hexo</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color2">golang</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color5">HTML</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color1">Javascript</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color4">CSS</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color3">js</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color5">html</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color5">Java</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color1">Redis</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color1">MYSQL</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color2">Spring</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color1">linux</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color2">papers</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color3">RL</a></li><li class="article-tag-list-item"> <a href="javascript:void(0)" class="js-tag color4">k8s</a></li><div class="clearfix"></div></ul></div><ul class="search-ul"><p q-show="jsonFail" style="padding:20px;font-size:12px"> 缺失模块。<br>1、请确保node版本大于6.2<br>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br> npm i hexo-generator-json-content --save<br><br> 3、在根目录_config.yml里添加配置：<pre style="font-size:12px" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre></p><li class="search-li" q-repeat="items" q-show="isShow"><a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a><p class="search-time"><i class="icon-calendar icon"></i><span q-text="date|dateformat"></span></p><p class="search-tag"><i class="icon-price-tags icon"></i><span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span></p></li></ul></section><section class="tools-section tools-section-friends" q-show="friends"><ul class="search-ul"><li class="search-li"><a href="https://leetcode-cn.com/u/louris/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i> 我的力扣</a></li><li class="search-li"><a href="https://www.six1110.top/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i> 饭勺</a></li><li class="search-li"><a href="https://www.zjcheng.site/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i> 中建</a></li></ul></section><section class="tools-section tools-section-me" q-show="aboutme"><div class="aboutme-wrap" id="js-aboutme">做一个安静细微的人，&lt;br&gt; 于角落里自在开放，&lt;br&gt; 默默悦人，&lt;br&gt; 却始终不引起过分热闹的关注，&lt;br&gt; 保有独立而随意的品格，&lt;br&gt; 这就很好。&lt;br&gt;&lt;br&gt; Stick to what you insist on,&lt;br&gt; believe what you believe!&lt;br&gt; Life hastily for decades,&lt;br&gt; do what I can!</div></section></div></div><div class="pswp" tabindex="-1" role="dialog" aria-hidden="true"><div class="pswp__bg"></div><div class="pswp__scroll-wrap"><div class="pswp__container"><div class="pswp__item"></div><div class="pswp__item"></div><div class="pswp__item"></div></div><div class="pswp__ui pswp__ui--hidden"><div class="pswp__top-bar"><div class="pswp__counter"></div> <button class="pswp__button pswp__button--close" title="Close (Esc)"></button> <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button> <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button> <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class="pswp__preloader"><div class="pswp__preloader__icn"><div class="pswp__preloader__cut"><div class="pswp__preloader__donut"></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class="pswp__share-tooltip"></div></div> <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button> <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class="pswp__caption"><div class="pswp__caption__center"></div></div></div></div></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/hijiki.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!1},react:{opacity:1},log:!1})</script></body></html>